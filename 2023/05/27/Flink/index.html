<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Markilue">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2023/05/27/flink/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="Flink turtorial">
<meta property="og:type" content="article">
<meta property="og:title" content="Flink">
<meta property="og:url" content="http://example.com/2023/05/27/Flink/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Flink turtorial">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230223205400529.png">
<meta property="article:published_time" content="2023-05-26T16:00:00.000Z">
<meta property="article:modified_time" content="2023-05-27T03:45:12.457Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="流处理">
<meta property="article:tag" content="实时数仓">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230223205400529.png">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/redefine-favicon.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/redefine-favicon.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/redefine-favicon.svg">
    <!--- Page Info-->
    
    <title>
        
            Flink -
        
        Markilue&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/assets/fonts.css">

    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.xml"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"busuanzi_counter":{"enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"With no fear in my heart, god comes into my mind","subtitle":{"text":["love, believe, wait","go ahead!"],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#d5d9dc","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"links":{"github":"https://github.com/kevinding1125","instagram":null,"zhihu":null,"twitter":null,"email":"markilue@163.com","fa-solid fa-circle-g":"https://gitee.com/dingjiawen","fa-solid fa-music":"markilue"}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.1.4","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Status":{"path":"https://status.evanluo.top/","icon":"fa-regular fa-chart-bar"},"About":{"icon":"fa-regular fa-user","submenus":{"Me":"/about","Github":"https://github.com/kevinding1125","Blog":"https://ohevan.com","Friends":"/friends"}},"Links":{"icon":"fa-regular fa-link","submenus":{"miykah's blog":"https://miykah.top/","Link2":"/link2","Link3":"/link3"}}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}}};
    Global.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Markilue&#39;s Blog
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        ARCHIVES
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    target="_blank" rel="noopener" href="https://status.evanluo.top/"  >
                                    
                                        
                                            <i class="fa-regular fa-chart-bar"></i>
                                        
                                        STATUS
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-user"></i>
                                        
                                        ABOUT&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a href="/about">ME
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://github.com/kevinding1125">GITHUB
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://ohevan.com">BLOG
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a href="/friends">FRIENDS
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-link"></i>
                                        
                                        LINKS&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://miykah.top/">MIYKAH&#39;S BLOG
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a href="/link2">LINK2
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a href="/link3">LINK3
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                ARCHIVES
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        target="_blank" rel="noopener" href="https://status.evanluo.top/"  >
                             
                                
                                    <i class="fa-regular fa-chart-bar"></i>
                                
                                STATUS
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-user"></i>
                                
                                ABOUT&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" href="/about">ME</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://github.com/kevinding1125">GITHUB</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://ohevan.com">BLOG</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" href="/friends">FRIENDS</a>
                            </li>
                        
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-link"></i>
                                
                                LINKS&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://miykah.top/">MIYKAH&#39;S BLOG</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" href="/link2">LINK2</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" href="/link3">LINK3</a>
                            </li>
                        
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">Flink</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/redefine-avatar.svg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Markilue</span>
                            
                                <span class="author-label">Lv1</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2023-05-27</span>
        <span class="mobile">2023-05-27 00</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2023-05-27 11:45:12</span>
            <span class="mobile">2023-05-27 11:45</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/%E6%B5%81%E5%A4%84%E7%90%86/">流处理</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/">实时数仓</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h1 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h1><h2 id="0-知识点总结"><a href="#0-知识点总结" class="headerlink" title="0  知识点总结"></a>0  知识点总结</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230223205400529.png"
                      alt="image-20230223205400529"
                ></p>
<h2 id="1-Flink简介"><a href="#1-Flink简介" class="headerlink" title="1  Flink简介"></a>1  Flink简介</h2><p>Apache Flink 是第三代分布式流处理器，它拥有极富竞争力的功能。它提供准确的大规模流处理，具有高吞吐量和低延迟。特别的是，以下功能使Flink 脱颖而出：</p>
<ul>
<li>事件时间（event-time）和处理时间（processing-tme）语义。即使对于无序事件流，事件时间（event-time）语义仍然能提供一致且准确的结果。而处理时间（processing-time）语义可用于具有极低延迟要求的应用程序。</li>
<li>精确一次（exactly-once）的状态一致性保证。</li>
<li>每秒处理数百万个事件，<strong>毫秒级延迟</strong>。Flink 应用程序可以扩展为在数千个核（cores）上运行。</li>
<li>分层API，具有不同的权衡表现力和易用性。本书介绍了DataStream API 和过程函数（process function），为常见的流处理操作提供原语，如窗口和异步操作，以及精确控制状态和时间的接口。本书不讨论Flink 的关系API，SQL 和LINQ 风格的Table API。</li>
<li>连接到最常用的存储系统，如Apache Kafka，Apache Cassandra，Elasticsearch，JDBC，Kinesis 和（分布式）文件系统，如HDFS 和S3。</li>
<li>由于其高可用的设置（无单点故障），以及与Kubernetes，YARN 和Apache Mesos的紧密集成，再加上从故障中快速恢复和动态扩展任务的能力，Flink 能够以极少的停机时间7*24 全天候运行流应用程序。</li>
<li>能够更新应用程序代码并将作业（jobs）迁移到不同的Flink 集群，而不会丢失应用程序的状态。</li>
<li>详细且可自定义的系统和应用程序指标集合，以提前识别问题并对其做出反应。</li>
<li>最后但同样重要的是，Flink 也是一个成熟的批处理器。</li>
</ul>
<h3 id="1-1-Flink特点"><a href="#1-1-Flink特点" class="headerlink" title="1.1  Flink特点"></a>1.1  Flink特点</h3><p>Flink的特点:</p>
<ol>
<li>支持高吞吐、低延迟、高性能的流处理</li>
<li><strong>支持带有事件时间的窗口（Window）操作</strong></li>
<li><strong>支持有状态计算的 Exactly-once 语义</strong></li>
<li>支持高度灵活的窗口（Window）操作，<strong>支持基于 time、count、session，以及 data-driven 的窗口操作</strong></li>
<li>支持具有 Backpressure 功能的持续流模型</li>
<li>支持基于轻量级分布式快照（Snapshot）实现的容错</li>
<li>一个运行时同时支持 Batch on Streaming 处理和 Streaming 处理</li>
<li><strong>Flink 在 JVM 内部实现了自己的内存管理</strong></li>
<li>支持迭代计算</li>
<li>支持程序自动优化：避<strong>免特定情况下 Shuffle、排序等昂贵操作，中间结果有必要进行缓存</strong></li>
</ol>
<h3 id="1-2-Flink基石"><a href="#1-2-Flink基石" class="headerlink" title="1.2  Flink基石"></a>1.2  Flink基石</h3><p>Flink 之所以能这么流行，离不开它最重要的四个基石：<strong>Checkpoint、State、Time、Window</strong>。</p>
<p><strong>首先是 Checkpoint 机制，这是 Flink 最重要的一个特性</strong>。Flink 基于<em>Chandy-Lamport</em>算法实现了一个分布式的一致性的快照，从而提供了一致性的语义。Chandy-Lamport 算法实际上在 1985 年的时候已经被提出来，但并没有被很广泛的应用，而 Flink 则把这个算法发扬光大了。</p>
<p>Spark 最近在实现 Continue streaming，Continue streaming 的目的是为了降低它处理的延时，其也需要提供这种一致性的语义，最终采用 Chandy-Lamport 这个算法，说明 Chandy-Lamport 算法在业界得到了一定的肯定。</p>
<p>提供了一致性的语义之后，Flink 为了让用户在编程时能够更轻松、更容易地去管理状态，还提供了一套非常简单明了的 State API，包括里面的有 <strong>ValueState</strong>、<strong>ListState</strong>、<strong>MapState</strong>，近期添加了 <strong>BroadcastState</strong>，使用 State API 能够自动享受到这种一致性的语义。</p>
<p>除此之外，Flink 还实现了 <strong>Watermark 的机制</strong>，能够支持<strong>基于事件的时间的处理</strong>，或者说基于系统时间的处理，能够<strong>容忍数据的延时、容忍数据的迟到、容忍乱序的数据。</strong></p>
<p>另外流计算中一般在对流数据进行操作之前都会先进行开窗，即基于一个什么样的窗口上做这个计算。Flink 提供了开箱即用的各种窗口，比如<strong>滑动窗口、滚动窗口、会话窗口以及非常灵活的自定义的窗口。</strong></p>
<h3 id="1-3-批处理和流处理"><a href="#1-3-批处理和流处理" class="headerlink" title="1.3  批处理和流处理"></a>1.3  批处理和流处理</h3><p>批处理的特点是有界、持久、大量，批处理非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。流处理的特点是无界、实时，流处理方式无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。</p>
<p>在 Spark 生态体系中，对于批处理和流处理采用了不同的技术框架，批处理由 SparkSQL 实现，流处理由 Spark Streaming 实现，这也是大部分框架采用的策略，使用独立的处理器实现批处理和流处理，而 Flink 可以同时实现批处理和流处理。</p>
<p>Flink 是如何同时实现批处理与流处理的呢？答案是，Flink 将批处理（即处理有限的静态数据）视作一种特殊的流处理。</p>
<p>Flink 的核心计算架构是下图中的 Flink Runtime 执行引擎，它是一个分布式系统，能够接受数据流程序并在一台或多台机器上以容错方式执行。</p>
<p>Flink Runtime 执行引擎可以作为 YARN（Yet Another Resource Negotiator）的应用程序在集群上运行，也可以在 Mesos 集群上运行，还可以在单机上运行（这对于调试 Flink 应用程序来说非常有用）。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230223213135907.png"
                      alt="image-20230223213135907"
                ></p>
<p>上图为 Flink 技术栈的核心组成部分，值得一提的是，<strong>Flink 分别提供了面向流式处理的接口（DataStream API）和面向批处理的接口（DataSet API）</strong>。因此，Flink 既可以完成流处理，也可以完成批处理。Flink 支持的拓展库涉及机器学习（FlinkML）、复杂事件处理（CEP）、以及图计算（Gelly），还有分别针对流处理和批处理的 Table API。</p>
<p>能被 Flink Runtime 执行引擎接受的程序很强大，但是这样的程序有着冗长的代码，编写起来也很费力，基于这个原因，Flink 提供了封装在 Runtime 执行引擎之上的 API，以帮助用户方便地生成流式计算程序。Flink 提供了<strong>用于流处理的 DataStream API</strong> 和<strong>用于批处理的 DataSet API</strong>。值得注意的是，尽管 Flink Runtime 执行引擎是基于流处理的，但是 DataSet API 先于 DataStream API 被开发出来，这是因为工业界对无限流处理的需求在 Flink 诞生之初并不大。</p>
<p><strong>DataStream API 可以流畅地分析无限数据流，并且可以用 Java 或者 Scala 等来实现</strong>。开发人员需要基于一个叫 DataStream 的数据结构来开发，这个数据结构用于表示永不停止的分布式数据流。</p>
<p>Flink 的分布式特点体现在它能够在成百上千台机器上运行，它将大型的计算任务分成许多小的部分，每个机器执行一部分。Flink 能够自动地确保发生机器故障或者其他错误时计算能够持续进行，或者在修复 bug 或进行版本升级后有计划地再执行一次。这种能力使得开发人员不需要担心运行失败。Flink 本质上使用容错性数据流，这使得开发人员可以分析持续生成且永远不结束的数据（即流处理）。</p>
<h2 id="2-Flink运行框架"><a href="#2-Flink运行框架" class="headerlink" title="2  Flink运行框架"></a>2  Flink运行框架</h2><h3 id="2-1-Flink程序架构"><a href="#2-1-Flink程序架构" class="headerlink" title="2.1  Flink程序架构"></a>2.1  Flink程序架构</h3><p>Flink 程序的基本构建块是流和转换（请注意，Flink 的 DataSet API 中使用的 DataSet 也是内部流 ）。从概念上讲，流是（可能永无止境的）数据记录流，而转换是将一个或多个流作为一个或多个流的操作。输入，并产生一个或多个输出流。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230223214401281.png"
                      alt="image-20230223214401281"
                ></p>
<p>Flink 应用程序结构就是如上图所示：</p>
<p><strong>Source</strong>: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、RabbitMQ 等，当然你也可以定义自己的 source。</p>
<p><strong>Transformation</strong>：数据转换的各种操作，有 <code>Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select</code>等，操作很多，可以将数据转换计算成你想要的数据。</p>
<p><strong>Sink</strong>：接收器，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。</p>
<h3 id="2-2-Flink-并行数据流"><a href="#2-2-Flink-并行数据流" class="headerlink" title="2.2  Flink 并行数据流"></a>2.2  Flink 并行数据流</h3><p>Flink 程序在执行的时候，会被映射成一个 Streaming Dataflow，一个 Streaming Dataflow 是由一组 Stream 和 Transformation Operator 组成的。在启动时从一个或多个 Source Operator 开始，结束于一个或多个 Sink Operator。</p>
<p><strong>Flink 程序本质上是并行的和分布式的</strong>，在执行过程中，一个流(stream)包含一个或多个流分区，而每一个 operator 包含一个或多个 operator 子任务。操作子任务间彼此独立，在不同的线程中执行，甚至是在不同的机器或不同的容器上。operator 子任务的数量是这一特定 operator 的并行度。相同程序中的不同 operator 有不同级别的并行度。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_5.png"
                      alt="img"
                ></p>
<p>一个 Stream 可以被分成多个 Stream 的分区，也就是 Stream Partition。一个 Operator 也可以被分为多个 Operator Subtask。如上图中，Source 被分成 Source1 和 Source2，它们分别为 Source 的 Operator Subtask。每一个 Operator Subtask 都是在不同的线程当中独立执行的。一个 Operator 的并行度，就等于 Operator Subtask 的个数。上图 Source 的并行度为 2。而一个 Stream 的并行度就等于它生成的 Operator 的并行度。</p>
<p>数据在两个 operator 之间传递的时候有两种模式：</p>
<p><strong>One to One 模式</strong>：两个 operator 用此模式传递的时候，会保持数据的分区数和数据的排序；如上图中的 Source1 到 Map1，它就保留的 Source 的分区特性，以及分区元素处理的有序性。</p>
<p><strong>Redistributing （重新分配）模式</strong>：这种模式会改变数据的分区数；每个一个 operator subtask 会根据选择 transformation 把数据发送到不同的目标 subtasks,比如 keyBy()会通过 hashcode 重新分区,broadcast()和 rebalance()方法会随机重新分区；</p>
<h3 id="2-3-Task-和-Operator-chain"><a href="#2-3-Task-和-Operator-chain" class="headerlink" title="2.3   Task 和 Operator chain"></a>2.3   Task 和 Operator chain</h3><p>Flink 的所有操作都称之为 Operator，客户端在提交任务的时候会对 Operator 进行优化操作，能进行合并的 Operator 会被合并为一个 Operator，合并后的 Operator 称为 Operator chain，实际上就是一个执行链，每个执行链会在 TaskManager 上一个独立的线程中执行。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_6.png"
                      alt="img"
                ></p>
<h3 id="2-4-任务调度与执行"><a href="#2-4-任务调度与执行" class="headerlink" title="2.4   任务调度与执行"></a>2.4   任务调度与执行</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_7.png"
                      alt="img"
                ></p>
<h4 id="2-4-1-组件介绍"><a href="#2-4-1-组件介绍" class="headerlink" title="2.4.1  组件介绍"></a>2.4.1  组件介绍</h4><p>Flink 运行时架构主要包括四个不同的组件，它们会在运行流处理应用程序时协同工作：作业管理器（JobManager）、资源管理器（ResourceManager）、任务管理器（TaskManager），以及分发器（Dispatcher）。因为Flink 是用Java 和Scala 实现的，所以所有组件都会运行在Java 虚拟机（JVMs）上。每个组件的职责如下：</p>
<ul>
<li>作业管理器（JobManager）是控制一个应用程序执行的主进程，也就是说，<strong>每个应用程序都会被一个不同的作业管理器所控制执行</strong>。作业管理器会先接收到要执行的应用程序。这个应用程序会包括：作业图（JobGraph）、逻辑数据流图（logical dataflowgraph）和打包了所有的类、库和其它资源的JAR 包。作业管理器会把JobGraph 转换成一个物理层面的数据流图，这个图被叫做“执行图”（ExecutionGraph），包含了所有可以并发执行的任务。作业管理器会向资源管理器（ResourceManager）请求执行任务必要的资源，也就是任务管理器（TaskManager）上的插槽（slot）。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的TaskManager 上。而在运行过程中，<strong>作业管理器会负责所有需要中央协调的操作，比如说检查点（checkpoints）的协调。</strong></li>
<li>ResourceManager 主要负责管理任务管理器（TaskManager）的插槽（slot）<strong>，TaskManger插槽是Flink 中定义的处理资源单元</strong>。Flink 为不同的环境和资源管理工具提供了不同资源管理器（ResourceManager），比如YARN、Mesos、K8s，以及standalone 部署。分配给作业管理器。如果ResourceManager 没有足够的插槽来满足作业管理器的请求，它还可以向资源提供平台发起会话，以提供启动TaskManager 进程的容器。另外，<strong>ResourceManager 还负责终止空闲的TaskManager，释放计算资源。</strong></li>
<li>任务管理器（TaskManager）是Flink 中的工作进程。通常在Flink 中会有多个TaskManager运行，每一个TaskManager 都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager 能够执行的任务数量。启动之后，T<strong>askManager 会向资源管理器注册它的插槽；</strong>收到资源管理器的指令后，TaskManager 就会将一个或者多个插槽提供给作业管理器调用。作业管理器就可以向插槽分配任务（tasks）来执行了。在执行过程中，<strong>一个TaskManager 可以跟其它运行同一应用程序的TaskManager 交换数据</strong>。任务的执行和插槽的概念会在“任务执行”一节做具体讨论。</li>
<li>分发器（Dispatcher）可以跨作业运行，<strong>它为应用提交提供了REST 接口</strong>。当一个应用被提交执行时，分发器就会启动并将应用移交给一个作业管理器。由于是REST接口，所以Dispatcher 可以作为集群的一个HTTP 接入点，这样就能够不受防火墙阻挡。Dispatcher 也会启动一个Web UI，用来方便地展示和监控作业执行的信息。Dispatcher 在架构中可能并不是必需的，这取决于应用提交运行的方式。</li>
<li><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230224140558630.png"
                      alt="image-20230224140558630"
                ></li>
</ul>
<blockquote>
<p>上图是从一个较为高层级的视角，来看应用中各组件的交互协作。如果部署的集群环境不同（例如YARN，Mesos，Kubernetes，standalone 等），其中一些步骤可以被省略，或是有些组件会运行在同一个JVM 进程中。</p>
<p><strong>框架（Framework）方式</strong><br>在这个模式下，Flink 应用被打包成一个Jar 文件，并由客户端提交给一个运行服务（running service）。这个服务可以是一个Flink 的Dispatcher，也可以是一个Flink 的作业管理器，或是Yarn 的ResourceManager。如果application 被提交给一个作业管理器，则它会立即开始执行这个application。 如果application 被提交给了一个Dispatcher，或是YarnResourceManager，则它会启动一个作业管理器，然后将application 交给它，再由作业管理器开始执行此应用。</p>
<p><strong>库（Library）方式</strong><br>在这个模式下，Flink Application 会被打包在一个容器（container）镜像里，例如一个Docker 镜像。此镜像包含了运行作业管理器和ResourceManager 的代码。当一个容器从镜像启动后，它会自动启动ResourceManager 和作业管理器，并提交打包好的应用。另<br>一种方法是：将应用打包到镜像后，只用于部署TaskManager 容器。从镜像启动的容器会自动启动一个TaskManager，然后连接ResourceManager 并注册它的slots。这些镜像的启动以及失败重启，通常都会由一个外部的资源管理器管理（比如Kubernetes）。</p>
<p><strong>框架模式遵循了传统的任务提交方式，从客户端提交到Flink 运行服务。而在库模式下，没有运行的Flink 服务。</strong>它是将Flink 作为一个库，与应用程序一同打包到了一个容器镜像。这种部署方式在微服务架构中较为常见。我们会在“运行管理流式应用程序”一节对这个话题做详细讨论。</p>
</blockquote>
<p>当作业管理器申请插槽资源时，ResourceManager 会将有空闲插槽的TaskManager</p>
<ol>
<li>当 Flink 执行 executor 会自动根据程序代码生成 DAG 数据流图；</li>
<li>ActorSystem 创建 Actor 将数据流图发送给 JobManager 中的 Actor；</li>
<li>JobManager 会不断接收 TaskManager 的心跳消息，从而可以获取到有效的 TaskManager；</li>
<li>JobManager 通过调度器在 TaskManager 中调度执行 Task（在 Flink 中，最小的调度单元就是 task，对应就是一个线程）；</li>
<li>在程序运行过程中，task 与 task 之间是可以进行数据传输的。</li>
</ol>
<p><strong>Job Client</strong>：</p>
<ol>
<li>主要职责是提交任务, 提交后可以结束进程, 也可以等待结果返回；</li>
<li>Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点；</li>
<li>Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户。</li>
</ol>
<p><strong>JobManager</strong>：</p>
<ol>
<li>主要职责是调度工作并协调任务做检查点；</li>
<li>集群中至少要有一个 master，master 负责调度 task，协调 checkpoints 和容错；</li>
<li>高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby；</li>
<li>Job Manager 包含 <strong>Actor System</strong>、<strong>Scheduler</strong>、<strong>CheckPoint</strong> 三个重要的组件；</li>
<li>JobManager 从客户端接收到任务以后, 首先生成优化过的执行计划, 再调度到 TaskManager 中执行。</li>
</ol>
<p><strong>TaskManager</strong>：</p>
<ol>
<li>主要职责是从 JobManager 处接收任务, 并部署和启动任务, 接收上游的数据并处理；</li>
<li>Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点；</li>
<li><strong>TaskManager 在创建之初就设置好了 Slot, 每个 Slot 可以执行一个任务。</strong></li>
</ol>
<h4 id="2-4-2-任务执行流程"><a href="#2-4-2-任务执行流程" class="headerlink" title="2.4.2  任务执行流程"></a>2.4.2  任务执行流程</h4><p>一个TaskManager 可以同时执行多个任务（tasks）。这些任务可以是同一个算子（operator）的子任务（数据并行），也可以是来自不同算子的（任务并行），甚至可以是另一个不同应用程序的（作业并行）。TaskManager 提供了一定数量的处理插槽（processing slots），用于控制可以并行执行的任务数。一个slot 可以执行应用的一个分片，也就是应用中每一个算子的一个并行任务。图3-2 展示了TaskManagers，slots，tasks 以及operators之间的关系：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230224144616085.png"
                      alt="image-20230224144616085"
                ></p>
<p>最左边是一个“作业图”（JobGraph），包含了5 个算子——它是应用程序的非并行表示。其中算子A 和C 是数据源（source），E 是输出端（sink）。C 和E 并行度为2，而其他的算子并行度为4。<strong>因为最高的并行度是4，所以应用需要至少四个slot 来执行任务。</strong></p>
<p>现在有两个TaskManager，每个又各有两个slot，所以我们的需求是满足的。作业管理器将JobGraph 转化为“执行图”（ExecutionGraph），并将任务分配到四个可用的slot 上。对于有4 个并行任务的算子，它的task 会分配到每个slot 上。而对于并行度为2 的operator C 和E，它们的任务被分配到slot 1.1、2.1 以及slot 1.2、2.2。将tasks 调度到slots 上，可以让多个tasks 跑在同一个TaskManager 内，也就可以是的<strong>tasks 之间的数据交换更高效。</strong></p>
<p><strong>然而将太多任务调度到同一个TaskManager 上会导致TaskManager 过载，继而影响效率</strong>。之后我们会在“控制任务调度”一节继续讨论如何控制任务的调度。</p>
<p><strong>TaskManager 在同一个JVM 中以多线程的方式执行任务</strong>。线程较进程会更轻量级，但是线程之间并没有对任务进行严格隔离。所以，<strong>单个任务的异常行为有可能会导致整个TaskManager 进程挂掉，当然也同时包括运行在此进程上的所有任务。</strong>通过为每个TaskManager 配置单独的slot，就可以将应用在TaskManager 上相互隔离开来。<strong>TaskManager内部有多线程并行的机制</strong>，而且在一台主机上可以部署多个TaskManager，所以Flink在资源配置上非常灵活，在部署应用时可以充分权衡性能和资源的隔离。我们将会在第九章对Flink 集群的配置和搭建继续做详细讨论。</p>
<h3 id="2-5-任务槽和槽共享"><a href="#2-5-任务槽和槽共享" class="headerlink" title="2.5   任务槽和槽共享"></a>2.5   任务槽和槽共享</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_8.png"
                      alt="img"
                ></p>
<p>每个 TaskManager 是一个 JVM 的进程, 可以在不同的线程中执行一个或多个子任务。 为了控制一个 worker 能接收多少个 task。worker 通过 task slot 来进行控制（一个 worker 至少有一个 task slot）。</p>
<h4 id="1-任务槽"><a href="#1-任务槽" class="headerlink" title="1) 任务槽"></a>1) 任务槽</h4><p>每个 task slot 表示 TaskManager 拥有资源的一个固定大小的子集。</p>
<p>flink 将进程的内存进行了划分到多个 slot 中。</p>
<p>图中有 2 个 TaskManager，每个 TaskManager 有 3 个 slot 的，每个 slot 占有 1&#x2F;3 的内存。</p>
<p>内存被划分到不同的 slot 之后可以获得如下好处:</p>
<ul>
<li><strong>TaskManager 最多能同时并发执行的任务是可以控制的，那就是 3 个，因为不能超过 slot 的数量。</strong></li>
<li><strong>slot 有独占的内存空间，这样在一个 TaskManager 中可以运行多个不同的作业，作业之间不受影响。</strong></li>
</ul>
<h4 id="2-槽共享"><a href="#2-槽共享" class="headerlink" title="2) 槽共享"></a>2) 槽共享</h4><p>默认情况下，<strong>Flink 允许子任务共享插槽，即使它们是不同任务的子任务，只要它们来自同一个作业。</strong>结果是一个槽可以保存作业的整个管道。允许插槽共享有两个主要好处：</p>
<ul>
<li>只需计算 Job 中最高并行度（parallelism）的 task slot,只要这个满足，其他的 job 也都能满足。</li>
<li>资源分配更加公平，如果有比较空闲的 slot 可以将更多的任务分配给它。图中若没有任务槽共享，负载不高的 Source&#x2F;Map 等 subtask 将会占据许多资源，而负载较高的窗口 subtask 则会缺乏资源。</li>
<li>有了任务槽共享，可以将基本并行度（base parallelism）从 2 提升到 6.提高了分槽资源的利用率。同时它还可以保障 TaskManager 给 subtask 的分配的 slot 方案更加公平。</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_9.png"
                      alt="img"
                ></p>
<h3 id="2-6-Flink的数据传输"><a href="#2-6-Flink的数据传输" class="headerlink" title="2.6  Flink的数据传输"></a>2.6  Flink的数据传输</h3><h4 id="2-6-1-基于信任度的流控制"><a href="#2-6-1-基于信任度的流控制" class="headerlink" title="2.6.1  基于信任度的流控制"></a>2.6.1  基于信任度的流控制</h4><p><strong>通过网络连接来发送每条数据的效率很低，会导致很大的开销。</strong>为了充分利用网络连接的带宽，就需要进行<strong>缓冲</strong>了。在流处理的上下文中，缓冲的<strong>一个缺点是会增加延迟</strong>，因为数据需要在缓冲区中进行收集，而不是立即发送。</p>
<p>Flink 实现了一个基于信任度的流量控制机制，其工作原理如下。接收任务授予发送任务一些“信任度”（credit），也就是为了接收其数据而保留的网络缓冲区数。当发送者收到一个信任度通知，它就会<strong>按照被授予的信任度，发送尽可能多的缓冲数据</strong>，并且同时<strong>发送目前积压数据的大小</strong>——也就是已填满并准备发送的网络缓冲的数量。接收者用保留的缓冲区处理发来的数据，并对<strong>发送者传来的积压量进行综合考量，为其所有连接的发送者确定下一个信用度授权的优先级。</strong></p>
<p>基于信用度的流控制可以减少延迟，因为<strong>发送者可以在接收者有足够的资源接受数据时立即发送数据</strong>。此外，在数据倾斜的情况下，这样分配网络资源是一种很有效的机制，因为<strong>信用度是根据发送者积压数据量的规模授予的</strong>。因此，基于信用的流量控制是Flink 实现高吞吐量和低延迟的重要组成部分。</p>
<h4 id="2-6-2-任务链"><a href="#2-6-2-任务链" class="headerlink" title="2.6.2  任务链"></a>2.6.2  任务链</h4><p>Flink 采用了一种称为任务链的优化技术，可以在特定条件下<strong>减少本地通信的开销</strong>。为了满足任务链的要求，必须将两个或多个算子设为相同的并行度，并通过<strong>本地转发（local forward）的方式进行连接</strong>。图3-5 所示的算子管道满足这些要求。它由三个算子组成，这些算子的任务并行度都被设为2，并且通过本地转发方式相连接。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230227215817730.png"
                      alt="image-20230227215817730"
                ></p>
<p>图3-6 展示了管道以任务链方式运行的过程。算子的函数被融合成了一个单一的任务，由一个线程执行。<strong>由函数生成的数据通过一个简单的方法调用移交给下一个函数；这样在函数之间直接传递数据，基本上没有序列化和通信成本。</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230227215835695.png"
                      alt="image-20230227215835695"
                ></p>
<p><strong>任务链可以显著降低本地任务之间的通信成本</strong>，但也有一些场景，在没有链接的情况下运行管道操作是有意义的。例如，如果<strong>任务链中某个函数执行的开销巨大</strong>，那就可以将一条长的任务链管道断开，或者将一条链断开为两个任务，从而可以将这个开销大的函数调度到不同的槽（slots）中。图3-7 显示了在没有任务链的情况下相同管道操作的执行情况。所有函数都由独立的单个任务来评估，每个任务都在专有的线程中运行。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230227215856689.png"
                      alt="image-20230227215856689"
                ></p>
<p>任务链在Flink 中默认会启用。在“控制任务链”一节中，我们展示了如何禁用应用程序的任务链，以及如何控制各个算子的链接行为。</p>
<h2 id="3-DataSet算子大全"><a href="#3-DataSet算子大全" class="headerlink" title="3  DataSet算子大全"></a>3  DataSet算子大全</h2><p>Flink和Spark类似，也是一种一站式处理的框架；既可以进行批处理（DataSet），也可以进行实时处理（DataStream）。</p>
<p>所以下面将Flink的算子分为两大类：一类是DataSet，一类是DataStream。</p>
<h3 id="3-1-source算子"><a href="#3-1-source算子" class="headerlink" title="3.1  source算子"></a>3.1  source算子</h3><h4 id="1-fromCollection"><a href="#1-fromCollection" class="headerlink" title="1. fromCollection"></a>1. fromCollection</h4><p>fromCollection：从本地集合读取数据</p>
<p>例：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> textDataSet: <span class="type">DataSet</span>[<span class="type">String</span>] = env.fromCollection(</span><br><span class="line">  <span class="type">List</span>(<span class="string">&quot;1,张三&quot;</span>, <span class="string">&quot;2,李四&quot;</span>, <span class="string">&quot;3,王五&quot;</span>, <span class="string">&quot;4,赵六&quot;</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.fromElements(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello world&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<h4 id="2-readTextFile"><a href="#2-readTextFile" class="headerlink" title="2. readTextFile"></a>2. readTextFile</h4><p>readTextFile：从文件中读取</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textDataSet: <span class="type">DataSet</span>[<span class="type">String</span>]  = env.readTextFile(<span class="string">&quot;/data/a.txt&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<h4 id="3-readTextFile：遍历目录"><a href="#3-readTextFile：遍历目录" class="headerlink" title="3. readTextFile：遍历目录"></a>3. readTextFile：遍历目录</h4><p>readTextFile可以对一个文件目录内的所有文件，包括所有子目录中的所有文件的遍历访问方式</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> parameters = <span class="keyword">new</span> <span class="type">Configuration</span></span><br><span class="line"><span class="comment">// recursive.file.enumeration 开启递归</span></span><br><span class="line">parameters.setBoolean(<span class="string">&quot;recursive.file.enumeration&quot;</span>, <span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> file = env.readTextFile(<span class="string">&quot;/data&quot;</span>).withParameters(parameters)</span><br></pre></td></tr></table></figure></div>

<h4 id="4-readTextFile：读取压缩文件"><a href="#4-readTextFile：读取压缩文件" class="headerlink" title="4. readTextFile：读取压缩文件"></a>4. readTextFile：读取压缩文件</h4><p>对于以下压缩类型，不需要指定任何额外的inputformat方法，flink可以自动识别并且解压。但是，压缩文件可能不会并行读取，可能是顺序读取的，这样可能会影响作业的可伸缩性。</p>
<table>
<thead>
<tr>
<th align="center">压缩方法</th>
<th align="center">文件扩展名</th>
<th align="center">是否可并行读取</th>
</tr>
</thead>
<tbody><tr>
<td align="center">DEFLATE</td>
<td align="center">.deflate</td>
<td align="center">no</td>
</tr>
<tr>
<td align="center">GZip</td>
<td align="center">.gz .gzip</td>
<td align="center">no</td>
</tr>
<tr>
<td align="center">Bzip2</td>
<td align="center">.bz2</td>
<td align="center">no</td>
</tr>
<tr>
<td align="center">XZ</td>
<td align="center">.xz</td>
<td align="center">no</td>
</tr>
</tbody></table>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> file = env.readTextFile(<span class="string">&quot;/data/file.gz&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="3-2-Transform转换算子"><a href="#3-2-Transform转换算子" class="headerlink" title="3.2  Transform转换算子"></a>3.2  Transform转换算子</h3><p>因为Transform算子基于Source算子操作，所以首先构建Flink执行环境及Source算子，后续Transform算子操作基于此：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> textDataSet: <span class="type">DataSet</span>[<span class="type">String</span>] = env.fromCollection(</span><br><span class="line">  <span class="type">List</span>(<span class="string">&quot;张三,1&quot;</span>, <span class="string">&quot;李四,2&quot;</span>, <span class="string">&quot;王五,3&quot;</span>, <span class="string">&quot;张三,4&quot;</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>

<h4 id="1-map"><a href="#1-map" class="headerlink" title="1. map"></a>1. map</h4><p>将DataSet中的每一个元素转换为另外一个元素</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用map将List转换为一个Scala的样例类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, id: <span class="type">String</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> userDataSet: <span class="type">DataSet</span>[<span class="type">User</span>] = textDataSet.map &#123;</span><br><span class="line">  text =&gt;</span><br><span class="line">    <span class="keyword">val</span> fieldArr = text.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    <span class="type">User</span>(fieldArr(<span class="number">0</span>), fieldArr(<span class="number">1</span>))</span><br><span class="line">&#125;</span><br><span class="line">userDataSet.print()</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//匿名函数的方式</span></span><br><span class="line">env</span><br><span class="line">        .addSource(<span class="keyword">new</span> <span class="title class_">SourceFunction</span>&lt;Integer&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">running</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">private</span> <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;Integer&gt; ctx)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> (running) &#123;</span><br><span class="line">                    ctx.collect(random.nextInt(<span class="number">1000</span>));</span><br><span class="line">                    Thread.sleep(<span class="number">1000L</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">                running = <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//lambada表达式的形式，java8推断不出来返回值类型,因此需要</span></span><br><span class="line">        .map(r -&gt; Tuple2.of(r,r))</span><br><span class="line">        <span class="comment">//会被擦出成Tuple2&lt;Object,Object&gt;</span></span><br><span class="line">        <span class="comment">//需要returns方法来标注一下map函数的输出类型</span></span><br><span class="line">        .returns(Types.TUPLE(Types.INT,Types.INT))</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//匿名内部类的方式</span></span><br><span class="line">env</span><br><span class="line">        .addSource(<span class="keyword">new</span> <span class="title class_">SourceFunction</span>&lt;Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">running</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">private</span> <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;Integer&gt; ctx)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> (running) &#123;</span><br><span class="line">                    ctx.collect(random.nextInt(<span class="number">1000</span>));</span><br><span class="line">                    Thread.sleep(<span class="number">1000L</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">                running = <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .map(<span class="keyword">new</span> <span class="title class_">MapFunction</span>&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">map</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(value, value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line"><span class="comment">//flatMap的方式</span></span><br><span class="line">env</span><br><span class="line">        .addSource(<span class="keyword">new</span> <span class="title class_">SourceFunction</span>&lt;Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">running</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">private</span> <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;Integer&gt; ctx)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> (running) &#123;</span><br><span class="line">                    ctx.collect(random.nextInt(<span class="number">1000</span>));</span><br><span class="line">                    Thread.sleep(<span class="number">1000L</span>);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">                running = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(Integer value, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                collector.collect((Tuple2.of(value,value)));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure></div>

<h4 id="2-flatMap"><a href="#2-flatMap" class="headerlink" title="2. flatMap"></a>2. flatMap</h4><p>将DataSet中的每一个元素转换为0…n个元素。</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用flatMap操作，将集合中的数据：</span></span><br><span class="line"><span class="comment">// 根据第一个元素，进行分组</span></span><br><span class="line"><span class="comment">// 根据第二个元素，进行聚合求值 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = textDataSet.flatMap(line =&gt; line)</span><br><span class="line">      .groupBy(<span class="number">0</span>) <span class="comment">// 根据第一个元素，进行分组</span></span><br><span class="line">      .sum(<span class="number">1</span>) <span class="comment">// 根据第二个元素，进行聚合求值</span></span><br><span class="line">      </span><br><span class="line">result.print()</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">        .flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(String value, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">if</span>(value.equals(<span class="string">&quot;white&quot;</span>))&#123;</span><br><span class="line">                    collector.collect(value);</span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span>(value.equals(<span class="string">&quot;black&quot;</span>))&#123;</span><br><span class="line">                    collector.collect(value);</span><br><span class="line">                    collector.collect(value);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line">stream.flatMap(</span><br><span class="line">        (String value,Collector&lt;String&gt; collector) -&gt;&#123;</span><br><span class="line">            <span class="keyword">if</span>(value.equals(<span class="string">&quot;white&quot;</span>))&#123;</span><br><span class="line">                collector.collect(value);</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(value.equals(<span class="string">&quot;black&quot;</span>))&#123;</span><br><span class="line">                collector.collect(value);</span><br><span class="line">                collector.collect(value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">)</span><br><span class="line">        .returns(Types.STRING);</span><br></pre></td></tr></table></figure></div>



<h4 id="3-mapPartition"><a href="#3-mapPartition" class="headerlink" title="3. mapPartition"></a>3. mapPartition</h4><p>将一个分区中的元素转换为另一个元素</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用mapPartition操作，将List转换为一个scala的样例类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, id: <span class="type">String</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataSet</span>[<span class="type">User</span>] = textDataSet.mapPartition(line =&gt; &#123;</span><br><span class="line">      line.map(index =&gt; <span class="type">User</span>(index._1, index._2))</span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line">result.print()</span><br></pre></td></tr></table></figure></div>

<h4 id="4-filter"><a href="#4-filter" class="headerlink" title="4. filter"></a>4. filter</h4><p>过滤出来一些符合条件的元素，返回<strong>boolean值为true</strong>的元素</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> source: <span class="type">DataSet</span>[<span class="type">String</span>] = env.fromElements(<span class="string">&quot;java&quot;</span>, <span class="string">&quot;scala&quot;</span>, <span class="string">&quot;java&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> filter:<span class="type">DataSet</span>[<span class="type">String</span>] = source.filter(line =&gt; line.contains(<span class="string">&quot;java&quot;</span>))<span class="comment">//过滤出带java的数据</span></span><br><span class="line">filter.print()</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">        stream.filter(r -&gt; r.user.equals(<span class="string">&quot;Mary&quot;</span>)).print();</span><br><span class="line"></span><br><span class="line">        stream</span><br><span class="line">                .filter(<span class="keyword">new</span> <span class="title class_">FilterFunction</span>&lt;Example1.Event&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">filter</span><span class="params">(Example1.Event value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="keyword">return</span> value.user.equals(<span class="string">&quot;Mary&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br><span class="line">stream</span><br><span class="line">                .flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;Example1.Event, Example1.Event&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(Example1.Event value, Collector&lt;Example1.Event&gt; collector)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="keyword">if</span>(value.user.equals(<span class="string">&quot;Mary&quot;</span>)) collector.collect(value);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure></div>

<h4 id="5-reduce"><a href="#5-reduce" class="headerlink" title="5. reduce"></a>5. reduce</h4><p>可以对一个dataset或者一个group来进行聚合计算，最终<strong>聚合成一个元素</strong></p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 fromElements 构建数据源</span></span><br><span class="line"><span class="keyword">val</span> source = env.fromElements((<span class="string">&quot;java&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;scala&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;java&quot;</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment">// 使用map转换成DataSet元组</span></span><br><span class="line"><span class="keyword">val</span> mapData: <span class="type">DataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = source.map(line =&gt; line)</span><br><span class="line"><span class="comment">// 根据首个元素分组</span></span><br><span class="line"><span class="keyword">val</span> groupData = mapData.groupBy(_._1)</span><br><span class="line"><span class="comment">// 使用reduce聚合</span></span><br><span class="line"><span class="keyword">val</span> reduceData = groupData.reduce((x, y) =&gt; (x._1, x._2 + y._2))</span><br><span class="line"><span class="comment">// 打印测试</span></span><br><span class="line">reduceData.print()</span><br></pre></td></tr></table></figure></div>

<h4 id="6-reduceGroup"><a href="#6-reduceGroup" class="headerlink" title="6. reduceGroup"></a>6. reduceGroup</h4><p>将一个dataset或者一个group<strong>聚合成一个或多个元素</strong>。<br>reduceGroup是reduce的一种优化方案；<br>它会先分组reduce，然后在做整体的reduce；这样做的好处就是可以减少网络IO</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 fromElements 构建数据源</span></span><br><span class="line"><span class="keyword">val</span> source: <span class="type">DataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = env.fromElements((<span class="string">&quot;java&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;scala&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;java&quot;</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment">// 根据首个元素分组</span></span><br><span class="line"><span class="keyword">val</span> groupData = source.groupBy(_._1)</span><br><span class="line"><span class="comment">// 使用reduceGroup聚合</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = groupData.reduceGroup &#123;</span><br><span class="line">      (in: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">Int</span>)], out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Int</span>)]) =&gt;</span><br><span class="line">        <span class="keyword">val</span> tuple = in.reduce((x, y) =&gt; (x._1, x._2 + y._2))</span><br><span class="line">        out.collect(tuple)</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 打印测试</span></span><br><span class="line">result.print()</span><br></pre></td></tr></table></figure></div>

<h4 id="7-minBy和maxBy"><a href="#7-minBy和maxBy" class="headerlink" title="7. minBy和maxBy"></a>7. minBy和maxBy</h4><p>选择具有最小值或最大值的<strong>元素</strong></p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用minBy操作，求List中每个人的最小值</span></span><br><span class="line"><span class="comment">// List(&quot;张三,1&quot;, &quot;李四,2&quot;, &quot;王五,3&quot;, &quot;张三,4&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, id: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="comment">// 将List转换为一个scala的样例类</span></span><br><span class="line"><span class="keyword">val</span> text: <span class="type">DataSet</span>[<span class="type">User</span>] = textDataSet.mapPartition(line =&gt; &#123;</span><br><span class="line">      line.map(index =&gt; <span class="type">User</span>(index._1, index._2))</span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">val</span> result = text</span><br><span class="line">          .groupBy(<span class="number">0</span>) <span class="comment">// 按照姓名分组</span></span><br><span class="line">          .minBy(<span class="number">1</span>)   <span class="comment">// 每个人的最小值</span></span><br></pre></td></tr></table></figure></div>

<h4 id="8-Aggregate"><a href="#8-Aggregate" class="headerlink" title="8. Aggregate"></a>8. Aggregate</h4><p>在数据集上进行聚合求<strong>最值</strong>（最大值、最小值）</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Double</span>)]</span><br><span class="line">    data.+=((<span class="number">1</span>, <span class="string">&quot;yuwen&quot;</span>, <span class="number">89.0</span>))</span><br><span class="line">    data.+=((<span class="number">2</span>, <span class="string">&quot;shuxue&quot;</span>, <span class="number">92.2</span>))</span><br><span class="line">    data.+=((<span class="number">3</span>, <span class="string">&quot;yuwen&quot;</span>, <span class="number">89.99</span>))</span><br><span class="line"><span class="comment">// 使用 fromElements 构建数据源</span></span><br><span class="line"><span class="keyword">val</span> input: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Double</span>)] = env.fromCollection(data)</span><br><span class="line"><span class="comment">// 使用group执行分组操作</span></span><br><span class="line"><span class="keyword">val</span> value = input.groupBy(<span class="number">1</span>)</span><br><span class="line">            <span class="comment">// 使用aggregate求最大值元素</span></span><br><span class="line">            .aggregate(<span class="type">Aggregations</span>.<span class="type">MAX</span>, <span class="number">2</span>) </span><br><span class="line"><span class="comment">// 打印测试</span></span><br><span class="line">value.print()       </span><br></pre></td></tr></table></figure></div>

<p><strong>Aggregate只能作用于元组上</strong></p>
<blockquote>
<p>注意：<br>要使用aggregate，只能使用字段索引名或索引名称来进行分组 <code>groupBy(0)</code> ，否则会报一下错误:<br>Exception in thread “main” java.lang.UnsupportedOperationException: Aggregate does not support grouping with KeySelector functions, yet.</p>
</blockquote>
<div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">        .keyBy(r -&gt; r.user)</span><br><span class="line">        .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">        .aggregate(<span class="keyword">new</span> <span class="title class_">CountAgg</span>())</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 实现AggregateFunction&lt;IN,累加器,out&gt;接口</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">CountAgg</span> <span class="keyword">implements</span> <span class="title class_">AggregateFunction</span>&lt;Event,Integer,Integer&gt; &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建累加器</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Integer <span class="title function_">createAccumulator</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定义累加规则</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Integer <span class="title function_">add</span><span class="params">(Event event, Integer accumulator)</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> accumulator+<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//在窗口关闭时返回结果</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Integer <span class="title function_">getResult</span><span class="params">(Integer accumulator)</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> accumulator;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//在窗口合并的时候merge</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Integer <span class="title function_">merge</span><span class="params">(Integer integer, Integer acc1)</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="9-distinct"><a href="#9-distinct" class="headerlink" title="9. distinct"></a>9. distinct</h4><p>去除重复的数据</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 数据源使用上一题的</span></span><br><span class="line"><span class="comment">// 使用distinct操作，根据科目去除集合中重复的元组数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> value: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Double</span>)] = input.distinct(<span class="number">1</span>)</span><br><span class="line">value.print()</span><br></pre></td></tr></table></figure></div>

<h4 id="10-first"><a href="#10-first" class="headerlink" title="10. first"></a>10. first</h4><p>取前N个数</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input.first(<span class="number">2</span>) <span class="comment">// 取前两个数</span></span><br></pre></td></tr></table></figure></div>

<h4 id="11-join"><a href="#11-join" class="headerlink" title="11. join"></a>11. join</h4><p>将两个DataSet按照一定条件连接到一起，形成新的DataSet</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// s1 和 s2 数据集格式如下：</span></span><br><span class="line"><span class="comment">// DataSet[(Int, String,String, Double)]</span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> joinData = s1.join(s2)  <span class="comment">// s1数据集 join s2数据集</span></span><br><span class="line">             .where(<span class="number">0</span>).equalTo(<span class="number">0</span>) &#123;     <span class="comment">// join的条件</span></span><br><span class="line">      (s1, s2) =&gt; (s1._1, s1._2, s2._2, s1._3)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="12-leftOuterJoin"><a href="#12-leftOuterJoin" class="headerlink" title="12. leftOuterJoin"></a>12. leftOuterJoin</h4><p>左外连接,左边的Dataset中的每一个元素，去连接右边的元素</p>
<p>此外还有：</p>
<p>rightOuterJoin：右外连接,左边的Dataset中的每一个元素，去连接左边的元素</p>
<p>fullOuterJoin：全外连接,左右两边的元素，全部连接</p>
<p>下面以 leftOuterJoin 进行示例：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> data1 = <span class="type">ListBuffer</span>[<span class="type">Tuple2</span>[<span class="type">Int</span>,<span class="type">String</span>]]()</span><br><span class="line">    data1.append((<span class="number">1</span>,<span class="string">&quot;zhangsan&quot;</span>))</span><br><span class="line">    data1.append((<span class="number">2</span>,<span class="string">&quot;lisi&quot;</span>))</span><br><span class="line">    data1.append((<span class="number">3</span>,<span class="string">&quot;wangwu&quot;</span>))</span><br><span class="line">    data1.append((<span class="number">4</span>,<span class="string">&quot;zhaoliu&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> data2 = <span class="type">ListBuffer</span>[<span class="type">Tuple2</span>[<span class="type">Int</span>,<span class="type">String</span>]]()</span><br><span class="line">    data2.append((<span class="number">1</span>,<span class="string">&quot;beijing&quot;</span>))</span><br><span class="line">    data2.append((<span class="number">2</span>,<span class="string">&quot;shanghai&quot;</span>))</span><br><span class="line">    data2.append((<span class="number">4</span>,<span class="string">&quot;guangzhou&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> text1 = env.fromCollection(data1)</span><br><span class="line"><span class="keyword">val</span> text2 = env.fromCollection(data2)</span><br><span class="line"></span><br><span class="line">text1.leftOuterJoin(text2).where(<span class="number">0</span>).equalTo(<span class="number">0</span>).apply((first,second)=&gt;&#123;</span><br><span class="line">      <span class="keyword">if</span>(second==<span class="literal">null</span>)&#123;</span><br><span class="line">        (first._1,first._2,<span class="string">&quot;null&quot;</span>)</span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        (first._1,first._2,second._2)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).print()</span><br></pre></td></tr></table></figure></div>

<h4 id="13-cross"><a href="#13-cross" class="headerlink" title="13. cross"></a>13. cross</h4><p>交叉操作，通过形成这个数据集和其他数据集的笛卡尔积，创建一个新的数据集</p>
<p>和join类似，但是这种交叉操作会产生笛卡尔积，在<strong>数据比较大的时候，是非常消耗内存的操作</strong></p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> cross = input1.cross(input2)&#123;</span><br><span class="line">      (input1 , input2) =&gt; (input1._1,input1._2,input1._3,input2._2)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">cross.print()</span><br></pre></td></tr></table></figure></div>

<h4 id="14-union"><a href="#14-union" class="headerlink" title="14. union"></a>14. union</h4><p>联合操作，创建包含来自该数据集和其他数据集的元素的新数据集,<strong>不会去重</strong></p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> unionData: <span class="type">DataSet</span>[<span class="type">String</span>] = elements1.union(elements2).union(elements3)</span><br><span class="line"><span class="comment">// 去除重复数据</span></span><br><span class="line"><span class="keyword">val</span> value = unionData.distinct(line =&gt; line)</span><br></pre></td></tr></table></figure></div>

<h4 id="15-rebalance-shuffle-broadcast"><a href="#15-rebalance-shuffle-broadcast" class="headerlink" title="15. rebalance  shuffle  broadcast"></a>15. rebalance  shuffle  broadcast</h4><p>Flink也有数据倾斜的时候，比如当前有数据量大概10亿条数据需要处理，在处理过程中可能会发生如图所示的状况：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210311_1.png"
                      alt="img"
                ></p>
<p>这个时候本来总体数据量只需要10分钟解决的问题，出现了数据倾斜，机器1上的任务需要4个小时才能完成，那么其他3台机器执行完毕也要等待机器1执行完毕后才算整体将任务完成； 所以在实际的工作中，出现这种情况比较好的解决方案就是接下来要介绍的—<strong>rebalance</strong>（内部使用round robin方法将数据均匀打散。这对于数据倾斜时是很好的选择。）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210311_2.png"
                      alt="img"
                ></p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用rebalance操作，避免数据倾斜</span></span><br><span class="line"><span class="keyword">val</span> rebalance = filterData.rebalance()</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//随机向下游分配</span></span><br><span class="line">        env</span><br><span class="line">                .fromElements(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .shuffle()</span><br><span class="line">                .print(<span class="string">&quot;shuffle:&quot;</span>).setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//平均分配  -底层采用轮询的方式</span></span><br><span class="line">        env</span><br><span class="line">                .fromElements(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .rebalance()</span><br><span class="line">                .print(<span class="string">&quot;rebalance:&quot;</span>).setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//广播的方式  -两个分区均有复制</span></span><br><span class="line">        env</span><br><span class="line">                .fromElements(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .broadcast()</span><br><span class="line">                .print(<span class="string">&quot;broadcast:&quot;</span>).setParallelism(<span class="number">2</span>);</span><br></pre></td></tr></table></figure></div>

<h4 id="16-partitionByHash"><a href="#16-partitionByHash" class="headerlink" title="16. partitionByHash"></a>16. partitionByHash</h4><p>按照指定的key进行hash分区</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">Int</span>, <span class="type">Long</span>, <span class="type">String</span>)]</span><br><span class="line">data.+=((<span class="number">1</span>, <span class="number">1</span>L, <span class="string">&quot;Hi&quot;</span>))</span><br><span class="line">data.+=((<span class="number">2</span>, <span class="number">2</span>L, <span class="string">&quot;Hello&quot;</span>))</span><br><span class="line">data.+=((<span class="number">3</span>, <span class="number">2</span>L, <span class="string">&quot;Hello world&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> collection = env.fromCollection(data)</span><br><span class="line"><span class="keyword">val</span> unique = collection.partitionByHash(<span class="number">1</span>).mapPartition&#123;</span><br><span class="line">  line =&gt;</span><br><span class="line">    line.map(x =&gt; (x._1 , x._2 , x._3))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">unique.writeAsText(<span class="string">&quot;hashPartition&quot;</span>, <span class="type">WriteMode</span>.<span class="type">NO_OVERWRITE</span>)</span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure></div>

<h4 id="17-partitionByRange"><a href="#17-partitionByRange" class="headerlink" title="17. partitionByRange"></a>17. partitionByRange</h4><p>根据指定的key对数据集进行范围分区</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">Int</span>, <span class="type">Long</span>, <span class="type">String</span>)]</span><br><span class="line">data.+=((<span class="number">1</span>, <span class="number">1</span>L, <span class="string">&quot;Hi&quot;</span>))</span><br><span class="line">data.+=((<span class="number">2</span>, <span class="number">2</span>L, <span class="string">&quot;Hello&quot;</span>))</span><br><span class="line">data.+=((<span class="number">3</span>, <span class="number">2</span>L, <span class="string">&quot;Hello world&quot;</span>))</span><br><span class="line">data.+=((<span class="number">4</span>, <span class="number">3</span>L, <span class="string">&quot;Hello world, how are you?&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> collection = env.fromCollection(data)</span><br><span class="line"><span class="keyword">val</span> unique = collection.partitionByRange(x =&gt; x._1).mapPartition(line =&gt; line.map&#123;</span><br><span class="line">  x=&gt;</span><br><span class="line">    (x._1 , x._2 , x._3)</span><br><span class="line">&#125;)</span><br><span class="line">unique.writeAsText(<span class="string">&quot;rangePartition&quot;</span>, <span class="type">WriteMode</span>.<span class="type">OVERWRITE</span>)</span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure></div>

<h4 id="18-sortPartition"><a href="#18-sortPartition" class="headerlink" title="18. sortPartition"></a>18. sortPartition</h4><p>根据指定的字段值进行分区的排序</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">Int</span>, <span class="type">Long</span>, <span class="type">String</span>)]</span><br><span class="line">    data.+=((<span class="number">1</span>, <span class="number">1</span>L, <span class="string">&quot;Hi&quot;</span>))</span><br><span class="line">    data.+=((<span class="number">2</span>, <span class="number">2</span>L, <span class="string">&quot;Hello&quot;</span>))</span><br><span class="line">    data.+=((<span class="number">3</span>, <span class="number">2</span>L, <span class="string">&quot;Hello world&quot;</span>))</span><br><span class="line">    data.+=((<span class="number">4</span>, <span class="number">3</span>L, <span class="string">&quot;Hello world, how are you?&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = env.fromCollection(data)</span><br><span class="line">    <span class="keyword">val</span> result = ds</span><br><span class="line">      .map &#123; x =&gt; x &#125;.setParallelism(<span class="number">2</span>)</span><br><span class="line">      .sortPartition(<span class="number">1</span>, <span class="type">Order</span>.<span class="type">DESCENDING</span>)<span class="comment">//第一个参数代表按照哪个字段进行分区</span></span><br><span class="line">      .mapPartition(line =&gt; line)</span><br><span class="line">      .collect()</span><br><span class="line"></span><br><span class="line">println(result)</span><br></pre></td></tr></table></figure></div>

<h3 id="3-3-Sink算子"><a href="#3-3-Sink算子" class="headerlink" title="3.3  Sink算子"></a>3.3  Sink算子</h3><h4 id="1-collect"><a href="#1-collect" class="headerlink" title="1. collect"></a>1. collect</h4><p>将数据输出到本地集合</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result.collect()</span><br></pre></td></tr></table></figure></div>

<h4 id="2-writeAsText"><a href="#2-writeAsText" class="headerlink" title="2. writeAsText"></a>2. writeAsText</h4><p>将数据输出到文件</p>
<p>Flink支持多种存储设备上的文件，包括本地文件，hdfs文件等</p>
<p>Flink支持多种文件的存储格式，包括text文件，CSV文件等</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将数据写入本地文件</span></span><br><span class="line">result.writeAsText(<span class="string">&quot;/data/a&quot;</span>, <span class="type">WriteMode</span>.<span class="type">OVERWRITE</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据写入HDFS</span></span><br><span class="line">result.writeAsText(<span class="string">&quot;hdfs://node01:9000/data/a&quot;</span>, <span class="type">WriteMode</span>.<span class="type">OVERWRITE</span>)</span><br></pre></td></tr></table></figure></div>

<h2 id="4-DataStream算子大全"><a href="#4-DataStream算子大全" class="headerlink" title="4  DataStream算子大全"></a>4  DataStream算子大全</h2><p>和DataSet一样，DataStream也包括一系列的Transformation操作</p>
<h3 id="4-1-Source算子"><a href="#4-1-Source算子" class="headerlink" title="4.1  Source算子"></a>4.1  Source算子</h3><p>Flink可以使用 StreamExecutionEnvironment.addSource(source) 来为我们的程序添加数据来源。<br>Flink 已经提供了若干实现好了的 source functions，当然我们也可以通过实现 SourceFunction 来自定义非并行的source或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source。</p>
<p>Flink在流处理上的source和在批处理上的source基本一致。大致有4大类：</p>
<ul>
<li>基于<strong>本地集合</strong>的source（Collection-based-source）</li>
<li>基于<strong>文件</strong>的source（File-based-source）- 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回</li>
<li>基于<strong>网络套接字</strong>的source（Socket-based-source）- 从 socket 读取。元素可以用分隔符切分。</li>
<li><strong>自定义</strong>的source（Custom-source）</li>
</ul>
<p>下面使用addSource将Kafka数据写入Flink为例：</p>
<p>如果需要外部数据源对接，可使用addSource，如将Kafka数据写入Flink， 先引入依赖：</p>
<div class="highlight-container" data-rel="Xml"><figure class="iseeu highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

<p>将Kafka数据写入Flink：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>)</span><br><span class="line">properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)</span><br><span class="line">properties.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>)</span><br><span class="line">properties.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>)</span><br><span class="line">properties.setProperty(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;latest&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> source = env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>](<span class="string">&quot;sensor&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br></pre></td></tr></table></figure></div>

<p>基于网络套接字的：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> source = env.socketTextStream(<span class="string">&quot;IP&quot;</span>, <span class="type">PORT</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="4-2-Transform转换算子"><a href="#4-2-Transform转换算子" class="headerlink" title="4.2  Transform转换算子"></a>4.2  Transform转换算子</h3><h4 id="1-map-1"><a href="#1-map-1" class="headerlink" title="1. map"></a>1. map</h4><p>将DataSet中的每一个元素转换为另外一个元素</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.map &#123; x =&gt; x * <span class="number">2</span> &#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="2-FlatMap"><a href="#2-FlatMap" class="headerlink" title="2. FlatMap"></a>2. FlatMap</h4><p>采用一个数据元并生成零个，一个或多个数据元。将句子分割为单词的flatmap函数</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.flatMap &#123; str =&gt; str.split(<span class="string">&quot; &quot;</span>) &#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="3-Filter"><a href="#3-Filter" class="headerlink" title="3. Filter"></a>3. Filter</h4><p>计算每个数据元的布尔函数，并保存函数返回true的数据元。过滤掉零值的过滤器</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.filter &#123; _ != <span class="number">0</span> &#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="4-KeyBy"><a href="#4-KeyBy" class="headerlink" title="4. KeyBy"></a>4. <strong>KeyBy</strong></h4><p>逻辑上将流分区为不相交的分区。具有相同Keys的所有记录都分配给同一分区。在内部，keyBy（）是使用散列分区实现的。指定键有不同的方法。</p>
<p>此转换返回KeyedStream，其中包括使用被Keys化状态所需的KeyedStream。</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">0</span>) </span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//键控流</span></span><br><span class="line">KeyedStream&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt; keyedStream = stream.keyBy(r -&gt; r.f0);</span><br><span class="line"></span><br><span class="line">keyedStream.sum(<span class="number">1</span>).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//reduce是以上几组算子的泛化实现</span></span><br><span class="line">keyedStream.reduce(<span class="keyword">new</span> <span class="title class_">ReduceFunction</span>&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">reduce</span><span class="params">(Tuple2&lt;Integer, Integer&gt; value1, Tuple2&lt;Integer, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> Tuple2.of(value1.f0, value1.f1 + value2.f1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure></div>

<h4 id="5-Reduce"><a href="#5-Reduce" class="headerlink" title="5. Reduce"></a>5. Reduce</h4><p>被Keys化数据流上的“滚动”Reduce。将当前数据元与最后一个Reduce的值组合并发出新值</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyedStream.reduce &#123; _ + _ &#125;  </span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//键控流</span></span><br><span class="line">KeyedStream&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt; keyedStream = stream.keyBy(r -&gt; r.f0);</span><br><span class="line"></span><br><span class="line">keyedStream.sum(<span class="number">1</span>).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//reduce是以上几组算子的泛化实现</span></span><br><span class="line">keyedStream.reduce(<span class="keyword">new</span> <span class="title class_">ReduceFunction</span>&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">reduce</span><span class="params">(Tuple2&lt;Integer, Integer&gt; value1, Tuple2&lt;Integer, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> Tuple2.of(value1.f0, value1.f1 + value2.f1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure></div>



<h4 id="6-Fold"><a href="#6-Fold" class="headerlink" title="6. Fold"></a>6. <strong>Fold</strong></h4><p>具有初始值的被Keys化数据流上的“滚动”折叠。将当前数据元与最后折叠的值组合并发出新值</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result: <span class="type">DataStream</span>[<span class="type">String</span>] =  keyedStream.fold(<span class="string">&quot;start&quot;</span>)((str, i) =&gt; &#123; str + <span class="string">&quot;-&quot;</span> + i &#125;) </span><br><span class="line"></span><br><span class="line"><span class="comment">// 解释：当上述代码应用于序列（1,2,3,4,5）时，输出结果“start-1”，“start-1-2”，“start-1-2-3”，...</span></span><br></pre></td></tr></table></figure></div>

<h4 id="7-Aggregations"><a href="#7-Aggregations" class="headerlink" title="7. Aggregations"></a>7. Aggregations</h4><p>在被Keys化数据流上滚动聚合。min和minBy之间的差异是min返回最小值，而minBy返回该字段中具有最小值的数据元（max和maxBy相同）。</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">keyedStream.sum(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">keyedStream.min(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">keyedStream.max(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">keyedStream.minBy(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">keyedStream.maxBy(<span class="number">0</span>);</span><br></pre></td></tr></table></figure></div>

<h4 id="8-Window"><a href="#8-Window" class="headerlink" title="8. Window"></a>8. <strong>Window</strong></h4><p>可以在已经分区的KeyedStream上定义Windows。Windows根据某些特征（例如，在最后5秒内到达的数据）对每个Keys中的数据进行分组。这里不再对窗口进行详解，有关窗口的完整说明，请查看这篇文章： <a class="link"   target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/S-RmP5OWiGqwn-C_TZNO5A" >Flink 中极其重要的 Time 与 Window 详细解析(opens new window) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">0</span>).window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>))); </span><br></pre></td></tr></table></figure></div>

<h4 id="9-WindowAll"><a href="#9-WindowAll" class="headerlink" title="9. WindowAll"></a>9. <strong>WindowAll</strong></h4><p>Windows可以在常规DataStream上定义。Windows根据某些特征（例如，在最后5秒内到达的数据）对所有流事件进行分组。</p>
<p>注意：在许多情况下，这是非并行转换。所有记录将收集在windowAll 算子的一个任务中。</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.windowAll(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br></pre></td></tr></table></figure></div>

<h4 id="10-Window-Apply"><a href="#10-Window-Apply" class="headerlink" title="10. Window Apply"></a>10. Window Apply</h4><p>将一般函数应用于整个窗口。</p>
<p>注意：如果您正在使用windowAll转换，则需要使用AllWindowFunction。</p>
<p>下面是一个手动求和窗口数据元的函数</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">windowedStream.apply &#123; <span class="type">WindowFunction</span> &#125;</span><br><span class="line"></span><br><span class="line">allWindowedStream.apply &#123; <span class="type">AllWindowFunction</span> &#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="11-Window-Reduce"><a href="#11-Window-Reduce" class="headerlink" title="11. Window Reduce"></a>11. Window Reduce</h4><p>将函数缩减函数应用于窗口并返回缩小的值</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">windowedStream.reduce &#123; _ + _ &#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="12-Window-Fold"><a href="#12-Window-Fold" class="headerlink" title="12. Window Fold"></a>12. Window Fold</h4><p>将函数折叠函数应用于窗口并返回折叠值</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result: <span class="type">DataStream</span>[<span class="type">String</span>] = windowedStream.fold(<span class="string">&quot;start&quot;</span>, (str, i) =&gt; &#123; str + <span class="string">&quot;-&quot;</span> + i &#125;) </span><br><span class="line"></span><br><span class="line"><span class="comment">// 上述代码应用于序列（1,2,3,4,5）时，将序列折叠为字符串“start-1-2-3-4-5”</span></span><br></pre></td></tr></table></figure></div>

<h4 id="13-Union"><a href="#13-Union" class="headerlink" title="13. Union"></a>13. <strong>Union</strong></h4><p>两个或多个数据流的联合，创建包含来自所有流的所有数据元的新流。注意：如果将数据流与自身联合，则会在结果流中获取两次数据元</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.union(otherStream1, otherStream2, ...)</span><br></pre></td></tr></table></figure></div>

<h4 id="14-Window-Join"><a href="#14-Window-Join" class="headerlink" title="14. Window Join"></a>14. <strong>Window Join</strong></h4><p>在给定Keys和公共窗口上连接两个数据流</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataStream.join(otherStream)</span><br><span class="line">    .where(&lt;key selector&gt;).equalTo(&lt;key selector&gt;)</span><br><span class="line">    .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">    .apply (<span class="keyword">new</span> <span class="type">JoinFunction</span> () &#123;...&#125;)</span><br></pre></td></tr></table></figure></div>

<h4 id="15-Interval-Join"><a href="#15-Interval-Join" class="headerlink" title="15. Interval Join"></a>15. Interval Join</h4><p>在给定的时间间隔内使用公共Keys关联两个被Key化的数据流的两个数据元e1和e2，以便e1.timestamp + lowerBound &lt;&#x3D; e2.timestamp &lt;&#x3D; e1.timestamp + upperBound</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">am.intervalJoin(otherKeyedStream)</span><br><span class="line">    .between(<span class="type">Time</span>.milliseconds(<span class="number">-2</span>), <span class="type">Time</span>.milliseconds(<span class="number">2</span>)) </span><br><span class="line">    .upperBoundExclusive(<span class="literal">true</span>) </span><br><span class="line">    .lowerBoundExclusive(<span class="literal">true</span>) </span><br><span class="line">    .process(<span class="keyword">new</span> <span class="type">IntervalJoinFunction</span>() &#123;...&#125;)</span><br></pre></td></tr></table></figure></div>

<h4 id="16-Window-CoGroup"><a href="#16-Window-CoGroup" class="headerlink" title="16. Window CoGroup"></a>16. Window CoGroup</h4><p>在给定Keys和公共窗口上对两个数据流进行Cogroup</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataStream.coGroup(otherStream)</span><br><span class="line">    .where(<span class="number">0</span>).equalTo(<span class="number">1</span>)</span><br><span class="line">    .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">    .apply (<span class="keyword">new</span> <span class="type">CoGroupFunction</span> () &#123;...&#125;)</span><br></pre></td></tr></table></figure></div>

<h4 id="17-Connect"><a href="#17-Connect" class="headerlink" title="17. Connect"></a>17. <strong>Connect</strong></h4><p>“连接”两个保存其类型的数据流。连接允许两个流之间的共享状态</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStream</span>&lt;<span class="type">Integer</span>&gt; someStream = ... <span class="type">DataStream</span>&lt;<span class="type">String</span>&gt; otherStream = ... <span class="type">ConnectedStreams</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; connectedStreams = someStream.connect(otherStream)</span><br><span class="line"></span><br><span class="line"><span class="comment">// ... 代表省略中间操作</span></span><br></pre></td></tr></table></figure></div>

<h4 id="18-CoMap，CoFlatMap"><a href="#18-CoMap，CoFlatMap" class="headerlink" title="18. CoMap，CoFlatMap"></a>18. <strong>CoMap，CoFlatMap</strong></h4><p>类似于连接数据流上的map和flatMap</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">connectedStreams.map(</span><br><span class="line">    (_ : <span class="type">Int</span>) =&gt; <span class="literal">true</span>,</span><br><span class="line">    (_ : <span class="type">String</span>) =&gt; <span class="literal">false</span>)connectedStreams.flatMap(</span><br><span class="line">    (_ : <span class="type">Int</span>) =&gt; <span class="literal">true</span>,</span><br><span class="line">    (_ : <span class="type">String</span>) =&gt; <span class="literal">false</span>)</span><br></pre></td></tr></table></figure></div>

<h4 id="19-Split"><a href="#19-Split" class="headerlink" title="19. Split"></a>19. <strong>Split</strong></h4><p>根据某些标准将流拆分为两个或更多个流</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> split = someDataStream.split(</span><br><span class="line">  (num: <span class="type">Int</span>) =&gt;</span><br><span class="line">    (num % <span class="number">2</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="number">0</span> =&gt; <span class="type">List</span>(<span class="string">&quot;even&quot;</span>)</span><br><span class="line">      <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="type">List</span>(<span class="string">&quot;odd&quot;</span>)</span><br><span class="line">    &#125;)      </span><br></pre></td></tr></table></figure></div>

<h4 id="20-Select"><a href="#20-Select" class="headerlink" title="20. Select"></a>20. <strong>Select</strong></h4><p>从拆分流中选择一个或多个流</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">SplitStream</span>&lt;<span class="type">Integer</span>&gt; split;<span class="type">DataStream</span>&lt;<span class="type">Integer</span>&gt; even = split.select(<span class="string">&quot;even&quot;</span>);<span class="type">DataStream</span>&lt;<span class="type">Integer</span>&gt; odd = split.select(<span class="string">&quot;odd&quot;</span>);<span class="type">DataStream</span>&lt;<span class="type">Integer</span>&gt; all = split.select(<span class="string">&quot;even&quot;</span>,<span class="string">&quot;odd&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<h4 id="20-1-sum"><a href="#20-1-sum" class="headerlink" title="20.1  sum"></a>20.1  sum</h4><div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//键控流</span></span><br><span class="line">KeyedStream&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt; keyedStream = stream.keyBy(r -&gt; r.f0);</span><br><span class="line"></span><br><span class="line">keyedStream.sum(<span class="number">1</span>).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//reduce是以上几组算子的泛化实现</span></span><br><span class="line">keyedStream.reduce(<span class="keyword">new</span> <span class="title class_">ReduceFunction</span>&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title function_">reduce</span><span class="params">(Tuple2&lt;Integer, Integer&gt; value1, Tuple2&lt;Integer, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> Tuple2.of(value1.f0, value1.f1 + value2.f1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure></div>

<h3 id="4-3-Sink算子"><a href="#4-3-Sink算子" class="headerlink" title="4.3  Sink算子"></a>4.3  Sink算子</h3><p>支持将数据输出到：</p>
<ul>
<li>本地文件(参考批处理)</li>
<li>本地集合(参考批处理)</li>
<li>HDFS(参考批处理)</li>
</ul>
<p>除此之外，还支持：</p>
<ul>
<li>sink到kafka</li>
<li>sink到mysql</li>
<li>sink到redis</li>
</ul>
<p>下面以sink到kafka为例：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sinkTopic = <span class="string">&quot;test&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">Int</span>, name: <span class="type">String</span>, addr: <span class="type">String</span>, sex: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="keyword">val</span> mapper: <span class="type">ObjectMapper</span> = <span class="keyword">new</span> <span class="type">ObjectMapper</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment">//将对象转换成字符串</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toJsonString</span></span>(<span class="type">T</span>: <span class="type">Object</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    mapper.registerModule(<span class="type">DefaultScalaModule</span>)</span><br><span class="line">    mapper.writeValueAsString(<span class="type">T</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建流执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//2.准备数据</span></span><br><span class="line">    <span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">Student</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Student</span>(<span class="number">8</span>, <span class="string">&quot;xiaoming&quot;</span>, <span class="string">&quot;beijing biejing&quot;</span>, <span class="string">&quot;female&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//将student转换成字符串</span></span><br><span class="line">    <span class="keyword">val</span> studentStream: <span class="type">DataStream</span>[<span class="type">String</span>] = dataStream.map(student =&gt;</span><br><span class="line">      toJsonString(student) <span class="comment">// 这里需要显示SerializerFeature中的某一个，否则会报同时匹配两个方法的错误</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//studentStream.print()</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node01:9092&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> myProducer = <span class="keyword">new</span> <span class="type">FlinkKafkaProducer011</span>[<span class="type">String</span>](sinkTopic, <span class="keyword">new</span> <span class="type">KeyedSerializationSchemaWrapper</span>[<span class="type">String</span>](<span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()), prop)</span><br><span class="line">    studentStream.addSink(myProducer)</span><br><span class="line">    studentStream.print()</span><br><span class="line">    env.execute(<span class="string">&quot;Flink add sink&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="5-流处理中的Time与Window"><a href="#5-流处理中的Time与Window" class="headerlink" title="5  流处理中的Time与Window"></a>5  流处理中的Time与Window</h2><p>Flink 是流式的、实时的 计算引擎。</p>
<p>上面一句话就有两个概念，一个是流式，一个是实时。</p>
<p><strong>流式</strong>：就是数据源源不断的流进来，也就是数据没有边界，但是我们计算的时候必须在一个有边界的范围内进行，所以这里面就有一个问题，边界怎么确定？ 无非就两种方式，<strong>根据时间段或者数据量进行确定</strong>，根据时间段就是每隔多长时间就划分一个边界，根据数据量就是每来多少条数据划分一个边界，Flink 中就是这么划分边界的，本文会详细讲解。</p>
<p><strong>实时</strong>：就是数据发送过来之后立马就进行相关的计算，然后将结果输出。这里的计算有两种：</p>
<ul>
<li><strong>一种是只有边界内的数据进行计算</strong>，这种好理解，比如统计每个用户最近五分钟内浏览的新闻数量，就可以取最近五分钟内的所有数据，然后根据每个用户分组，统计新闻的总数。</li>
<li><strong>另一种是边界内数据与外部数据进行关联计算</strong>，比如：统计最近五分钟内浏览新闻的用户都是来自哪些地区，这种就需要将五分钟内浏览新闻的用户信息与 hive 中的地区维表进行关联，然后在进行相关计算。</li>
</ul>
<p>本节所讲的 Flink 内容就是围绕以上概念进行详细剖析的！</p>
<h3 id="5-1-Time"><a href="#5-1-Time" class="headerlink" title="5.1. Time"></a>5.1. Time</h3><p>在 Flink 中，如果以时间段划分边界的话，那么时间就是一个极其重要的字段。</p>
<p>Flink 中的时间有三种类型，如下图所示：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210123_1.png"
                      alt="img"
                ></p>
<ul>
<li><strong>Event Time</strong>：是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink 通过时间戳分配器访问事件时间戳。</li>
<li><strong>Ingestion Time</strong>：是数据进入 Flink 的时间。</li>
<li><strong>Processing Time</strong>：是每一个执行基于时间操作的算子的本地系统时间，与机器相关，默认的时间属性就是 Processing Time。</li>
</ul>
<p>例如，一条日志进入 Flink 的时间为 2021-01-22 10:00:00.123，到达 Window 的系统时间为 2021-01-22 10:00:01.234，日志的内容如下：<br>2021-01-06 18:37:15.624 INFO Fail over to rm2</p>
<p>对于业务来说，要统计 1min 内的故障日志个数，哪个时间是最有意义的？—— eventTime，因为我们要根据日志的生成时间进行统计。</p>
<h3 id="5-2-Window"><a href="#5-2-Window" class="headerlink" title="5.2. Window"></a>5.2. Window</h3><p>Window，即窗口，我们前面一直提到的边界就是这里的 Window(窗口)。</p>
<p>官方解释：<strong>流式计算是一种被设计用于处理无限数据集的数据处理引擎，而无限数据集是指一种不断增长的本质上无限的数据集，而 window 是一种切割无限数据为有限块进行处理的手段</strong>。</p>
<p>所以<strong>Window 是无限数据流处理的核心，Window 将一个无限的 stream 拆分成有限大小的”buckets”桶，我们可以在这些桶上做计算操作</strong>。</p>
<h4 id="Window-类型"><a href="#Window-类型" class="headerlink" title="Window 类型"></a>Window 类型</h4><p>本文刚开始提到，划分窗口就两种方式：</p>
<ol>
<li>根据时间进行截取(time-driven-window)，比如每 1 分钟统计一次或每 10 分钟统计一次。</li>
<li>根据数据进行截取(data-driven-window)，比如每 5 个数据统计一次或每 50 个数据统计一次。</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210123_2.png"
                      alt="窗口类型"
                ></p>
<p>对于 TimeWindow(根据时间划分窗口)， 可以根据窗口实现原理的不同分成三类：<strong>滚动窗口（Tumbling Window）、滑动窗口（Sliding Window）和会话窗口（Session Window）</strong>。</p>
<ol>
<li><strong>滚动窗口（Tumbling Windows）</strong></li>
</ol>
<p>将数据依据固定的窗口长度对数据进行切片。</p>
<p>特点：<strong>时间对齐，窗口长度固定，没有重叠</strong>。</p>
<p>滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口中，滚动窗口有一个固定的大小，并且不会出现重叠。</p>
<p>例如：如果你指定了一个 5 分钟大小的滚动窗口，窗口的创建如下图所示：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210123_3.png"
                      alt="滚动窗口"
                ></p>
<p>适用场景：适合做 BI 统计等（做每个时间段的聚合计算）。</p>
<ol>
<li><strong>滑动窗口（Sliding Windows）</strong></li>
</ol>
<p>滑动窗口是固定窗口的更广义的一种形式，滑动窗口由固定的窗口长度和滑动间隔组成。</p>
<p>特点：<strong>时间对齐，窗口长度固定，有重叠</strong>。</p>
<p>滑动窗口分配器将元素分配到固定长度的窗口中，与滚动窗口类似，窗口的大小由窗口大小参数来配置，另一个窗口滑动参数控制滑动窗口开始的频率。因此，滑动窗口如果滑动参数小于窗口大小的话，窗口是可以重叠的，在这种情况下元素会被分配到多个窗口中。</p>
<p>例如，你有 10 分钟的窗口和 5 分钟的滑动，那么每个窗口中 5 分钟的窗口里包含着上个 10 分钟产生的数据，如下图所示：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210123_9.png"
                      alt="滑动窗口"
                ></p>
<p>适用场景：对最近一个时间段内的统计（求某接口最近 5min 的失败率来决定是否要报警）。</p>
<ol>
<li><strong>会话窗口（Session Windows）</strong></li>
</ol>
<p>由一系列事件组合一个指定时间长度的 timeout 间隙组成，类似于 web 应用的 session，也就是一段时间没有接收到新数据就会生成新的窗口。</p>
<p>特点：<strong>时间无对齐</strong>。</p>
<p>session 窗口分配器通过 session 活动来对元素进行分组，session 窗口跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况，相反，<strong>当它在一个固定的时间周期内不再收到元素，即非活动间隔产生，那个这个窗口就会关闭</strong>。一个 session 窗口通过一个 session 间隔来配置，这个 session 间隔定义了非活跃周期的长度，当这个非活跃周期产生，那么当前的 session 将关闭并且后续的元素将被分配到新的 session 窗口中去。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210123_10.png"
                      alt="会话窗口"
                ></p>
<h3 id="5-3-Window-API"><a href="#5-3-Window-API" class="headerlink" title="5.3. Window API"></a>5.3. Window API</h3><h4 id="1-TimeWindow"><a href="#1-TimeWindow" class="headerlink" title="1) TimeWindow"></a>1) TimeWindow</h4><p>TimeWindow 是将指定时间范围内的所有数据组成一个 window，一次对一个 window 里面的所有数据进行计算（就是本文开头说的对一个边界内的数据进行计算）。</p>
<p>我们以 <strong>红绿灯路口通过的汽车数量</strong> 为例子：</p>
<p>红绿灯路口会有汽车通过，一共会有多少汽车通过，无法计算。因为车流源源不断，计算没有边界。</p>
<p>所以我们统计每 15 秒钟通过红路灯的汽车数量，如第一个 15 秒为 2 辆，第二个 15 秒为 3 辆，第三个 15 秒为 1 辆 …</p>
<ul>
<li><strong>tumbling-time-window (无重叠数据)</strong></li>
</ul>
<p>我们使用 Linux 中的 nc 命令模拟数据的发送方</p>
<div class="highlight-container" data-rel="Shell"><figure class="iseeu highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1.开启发送端口，端口号为9999</span><br><span class="line">nc -lk 9999</span><br><span class="line"></span><br><span class="line">2.发送内容（key 代表不同的路口，value 代表每次通过的车辆）</span><br><span class="line">一次发送一行，发送的时间间隔代表汽车经过的时间间隔</span><br><span class="line">9,3</span><br><span class="line">9,2</span><br><span class="line">9,7</span><br><span class="line">4,9</span><br><span class="line">2,6</span><br><span class="line">1,5</span><br><span class="line">2,3</span><br><span class="line">5,7</span><br><span class="line">5,4</span><br></pre></td></tr></table></figure></div>

<p>Flink 进行采集数据并计算：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//TODO time-window</span></span><br><span class="line">    <span class="comment">//1.创建运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.定义数据流来源</span></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.转换数据格式，text-&gt;CarWc</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CarWc</span>(<span class="params">sensorId: <span class="type">Int</span>, carCnt: <span class="type">Int</span></span>)</span></span><br><span class="line">    <span class="keyword">val</span> ds1: <span class="type">DataStream</span>[<span class="type">CarWc</span>] = text.map &#123;</span><br><span class="line">      line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> tokens = line.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="type">CarWc</span>(tokens(<span class="number">0</span>).trim.toInt, tokens(<span class="number">1</span>).trim.toInt)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.执行统计操作，每个sensorId一个tumbling窗口，窗口的大小为5秒</span></span><br><span class="line">    <span class="comment">//也就是说，每5秒钟统计一次，在这过去的5秒钟内，各个路口通过红绿灯汽车的数量。</span></span><br><span class="line">    <span class="keyword">val</span> ds2: <span class="type">DataStream</span>[<span class="type">CarWc</span>] = ds1</span><br><span class="line">      .keyBy(<span class="string">&quot;sensorId&quot;</span>)</span><br><span class="line">      .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">      .sum(<span class="string">&quot;carCnt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.显示统计结果</span></span><br><span class="line">    ds2.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.触发流计算</span></span><br><span class="line">    env.execute(<span class="keyword">this</span>.getClass.getName)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>我们发送的数据并没有指定时间字段，&#x3D;&#x3D;所以 <strong>Flink 使用的是默认的 Processing Time，也就是 Flink 系统处理数据时的时间。</strong>&#x3D;&#x3D;</p>
<ul>
<li><strong>sliding-time-window (有重叠数据)</strong></li>
</ul>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1.创建运行环境</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.定义数据流来源</span></span><br><span class="line"><span class="keyword">val</span> text = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.转换数据格式，text-&gt;CarWc</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CarWc</span>(<span class="params">sensorId: <span class="type">Int</span>, carCnt: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds1: <span class="type">DataStream</span>[<span class="type">CarWc</span>] = text.map &#123;</span><br><span class="line">  line =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> tokens = line.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    <span class="type">CarWc</span>(tokens(<span class="number">0</span>).trim.toInt, tokens(<span class="number">1</span>).trim.toInt)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//4.执行统计操作，每个sensorId一个sliding窗口，窗口时间10秒,滑动时间5秒</span></span><br><span class="line"><span class="comment">//也就是说，每5秒钟统计一次，在这过去的10秒钟内，各个路口通过红绿灯汽车的数量。</span></span><br><span class="line"><span class="keyword">val</span> ds2: <span class="type">DataStream</span>[<span class="type">CarWc</span>] = ds1</span><br><span class="line">  .keyBy(<span class="string">&quot;sensorId&quot;</span>)</span><br><span class="line">  .timeWindow(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">  .sum(<span class="string">&quot;carCnt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.显示统计结果</span></span><br><span class="line">ds2.print()</span><br><span class="line"></span><br><span class="line"><span class="comment">//6.触发流计算</span></span><br><span class="line">env.execute(<span class="keyword">this</span>.getClass.getName)</span><br></pre></td></tr></table></figure></div>

<h4 id="2-CountWindow"><a href="#2-CountWindow" class="headerlink" title="2) CountWindow"></a>2) CountWindow</h4><p>CountWindow 根据<strong>窗口中相同 key 元素的数量来触发执行</strong>，执行时只计算元素数量达到窗口大小的 key 对应的结果。</p>
<p><strong>注意：CountWindow 的 window_size 指的是相同 Key 的元素的个数，不是输入的所有元素的总数</strong>。</p>
<ul>
<li><strong>tumbling-count-window (无重叠数据)</strong></li>
</ul>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1.创建运行环境</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.定义数据流来源</span></span><br><span class="line"><span class="keyword">val</span> text = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.转换数据格式，text-&gt;CarWc</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CarWc</span>(<span class="params">sensorId: <span class="type">Int</span>, carCnt: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds1: <span class="type">DataStream</span>[<span class="type">CarWc</span>] = text.map &#123;</span><br><span class="line">  (f) =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> tokens = f.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    <span class="type">CarWc</span>(tokens(<span class="number">0</span>).trim.toInt, tokens(<span class="number">1</span>).trim.toInt)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//4.执行统计操作，每个sensorId一个tumbling窗口，窗口的大小为5</span></span><br><span class="line"><span class="comment">//按照key进行收集，对应的key出现的次数达到5次作为一个结果</span></span><br><span class="line"><span class="keyword">val</span> ds2: <span class="type">DataStream</span>[<span class="type">CarWc</span>] = ds1</span><br><span class="line">  .keyBy(<span class="string">&quot;sensorId&quot;</span>)</span><br><span class="line">  .countWindow(<span class="number">5</span>)</span><br><span class="line">  .sum(<span class="string">&quot;carCnt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.显示统计结果</span></span><br><span class="line">ds2.print()</span><br><span class="line"></span><br><span class="line"><span class="comment">//6.触发流计算</span></span><br><span class="line">env.execute(<span class="keyword">this</span>.getClass.getName)</span><br></pre></td></tr></table></figure></div>

<hr>
<ul>
<li><strong>sliding-count-window (有重叠数据)</strong></li>
</ul>
<p>同样也是窗口长度和滑动窗口的操作：窗口长度是 5，滑动长度是 3</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1.创建运行环境</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.定义数据流来源</span></span><br><span class="line"><span class="keyword">val</span> text = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.转换数据格式，text-&gt;CarWc</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CarWc</span>(<span class="params">sensorId: <span class="type">Int</span>, carCnt: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds1: <span class="type">DataStream</span>[<span class="type">CarWc</span>] = text.map &#123;</span><br><span class="line">  (f) =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> tokens = f.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    <span class="type">CarWc</span>(tokens(<span class="number">0</span>).trim.toInt, tokens(<span class="number">1</span>).trim.toInt)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//4.执行统计操作，每个sensorId一个sliding窗口，窗口大小3条数据,窗口滑动为3条数据</span></span><br><span class="line"><span class="comment">//也就是说，每个路口分别统计，收到关于它的3条消息时统计在最近5条消息中，各自路口通过的汽车数量</span></span><br><span class="line"><span class="keyword">val</span> ds2: <span class="type">DataStream</span>[<span class="type">CarWc</span>] = ds1</span><br><span class="line">  .keyBy(<span class="string">&quot;sensorId&quot;</span>)</span><br><span class="line">  .countWindow(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">  .sum(<span class="string">&quot;carCnt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.显示统计结果</span></span><br><span class="line">ds2.print()</span><br><span class="line"></span><br><span class="line"><span class="comment">//6.触发流计算</span></span><br><span class="line">env.execute(<span class="keyword">this</span>.getClass.getName)</span><br></pre></td></tr></table></figure></div>

<hr>
<ul>
<li><strong>Window 总结</strong></li>
</ul>
<ol>
<li>flink 支持两种划分窗口的方式（time 和 count）<ul>
<li>如果根据时间划分窗口，那么它就是一个 time-window</li>
<li>如果根据数据划分窗口，那么它就是一个 count-window</li>
</ul>
</li>
<li>flink 支持窗口的两个重要属性（size 和 interval）<ul>
<li>如果 size&#x3D;interval,那么就会形成 tumbling-window(无重叠数据)</li>
<li>如果 size&gt;interval,那么就会形成 sliding-window(有重叠数据)</li>
<li>如果 size&lt;interval,那么这种窗口将会丢失数据。比如每 5 秒钟，统计过去 3 秒的通过路口汽车的数据，将会漏掉 2 秒钟的数据。</li>
</ul>
</li>
<li>通过组合可以得出四种基本窗口<ul>
<li>time-tumbling-window 无重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5))</li>
<li>time-sliding-window 有重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5), Time.seconds(3))</li>
<li>count-tumbling-window 无重叠数据的数量窗口，设置方式举例：countWindow(5)</li>
<li>count-sliding-window 有重叠数据的数量窗口，设置方式举例：countWindow(5,3)</li>
</ul>
</li>
</ol>
<h4 id="3-Window-Reduce"><a href="#3-Window-Reduce" class="headerlink" title="3) Window Reduce"></a>3) Window Reduce</h4><p>WindowedStream → DataStream：给 window 赋一个 reduce 功能的函数，并返回一个聚合的结果。</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWindowReduce</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 获取执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建SocketSource</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.socketTextStream(<span class="string">&quot;node01&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对stream进行处理并按key聚合</span></span><br><span class="line">    <span class="keyword">val</span> streamKeyBy = stream.map(item =&gt; (item, <span class="number">1</span>)).keyBy(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 引入时间窗口</span></span><br><span class="line">    <span class="keyword">val</span> streamWindow = streamKeyBy.timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行聚合操作</span></span><br><span class="line">    <span class="keyword">val</span> streamReduce = streamWindow.reduce(</span><br><span class="line">      (item1, item2) =&gt; (item1._1, item1._2 + item2._2)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将聚合数据写入文件</span></span><br><span class="line">    streamReduce.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行程序</span></span><br><span class="line">    env.execute(<span class="string">&quot;TumblingWindow&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="4-Window-Apply"><a href="#4-Window-Apply" class="headerlink" title="4) Window Apply"></a>4) Window Apply</h4><p>apply 方法可以进行一些自定义处理，通过匿名内部类的方法来实现。当有一些复杂计算时使用。</p>
<p>用法</p>
<ol>
<li>实现一个 WindowFunction 类</li>
<li>指定该类的泛型为 [输入数据类型, 输出数据类型, keyBy 中使用分组字段的类型, 窗口类型]</li>
</ol>
<p>示例：使用 apply 方法来实现单词统计</p>
<p>步骤：</p>
<ol>
<li>获取流处理运行环境</li>
<li>构建 socket 流数据源，并指定 IP 地址和端口号</li>
<li>对接收到的数据转换成单词元组</li>
<li>使用 keyBy 进行分流（分组）</li>
<li>使用 timeWinodw 指定窗口的长度（每 3 秒计算一次）</li>
<li>实现一个 WindowFunction 匿名内部类<ul>
<li>apply 方法中实现聚合计算</li>
<li>使用 Collector.collect 收集数据</li>
</ul>
</li>
</ol>
<p>核心代码如下：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1. 获取流处理运行环境</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">//2. 构建socket流数据源，并指定IP地址和端口号</span></span><br><span class="line"><span class="keyword">val</span> textDataStream = env.socketTextStream(<span class="string">&quot;node01&quot;</span>, <span class="number">9999</span>).flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//3. 对接收到的数据转换成单词元组</span></span><br><span class="line"><span class="keyword">val</span> wordDataStream = textDataStream.map(_-&gt;<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//4. 使用 keyBy 进行分流（分组）</span></span><br><span class="line"><span class="keyword">val</span> groupedDataStream: <span class="type">KeyedStream</span>[(<span class="type">String</span>, <span class="type">Int</span>), <span class="type">String</span>] = wordDataStream.keyBy(_._1)</span><br><span class="line"></span><br><span class="line"><span class="comment">//5. 使用 timeWinodw 指定窗口的长度（每3秒计算一次）</span></span><br><span class="line"><span class="keyword">val</span> windowDataStream: <span class="type">WindowedStream</span>[(<span class="type">String</span>, <span class="type">Int</span>), <span class="type">String</span>, <span class="type">TimeWindow</span>] = groupedDataStream.timeWindow(<span class="type">Time</span>.seconds(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//6. 实现一个WindowFunction匿名内部类</span></span><br><span class="line"><span class="keyword">val</span> reduceDatStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = windowDataStream.apply(<span class="keyword">new</span> <span class="type">RichWindowFunction</span>[(<span class="type">String</span>, <span class="type">Int</span>), (<span class="type">String</span>, <span class="type">Int</span>), <span class="type">String</span>, <span class="type">TimeWindow</span>] &#123;</span><br><span class="line">  <span class="comment">//在apply方法中实现数据的聚合</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">String</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)], out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;hello world&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> tuple = input.reduce((t1, t2) =&gt; &#123;</span><br><span class="line">      (t1._1, t1._2 + t2._2)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//将要返回的数据收集起来，发送回去</span></span><br><span class="line">    out.collect(tuple)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line">reduceDatStream.print()</span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure></div>

<h4 id="5-Window-Fold"><a href="#5-Window-Fold" class="headerlink" title="5) Window Fold"></a>5) Window Fold</h4><p>WindowedStream → DataStream：给窗口赋一个 fold 功能的函数，并返回一个 fold 后的结果。</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWindowFold</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 获取执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建SocketSource</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.socketTextStream(<span class="string">&quot;node01&quot;</span>, <span class="number">9999</span>,&#x27;\n&#x27;,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对stream进行处理并按key聚合</span></span><br><span class="line">    <span class="keyword">val</span> streamKeyBy = stream.map(item =&gt; (item, <span class="number">1</span>)).keyBy(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 引入滚动窗口</span></span><br><span class="line">    <span class="keyword">val</span> streamWindow = streamKeyBy.timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行fold操作</span></span><br><span class="line">    <span class="keyword">val</span> streamFold = streamWindow.fold(<span class="number">100</span>)&#123;</span><br><span class="line">      (begin, item) =&gt;</span><br><span class="line">        begin + item._2</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将聚合数据写入文件</span></span><br><span class="line">    streamFold.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行程序</span></span><br><span class="line">    env.execute(<span class="string">&quot;TumblingWindow&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="6-Aggregation-on-Window"><a href="#6-Aggregation-on-Window" class="headerlink" title="6) Aggregation on Window"></a>6) Aggregation on Window</h4><p>WindowedStream → DataStream：对一个 window 内的所有元素做聚合操作。min 和 minBy 的区别是 min 返回的是最小值，而 minBy 返回的是包含最小值字段的元素(同样的原理适用于 max 和 maxBy)。</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWindowAggregation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 获取执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建SocketSource</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.socketTextStream(<span class="string">&quot;node01&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对stream进行处理并按key聚合</span></span><br><span class="line">    <span class="keyword">val</span> streamKeyBy = stream.map(item =&gt; (item.split(<span class="string">&quot; &quot;</span>)(<span class="number">0</span>), item.split(<span class="string">&quot; &quot;</span>)(<span class="number">1</span>))).keyBy(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 引入滚动窗口</span></span><br><span class="line">    <span class="keyword">val</span> streamWindow = streamKeyBy.timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行聚合操作</span></span><br><span class="line">    <span class="keyword">val</span> streamMax = streamWindow.max(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将聚合数据写入文件</span></span><br><span class="line">    streamMax.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行程序</span></span><br><span class="line">    env.execute(<span class="string">&quot;TumblingWindow&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="5-4-EventTime-与-Window"><a href="#5-4-EventTime-与-Window" class="headerlink" title="5.4. EventTime 与 Window"></a>5.4. EventTime 与 Window</h3><h4 id="1-EventTime-的引入"><a href="#1-EventTime-的引入" class="headerlink" title="1) EventTime 的引入"></a>1) EventTime 的引入</h4><ol>
<li>与现实世界中的时间是不一致的，在 flink 中被划分为事件时间，提取时间，处理时间三种。</li>
<li>如果以 EventTime 为基准来定义时间窗口那将形成 EventTimeWindow,要求消息本身就应该携带 EventTime</li>
<li>如果以 IngesingtTime 为基准来定义时间窗口那将形成 IngestingTimeWindow,以 source 的 systemTime 为准。</li>
<li>如果以 ProcessingTime 基准来定义时间窗口那将形成 ProcessingTimeWindow，以 operator 的 systemTime 为准。</li>
</ol>
<p>在 Flink 的流式处理中，绝大部分的业务都会使用 eventTime，一般只在 eventTime 无法使用时，才会被迫使用 ProcessingTime 或者 IngestionTime。</p>
<p>如果要使用 EventTime，那么需要引入 EventTime 的时间属性，引入方式如下所示：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从调用时刻开始给env创建的每一个stream追加时间特征</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br></pre></td></tr></table></figure></div>

<h4 id="2-Watermark"><a href="#2-Watermark" class="headerlink" title="2) Watermark"></a>2) Watermark</h4><p>我们知道，流处理从事件产生，到流经 source，再到 operator，中间是有一个过程和时间的，虽然大部分情况下，流到 operator 的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生，所谓乱序，就是指 Flink 接收到的事件的先后顺序不是严格按照事件的 Event Time 顺序排列的，所以 Flink 最初设计的时候，就考虑到了网络延迟，网络乱序等问题，所以提出了一个抽象概念：水印（WaterMark）；</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210123_6.png"
                      alt="img"
                ></p>
<p>如上图所示，就出现一个问题，一旦出现乱序，如果只根据 EventTime 决定 Window 的运行，我们不能明确数据是否全部到位，但又不能无限期的等下去，此时必须要有个机制来保证一个特定的时间后，必须触发 Window 去进行计算了，这个特别的机制，就是 Watermark。</p>
<p><strong>Watermark 是用于处理乱序事件的，而正确的处理乱序事件，通常用 Watermark 机制结合 Window 来实现</strong>。</p>
<p><strong>数据流中的 Watermark 用于表示 timestamp 小于 Watermark 的数据，都已经到达了，因此，Window 的执行也是由 Watermark 触发的。</strong></p>
<p><strong>Watermark 可以理解成一个延迟触发机制，我们可以设置 Watermark 的延时时长 t，每次系统会校验已经到达的数据中最大的 maxEventTime，然后认定 EventTime 小于 maxEventTime - t 的所有数据都已经到达，如果有窗口的停止时间等于 maxEventTime – t，那么这个窗口被触发执行</strong>。</p>
<p>有序流的 Watermarker 如下图所示：（Watermark 设置为 0）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210123_7.png"
                      alt="有序数据的Watermark"
                ></p>
<p>乱序流的 Watermarker 如下图所示：（Watermark 设置为 2）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210123_8.png"
                      alt="无序数据的Watermark"
                ></p>
<p><strong>当 Flink 接收到每一条数据时，都会产生一条 Watermark，这条 Watermark 就等于当前所有到达数据中的 maxEventTime - 延迟时长，也就是说，Watermark 是由数据携带的，一旦数据携带的 Watermark 比当前未触发的窗口的停止时间要晚，那么就会触发相应窗口的执行。由于 Watermark 是由数据携带的，因此，如果运行过程中无法获取新的数据，那么没有被触发的窗口将永远都不被触发</strong>。</p>
<p>上图中，我们设置的允许最大延迟到达时间为 2s，所以时间戳为 7s 的事件对应的 Watermark 是 5s，时间戳为 12s 的事件的 Watermark 是 10s，如果我们的窗口 1 是 1s<del>5s，窗口 2 是 6s</del>10s，那么时间戳为 7s 的事件到达时的 Watermarker 恰好触发窗口 1，时间戳为 12s 的事件到达时的 Watermark 恰好触发窗口 2。</p>
<p>&#x3D;&#x3D;一个和多个分区之间的水位线传递规则：&#x3D;&#x3D;</p>
<ul>
<li>任务为每个输入分区维护一个分区水位线（watermark）。当从一个分区接收到watermark时，它会比较新接收到的值和当前水位值，然后将相应的分区watermark 更新为<strong>两者的最大值</strong></li>
<li>任务会比较所有分区watermark 的大小，将其事件时钟更新为<strong>所有分区watermark 的最小值</strong>。</li>
<li><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230227222036120.png"
                      alt="image-20230227222036120"
                ></li>
</ul>
<p><strong>从2可知，永远传递分区中的最小水位线</strong></p>
<p>&#x3D;&#x3D;多流间的水位线传递规则&#x3D;&#x3D;</p>
<p>具有两个或多个输入流（如Union 或CoFlatMap）的算子任务（参见“多流转换”一节）也会以<strong>所有分区watermark 的最小值</strong>作为事件时间时钟。它们并不区分不同输入流的分区watermark，所以两个输入流的数据都是基于相同的事件时间时钟进行处理的。当然我们可以想到，如果应用程序的各个输入流的事件时间不一致，那么这种处理方式可能会导致问题。</p>
<p>Flink 的水位处理和传递算法，确保了算子任务发出的时间戳和watermark 是“对齐”的。不过它依赖一个条件，那就是所有分区都会提供不断增长的watermark。<strong>一旦一个分区不再推进水位线的上升，或者完全处于空闲状态、不再发送任何数据和watermark，任务的事件时间时钟就将停滞不前，任务的定时器也就无法触发了。</strong>对于基于时间的算子来说，它们需要依赖时钟的推进来执行计算和清除状态，这种情况显然就会有问题。如果任务没有定期从所有输入任务接收到新的watermark，那么基于时间的算子的处理延迟和状态空间的大小都会显著增加。</p>
<p>对于具有两个输入流而且watermark 明显不同的算子，也会出现类似的情况。具有两个输入流的任务的事件时间时钟，将会同较慢的那条流的watermark 保持一致，<strong>而通常较快流的数据或者中间结果会在state 中缓冲，直到事件时间时钟达到这条流的watermark，才会允许处理它们。</strong></p>
<h4 id="3-Flink-对于迟到数据的处理"><a href="#3-Flink-对于迟到数据的处理" class="headerlink" title="3) Flink 对于迟到数据的处理"></a>3) Flink 对于迟到数据的处理</h4><p>waterMark 和 Window 机制解决了流式数据的乱序问题，对于因为延迟而顺序有误的数据，可以根据 eventTime 进行业务处理，于延迟的数据 Flink 也有自己的解决办法，主要的办法是给定一个允许延迟的时间，在该时间范围内仍可以接受处理延迟数据。</p>
<p>设置允许延迟的时间是通过 <strong>allowedLateness(lateness: Time)</strong> 设置</p>
<p>保存延迟数据则是通过 <strong>sideOutputLateData(outputTag: OutputTag[T])</strong> 保存</p>
<p>获取延迟数据是通过 <strong>DataStream.getSideOutput(tag: OutputTag[X])</strong> 获取</p>
<p>具体的用法如下：</p>
<p><strong>allowedLateness(lateness: Time)</strong></p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">allowedLateness</span></span>(lateness: <span class="type">Time</span>): <span class="type">WindowedStream</span>[<span class="type">T</span>, <span class="type">K</span>, <span class="type">W</span>] = &#123;</span><br><span class="line">  javaStream.allowedLateness(lateness)</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>该方法传入一个 Time 值，设置允许数据迟到的时间，这个时间和 WaterMark 中的时间概念不同。再来回顾一下：</p>
<p>WaterMark&#x3D;数据的事件时间-允许乱序时间值</p>
<p>随着新数据的到来，waterMark 的值会更新为最新数据事件时间-允许乱序时间值，但是如果这时候来了一条历史数据，waterMark 值则不会更新。总的来说，waterMark 是为了能接收到尽可能多的乱序数据。</p>
<p>那这里的 Time 值，主要是为了等待迟到的数据，在一定时间范围内，如果属于该窗口的数据到来，仍会进行计算，后面会对计算方式仔细说明</p>
<p>注意：该方法只针对于基于 event-time 的窗口，如果是基于 processing-time，并且指定了非零的 time 值则会抛出异常。</p>
<p><strong>sideOutputLateData(outputTag: OutputTag[T])</strong></p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sideOutputLateData</span></span>(outputTag: <span class="type">OutputTag</span>[<span class="type">T</span>]): <span class="type">WindowedStream</span>[<span class="type">T</span>, <span class="type">K</span>, <span class="type">W</span>] = &#123;</span><br><span class="line">  javaStream.sideOutputLateData(outputTag)</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>该方法是将迟来的数据保存至给定的 outputTag 参数，而 OutputTag 则是用来标记延迟数据的一个对象。</p>
<p><strong>DataStream.getSideOutput(tag: OutputTag[X])</strong></p>
<p>通过 window 等操作返回的 DataStream 调用该方法，传入标记延迟数据的对象来获取延迟的数据。</p>
<p><strong>对延迟数据的理解</strong></p>
<p>延迟数据是指：</p>
<p>在当前窗口【假设窗口范围为 10-15】已经计算之后，又来了一个属于该窗口的数据【假设事件时间为 13】，这时候仍会触发 Window 操作，这种数据就称为延迟数据。</p>
<p>那么问题来了，延迟时间怎么计算呢？</p>
<p>假设窗口范围为 10-15，延迟时间为 2s，则只要 WaterMark&lt;15+2，并且属于该窗口，就能触发 Window 操作。而如果来了一条数据使得 WaterMark&gt;&#x3D;15+2，10-15 这个窗口就不能再触发 Window 操作，即使新来的数据的 Event Time 属于这个窗口时间内 。</p>
<h4 id="4）水位线的产生"><a href="#4）水位线的产生" class="headerlink" title="4）水位线的产生"></a>4）水位线的产生</h4><p>以应用程序必须首先显式地分配时间戳并生成watermark。Flink 流应用程序可以通过三种方式分配时间戳和生成watermark：</p>
<ul>
<li><strong>在数据源（source）处分配：</strong>当数据流被摄入到应用程序中时，可以由“源函数”SourceFunction 分配和生成时间戳和watermark。SourceFunction 可以产生并发送一个数据流；数据会与相关的时间戳一起发送出去，而watermark 可以作为一条特殊数据在任何时间点发出。如果SourceFunction（暂时）不再发出watermark，它可以声明自己处于“空闲”（idle）状态。Flink 会在后续算子的水位计算中，把空闲的SourceFunction 产生的流分区排除掉。source 的这一空闲机制，可以用来解决前面提到的水位不再上升的问题。源函数（Source Function）在“实现自定义源函数”一节中进行了更详细的讨论。</li>
<li><strong>定期分配：</strong>在Flink 中，DataStream API 提供一个名为AssignerWithPeriodicWatermarks的用户定义函数，它可以从每个数据中提取时间戳，并被定期调用以生成当前watermark。提取出的时间戳被分配给相应的数据，而生成的watermark 也会添加到流中。这个函数将在“分配时间戳和生成水位线”一节中讨论。</li>
<li><strong>间断分配：</strong>AssignerWithPunctuatedWatermarks 是另一个用户定义的函数，它同样会从每个数据中提取一个时间戳。它可以用于生成特殊输入数据中的watermark。与AssignerWithPeriodicWatermarks 相比，此函数可以（但不是必须）从每个记录中提取watermark。我们在“分配时间戳和生成水位线”一节中同样讨论了该函数。</li>
</ul>
<div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//在数据源处分配</span></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; result = env</span><br><span class="line">                <span class="comment">//自定义一个数据源</span></span><br><span class="line">                .addSource(<span class="keyword">new</span> <span class="title class_">SourceFunction</span>&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;Tuple2&lt;String, Long&gt;&gt; ctx)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="comment">//指定时间戳发送数据</span></span><br><span class="line">                        ctx.collectWithTimestamp(Tuple2.of(<span class="string">&quot;hello world&quot;</span>, <span class="number">1000L</span>), <span class="number">1000L</span>);</span><br><span class="line">                        <span class="comment">//发送水位线</span></span><br><span class="line">                        ctx.emitWatermark(<span class="keyword">new</span> <span class="title class_">Watermark</span>(<span class="number">999L</span>));</span><br><span class="line"></span><br><span class="line">                        ctx.collectWithTimestamp(Tuple2.of(<span class="string">&quot;hello flink&quot;</span>, <span class="number">2000L</span>), <span class="number">2000L</span>);</span><br><span class="line">                        ctx.emitWatermark(<span class="keyword">new</span> <span class="title class_">Watermark</span>(<span class="number">1999L</span>));</span><br><span class="line"></span><br><span class="line">                        ctx.collectWithTimestamp(Tuple2.of(<span class="string">&quot;hello late&quot;</span>, <span class="number">1000L</span>), <span class="number">1000L</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br></pre></td></tr></table></figure></div>



<h4 id="5-Flink-关联-Hive-分区表"><a href="#5-Flink-关联-Hive-分区表" class="headerlink" title="5) Flink 关联 Hive 分区表"></a>5) Flink 关联 Hive 分区表</h4><p>Flink 1.12 支持了 Hive 最新的分区作为时态表的功能，可以通过 SQL 的方式直接关联 Hive 分区表的最新分区，并且会自动监听最新的 Hive 分区，当监控到新的分区后，会自动地做维表数据的全量替换。通过这种方式，用户无需编写 DataStream 程序即可完成 <strong>Kafka 流实时关联最新的 Hive 分区实现数据打宽</strong>。</p>
<p>具体用法：</p>
<p>在 Sql Client 中注册 HiveCatalog：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim conf/sql-client-defaults.yaml</span><br><span class="line">catalogs:</span><br><span class="line">  - name: hive_catalog</span><br><span class="line">    <span class="class"><span class="keyword">type</span></span>: hive</span><br><span class="line">    hive-conf-dir: /disk0/soft/hive-conf/ #该目录需要包hive-site.xml文件</span><br></pre></td></tr></table></figure></div>

<p><strong>创建 Kafka 表</strong></p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">CREATE</span> <span class="type">TABLE</span> hive_catalog.flink_db.kfk_fact_bill_master_12 (</span><br><span class="line">    master <span class="type">Row</span>&lt;reportDate <span class="type">String</span>, groupID int, shopID int, shopName <span class="type">String</span>, action int, orderStatus int, orderKey <span class="type">String</span>, actionTime bigint, areaName <span class="type">String</span>, paidAmount double, foodAmount double, startTime <span class="type">String</span>, person double, orderSubType int, checkoutTime <span class="type">String</span>&gt;,</span><br><span class="line">proctime as <span class="type">PROCTIME</span>()  -- <span class="type">PROCTIME</span>用来和<span class="type">Hive</span>时态表关联</span><br><span class="line">) <span class="type">WITH</span> (</span><br><span class="line"> &#x27;connector&#x27; = &#x27;kafka&#x27;,</span><br><span class="line"> &#x27;topic&#x27; = &#x27;topic_name&#x27;,</span><br><span class="line"> &#x27;format&#x27; = &#x27;json&#x27;,</span><br><span class="line"> &#x27;properties.bootstrap.servers&#x27; = &#x27;host:<span class="number">9092</span>&#x27;,</span><br><span class="line"> &#x27;properties.group.id&#x27; = &#x27;flinkTestGroup&#x27;,</span><br><span class="line"> &#x27;scan.startup.mode&#x27; = &#x27;timestamp&#x27;,</span><br><span class="line"> &#x27;scan.startup.timestamp-millis&#x27; = &#x27;<span class="number">1607844694000</span>&#x27;</span><br><span class="line">);</span><br></pre></td></tr></table></figure></div>

<p><strong>Flink 事实表与 Hive 最新分区数据关联</strong></p>
<p>dim_extend_shop_info 是 Hive 中已存在的表，所以我们用 table hint 动态地开启维表参数。</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">CREATE</span> <span class="type">VIEW</span> <span class="type">IF</span> <span class="type">NOT</span> <span class="type">EXISTS</span> hive_catalog.flink_db.view_fact_bill_master as</span><br><span class="line"><span class="type">SELECT</span> * <span class="type">FROM</span></span><br><span class="line"> (select t1.*, t2.group_id, t2.shop_id, t2.group_name, t2.shop_name, t2.brand_id,</span><br><span class="line">     <span class="type">ROW_NUMBER</span>() <span class="type">OVER</span> (<span class="type">PARTITION</span> <span class="type">BY</span> groupID, shopID, orderKey <span class="type">ORDER</span> <span class="type">BY</span> actionTime desc) rn</span><br><span class="line">    from hive_catalog.flink_db.kfk_fact_bill_master_12 t1</span><br><span class="line">       <span class="type">JOIN</span> hive_catalog.flink_db.dim_extend_shop_info</span><br><span class="line">  <span class="comment">/*+ OPTIONS(&#x27;streaming-source.enable&#x27;=&#x27;true&#x27;,</span></span><br><span class="line"><span class="comment">     &#x27;streaming-source.partition.include&#x27; = &#x27;latest&#x27;,</span></span><br><span class="line"><span class="comment">     &#x27;streaming-source.monitor-interval&#x27; = &#x27;1 h&#x27;,</span></span><br><span class="line"><span class="comment">     &#x27;streaming-source.partition-order&#x27; = &#x27;partition-name&#x27;) */</span></span><br><span class="line">    <span class="type">FOR</span> <span class="type">SYSTEM_TIME</span> <span class="type">AS</span> <span class="type">OF</span> t1.proctime <span class="type">AS</span> t2 --时态表</span><br><span class="line">    <span class="type">ON</span> t1.groupID = t2.group_id and t1.shopID = t2.shop_id</span><br><span class="line">    where groupID in (<span class="number">202042</span>)) t  where t.rn = <span class="number">1</span></span><br></pre></td></tr></table></figure></div>

<p>参数解释：</p>
<ul>
<li><strong>streaming-source.enable</strong> 开启流式读取 Hive 数据。</li>
<li><strong>streaming-source.partition.include</strong> 有以下两个值：<ol>
<li>latest 属性: 只读取最新分区数据。</li>
<li>all: 读取全量分区数据 ，默认值为 all，表示读所有分区，latest 只能用在 temporal join 中，用于读取最新分区作为维表，不能直接读取最新分区数据。</li>
</ol>
</li>
<li><strong>streaming-source.monitor-interval</strong> 监听新分区生成的时间、不宜过短 、最短是 1 个小时，因为目前的实现是每个 task 都会查询 metastore，高频的查可能会对 metastore 产生过大的压力。需要注意的是，1.12.1 放开了这个限制，但仍建议按照实际业务不要配个太短的 interval。</li>
<li><strong>streaming-source.partition-order</strong> 分区策略，主要有以下 3 种，其中最为推荐的是 <strong>partition-name</strong>：<ol>
<li>partition-name 使用默认分区名称顺序加载最新分区</li>
<li>create-time 使用分区文件创建时间顺序</li>
<li>partition-time 使用分区时间顺序</li>
</ol>
</li>
</ul>
<h2 id="6-状态管理"><a href="#6-状态管理" class="headerlink" title="6  状态管理"></a>6  状态管理</h2><p>我们前面写的 wordcount 的例子，没有包含状态管理。如果一个 task 在处理过程中挂掉了，那么它在内存中的状态都会丢失，所有的数据都需要重新计算。<strong>从容错和消息处理的语义上(at least once, exactly once)，Flink 引入了 state 和 checkpoint。</strong></p>
<blockquote>
<p>因此可以说 flink 因为引入了 state 和 checkpoint 所以才支持的 exactly once</p>
</blockquote>
<p>首先区分一下两个概念：</p>
<p><strong>state</strong>：</p>
<p>state 一般指一个具体的 task&#x2F;operator 的状态：</p>
<ul>
<li>state 数据默认保存在 java 的堆内存中，TaskManager 节点的内存中。</li>
<li>operator 表示一些算子在运行的过程中会产生的一些中间结果。</li>
</ul>
<p><strong>checkpoint</strong>：</p>
<p><em>checkpoint 可以理解为 checkpoint 是把 state 数据<strong>定时持久化存储</strong>了</em>，则表示了一个 Flink Job 在一个特定时刻的一份<strong>全局状态快照</strong>，即<strong>包含了所有 task&#x2F;operator 的状态。</strong></p>
<blockquote>
<p>注意：task(subTask)是 Flink 中执行的基本单位。operator 指算子(transformation)</p>
</blockquote>
<p>State 可以被记录，在失败的情况下数据还可以恢复。</p>
<p>Flink 中有两种基本类型的 State：</p>
<ul>
<li>Keyed State</li>
<li>Operator State</li>
</ul>
<p>Keyed State 和 Operator State，可以以两种形式存在：</p>
<ul>
<li>原始状态(raw state)</li>
<li>托管状态(managed state)</li>
</ul>
<p>托管状态是由 Flink 框架管理的状态。</p>
<blockquote>
<p>我们说 operator 算子保存了数据的中间结果，中间结果保存在什么类型中，如果我们这里是托管状态，则由 flink 框架自行管理</p>
</blockquote>
<p>原始状态由用户自行管理状态具体的数据结构，框架在做 checkpoint 的时候，使用 byte[]来读写状态内容，对其内部数据结构一无所知。</p>
<p><strong>通常在 DataStream 上的状态推荐使用托管的状态，当实现一个用户自定义的 operator 时，会使用到原始状态。</strong></p>
<h3 id="6-1-State-Keyed-State"><a href="#6-1-State-Keyed-State" class="headerlink" title="6.1  State-Keyed State"></a>6.1  State-Keyed State</h3><p>基于 KeyedStream 上的状态。这个状态是跟特定的 key 绑定的，对 KeyedStream 流上的每一个 key，都对应一个 state，比如：stream.keyBy(…)。KeyBy 之后的 Operator State,可以理解为分区过的 Operator State。</p>
<p>保存 state 的数据结构：</p>
<p><strong>ValueState</strong>：即类型为 T 的单值状态。这个状态与对应的 key 绑定，是最简单的状态了。它可以通过 update 方法更新状态值，通过 value()方法获取状态值。</p>
<p><strong>ListState</strong>：即 key 上的状态值为一个列表。可以通过 add 方法往列表中附加值；也可以通过 get()方法返回一个 Iterable来遍历状态值。</p>
<p><strong>ReducingState</strong>:这种状态通过用户传入的 reduceFunction，<strong>每次调用 add 方法添加值的时候，会调用 reduceFunction</strong>，最后合并到一个单一的状态值。</p>
<p><strong>MapState</strong>&lt;UK, UV&gt;:即状态值为一个 map。<strong>用户通过 put 或 putAll 方法添加元素。</strong></p>
<p><strong>需要注意的是</strong>，以上所述的 State 对象，仅仅用于与状态进行交互（更新、删除、清空等），而真正的状态值，有可能是存在内存、磁盘、或者其他分布式存储系统中。<strong>相当于我们只是持有了这个状态的句柄。</strong></p>
<h4 id="1-ValueState"><a href="#1-ValueState" class="headerlink" title="1. ValueState"></a>1. ValueState</h4><p>使用 ValueState 保存中间结果对下面数据进行分组求和。</p>
<p>开发步骤：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 获取流处理执行环境</span><br><span class="line">2. 加载数据源</span><br><span class="line">3. 数据分组</span><br><span class="line">4. 数据转换，定义ValueState,保存中间结果</span><br><span class="line">5. 数据打印</span><br><span class="line">6. 触发执行</span><br></pre></td></tr></table></figure></div>

<p>ValueState:测试数据源：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> <span class="type">List</span>(</span><br><span class="line">   (<span class="number">1</span>L, <span class="number">4</span>L),</span><br><span class="line">   (<span class="number">2</span>L, <span class="number">3</span>L),</span><br><span class="line">   (<span class="number">3</span>L, <span class="number">1</span>L),</span><br><span class="line">   (<span class="number">1</span>L, <span class="number">2</span>L),</span><br><span class="line">   (<span class="number">3</span>L, <span class="number">2</span>L),</span><br><span class="line">   (<span class="number">1</span>L, <span class="number">2</span>L),</span><br><span class="line">   (<span class="number">2</span>L, <span class="number">2</span>L),</span><br><span class="line">   (<span class="number">2</span>L, <span class="number">9</span>L)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>

<p>示例代码：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichFlatMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.&#123;<span class="type">TypeHint</span>, <span class="type">TypeInformation</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestKeyedState</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">CountWithKeyedState</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>[(<span class="type">Long</span>, <span class="type">Long</span>), (<span class="type">Long</span>, <span class="type">Long</span>)] </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * ValueState状态句柄. 第一个值为count，第二个值为sum。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> sum: <span class="type">ValueState</span>[(<span class="type">Long</span>, <span class="type">Long</span>)] = _</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(input: (<span class="type">Long</span>, <span class="type">Long</span>), out: <span class="type">Collector</span>[(<span class="type">Long</span>, <span class="type">Long</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 获取当前状态值</span></span><br><span class="line">      <span class="keyword">val</span> tmpCurrentSum: (<span class="type">Long</span>, <span class="type">Long</span>) = sum.value</span><br><span class="line">      <span class="comment">// 状态默认值</span></span><br><span class="line">      <span class="keyword">val</span> currentSum = <span class="keyword">if</span> (tmpCurrentSum != <span class="literal">null</span>) &#123;</span><br><span class="line">        tmpCurrentSum</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        (<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 更新</span></span><br><span class="line">      <span class="keyword">val</span> newSum = (currentSum._1 + <span class="number">1</span>, currentSum._2 + input._2)</span><br><span class="line">      <span class="comment">// 更新状态值</span></span><br><span class="line">      sum.update(newSum)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 如果count &gt;=3 清空状态值，重新计算</span></span><br><span class="line">      <span class="keyword">if</span> (newSum._1 &gt;= <span class="number">3</span>) &#123;</span><br><span class="line">        out.collect((input._1, newSum._2 / newSum._1))</span><br><span class="line">        sum.clear()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      sum = getRuntimeContext.getState(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[(<span class="type">Long</span>, <span class="type">Long</span>)](<span class="string">&quot;average&quot;</span>, <span class="comment">// 状态名称</span></span><br><span class="line">          <span class="type">TypeInformation</span>.of(<span class="keyword">new</span> <span class="type">TypeHint</span>[(<span class="type">Long</span>, <span class="type">Long</span>)]()&#123;&#125;) )<span class="comment">// 状态类型</span></span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//初始化执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//构建数据源</span></span><br><span class="line">    <span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[(<span class="type">Long</span>, <span class="type">Long</span>)] = env.fromCollection(</span><br><span class="line">      <span class="type">List</span>(</span><br><span class="line">        (<span class="number">1</span>L, <span class="number">4</span>L),</span><br><span class="line">        (<span class="number">2</span>L, <span class="number">3</span>L),</span><br><span class="line">        (<span class="number">3</span>L, <span class="number">1</span>L),</span><br><span class="line">        (<span class="number">1</span>L, <span class="number">2</span>L),</span><br><span class="line">        (<span class="number">3</span>L, <span class="number">2</span>L),</span><br><span class="line">        (<span class="number">1</span>L, <span class="number">2</span>L),</span><br><span class="line">        (<span class="number">2</span>L, <span class="number">2</span>L),</span><br><span class="line">        (<span class="number">2</span>L, <span class="number">9</span>L))</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//执行数据处理</span></span><br><span class="line">    inputStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      .flatMap(<span class="keyword">new</span> <span class="type">CountWithKeyedState</span>)</span><br><span class="line">      .setParallelism(<span class="number">1</span>)</span><br><span class="line">      .print</span><br><span class="line">    <span class="comment">//运行任务</span></span><br><span class="line">    env.execute</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="2-MapState"><a href="#2-MapState" class="headerlink" title="2. MapState"></a>2. MapState</h4><p>使用 MapState 保存中间结果对下面数据进行分组求和:</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 获取流处理执行环境</span><br><span class="line">2. 加载数据源</span><br><span class="line">3. 数据分组</span><br><span class="line">4. 数据转换，定义MapState,保存中间结果</span><br><span class="line">5. 数据打印</span><br><span class="line">6. 触发执行</span><br></pre></td></tr></table></figure></div>

<p>MapState:测试数据源：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">List</span>(</span><br><span class="line">   (<span class="string">&quot;java&quot;</span>, <span class="number">1</span>),</span><br><span class="line">   (<span class="string">&quot;python&quot;</span>, <span class="number">3</span>),</span><br><span class="line">   (<span class="string">&quot;java&quot;</span>, <span class="number">2</span>),</span><br><span class="line">   (<span class="string">&quot;scala&quot;</span>, <span class="number">2</span>),</span><br><span class="line">   (<span class="string">&quot;python&quot;</span>, <span class="number">1</span>),</span><br><span class="line">   (<span class="string">&quot;java&quot;</span>, <span class="number">1</span>),</span><br><span class="line">   (<span class="string">&quot;scala&quot;</span>, <span class="number">2</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>

<p>示例代码：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MapState</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 使用MapState保存中间结果对下面数据进行分组求和</span></span><br><span class="line"><span class="comment">      * 1.获取流处理执行环境</span></span><br><span class="line"><span class="comment">      * 2.加载数据源</span></span><br><span class="line"><span class="comment">      * 3.数据分组</span></span><br><span class="line"><span class="comment">      * 4.数据转换，定义MapState,保存中间结果</span></span><br><span class="line"><span class="comment">      * 5.数据打印</span></span><br><span class="line"><span class="comment">      * 6.触发执行</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = env.fromCollection(<span class="type">List</span>(</span><br><span class="line">      (<span class="string">&quot;java&quot;</span>, <span class="number">1</span>),</span><br><span class="line">      (<span class="string">&quot;python&quot;</span>, <span class="number">3</span>),</span><br><span class="line">      (<span class="string">&quot;java&quot;</span>, <span class="number">2</span>),</span><br><span class="line">      (<span class="string">&quot;scala&quot;</span>, <span class="number">2</span>),</span><br><span class="line">      (<span class="string">&quot;python&quot;</span>, <span class="number">1</span>),</span><br><span class="line">      (<span class="string">&quot;java&quot;</span>, <span class="number">1</span>),</span><br><span class="line">      (<span class="string">&quot;scala&quot;</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">    source.keyBy(<span class="number">0</span>)</span><br><span class="line">      .map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>, <span class="type">Int</span>), (<span class="type">String</span>, <span class="type">Int</span>)] &#123;</span><br><span class="line">        <span class="keyword">var</span> mste: <span class="type">MapState</span>[<span class="type">String</span>, <span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> msState = <span class="keyword">new</span> <span class="type">MapStateDescriptor</span>[<span class="type">String</span>, <span class="type">Int</span>](<span class="string">&quot;ms&quot;</span>,</span><br><span class="line">            <span class="type">TypeInformation</span>.of(<span class="keyword">new</span> <span class="type">TypeHint</span>[(<span class="type">String</span>)] &#123;&#125;),</span><br><span class="line">            <span class="type">TypeInformation</span>.of(<span class="keyword">new</span> <span class="type">TypeHint</span>[(<span class="type">Int</span>)] &#123;&#125;))</span><br><span class="line"></span><br><span class="line">          mste = getRuntimeContext.getMapState(msState)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>, <span class="type">Int</span>) = &#123;</span><br><span class="line">          <span class="keyword">val</span> i: <span class="type">Int</span> = mste.get(value._1)</span><br><span class="line">          mste.put(value._1, value._2 + i)</span><br><span class="line">          (value._1, value._2 + i)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="6-2-State-Operator-State"><a href="#6-2-State-Operator-State" class="headerlink" title="6.2   State-Operator State"></a>6.2   State-Operator State</h3><p><strong>与 Key 无关的 State，与 Operator 绑定的 state，整个 operator 只对应一个 state</strong>。</p>
<p>保存 state 的数据结构：</p>
<p>ListState</p>
<p>举例来说，Flink 中的 Kafka Connector，就使用了 operator state。它会在每个 connector 实例中，保存该实例中消费 topic 的所有(partition, offset)映射。</p>
<p>步骤：</p>
<ol>
<li>获取执行环境</li>
<li>设置检查点机制：路径，重启策略</li>
<li>自定义数据源<ul>
<li>需要继承并行数据源和 CheckpointedFunction</li>
<li>设置 listState,通过上下文对象 context 获取</li>
<li>数据处理，保留 offset</li>
<li>制作快照</li>
</ul>
</li>
<li>数据打印</li>
<li>触发执行</li>
</ol>
<p>示例代码：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.restartstrategy.<span class="type">RestartStrategies</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ListState</span>, <span class="type">ListStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.&#123;<span class="type">TypeHint</span>, <span class="type">TypeInformation</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.&#123;<span class="type">FunctionInitializationContext</span>, <span class="type">FunctionSnapshotContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.filesystem.<span class="type">FsStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.checkpoint.<span class="type">CheckpointedFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">RichParallelSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ListOperate</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">FsStateBackend</span>(<span class="string">&quot;hdfs://node01:8020/tmp/check/8&quot;</span>))</span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    env.getCheckpointConfig.setFailOnCheckpointingErrors(<span class="literal">false</span>)</span><br><span class="line">        env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重启策略</span></span><br><span class="line">    env.setRestartStrategy(<span class="type">RestartStrategies</span>.failureRateRestart(<span class="number">3</span>, <span class="type">Time</span>.minutes(<span class="number">1</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//模拟kakfa偏移量</span></span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">MyRichParrelSourceFun</span>)</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRichParrelSourceFun</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>[<span class="type">String</span>]</span></span><br><span class="line">  <span class="keyword">with</span> <span class="type">CheckpointedFunction</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> listState: <span class="type">ListState</span>[<span class="type">Long</span>] = _</span><br><span class="line">  <span class="keyword">var</span> offset: <span class="type">Long</span> = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务运行</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> iterState: util.<span class="type">Iterator</span>[<span class="type">Long</span>] = listState.get().iterator()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (iterState.hasNext) &#123;</span><br><span class="line">      offset = iterState.next()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">      offset += <span class="number">1</span></span><br><span class="line">      ctx.collect(<span class="string">&quot;offset:&quot;</span>+offset)</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">      <span class="keyword">if</span>(offset &gt; <span class="number">10</span>)&#123;</span><br><span class="line">        <span class="number">1</span>/<span class="number">0</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//取消任务</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = ???</span><br><span class="line"></span><br><span class="line">  <span class="comment">//制作快照</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">snapshotState</span></span>(context: <span class="type">FunctionSnapshotContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    listState.clear()</span><br><span class="line">    listState.add(offset)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//初始化状态</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeState</span></span>(context: <span class="type">FunctionInitializationContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    listState = context.getOperatorStateStore.getListState(<span class="keyword">new</span> <span class="type">ListStateDescriptor</span>[<span class="type">Long</span>](</span><br><span class="line">      <span class="string">&quot;listState&quot;</span>, <span class="type">TypeInformation</span>.of(<span class="keyword">new</span> <span class="type">TypeHint</span>[<span class="type">Long</span>] &#123;&#125;)</span><br><span class="line">    ))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="6-3-Broadcast-State"><a href="#6-3-Broadcast-State" class="headerlink" title="6.3  Broadcast State"></a>6.3  Broadcast State</h3><p>Broadcast State 是 Flink 1.5 引入的新特性。在开发过程中，如果遇到需要下发&#x2F;广播配置、规则等低吞吐事件流到下游所有 task 时，就可以使用 Broadcast State 特性。下游的 task 接收这些配置、规则并保存为 BroadcastState, 将这些配置应用到另一个数据流的计算中 。</p>
<h4 id="1-API-介绍"><a href="#1-API-介绍" class="headerlink" title="1) API 介绍"></a>1) API 介绍</h4><p><strong>通常，我们首先会创建一个 Keyed 或 Non-Keyed 的 Data Stream，然后再创建一个 Broadcasted Stream，最后通过 Data Stream 来连接（调用 connect 方法）到 Broadcasted Stream 上，这样实现将 Broadcast State 广播到 Data Stream 下游的每个 Task 中。</strong></p>
<p>如果 Data Stream 是 Keyed Stream，则连接到 Broadcasted Stream 后，添加处理 ProcessFunction 时需要使用 KeyedBroadcastProcessFunction 来实现，下面是 KeyedBroadcastProcessFunction 的 API，代码如下所示：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedBroadcastProcessFunction&lt;KS</span>, <span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT&gt;</span> <span class="keyword">extends</span> <span class="title">BaseBroadcastProcessFunction</span> </span>&#123;</span><br><span class="line">    public <span class="keyword">abstract</span> void processElement(<span class="keyword">final</span> <span class="type">IN1</span> value, <span class="keyword">final</span> <span class="type">ReadOnlyContext</span> ctx, <span class="keyword">final</span> <span class="type">Collector</span>&lt;<span class="type">OUT</span>&gt; out) <span class="keyword">throws</span> <span class="type">Exception</span>;</span><br><span class="line">    public <span class="keyword">abstract</span> void processBroadcastElement(<span class="keyword">final</span> <span class="type">IN2</span> value, <span class="keyword">final</span> <span class="type">Context</span> ctx, <span class="keyword">final</span> <span class="type">Collector</span>&lt;<span class="type">OUT</span>&gt; out) <span class="keyword">throws</span> <span class="type">Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p>上面泛型中的各个参数的含义，说明如下：</p>
<ul>
<li>KS：表示 Flink 程序从最上游的 Source Operator 开始构建 Stream，当调用 keyBy 时所依赖的 Key 的类型；</li>
<li>IN1：表示非 Broadcast 的 Data Stream 中的数据记录的类型；</li>
<li>IN2：表示 Broadcast Stream 中的数据记录的类型；</li>
<li>OUT：表示经过 KeyedBroadcastProcessFunction 的 processElement()和 processBroadcastElement()方法处理后输出结果数据记录的类型。</li>
</ul>
<p>如果 Data Stream 是 Non-Keyed Stream，则连接到 Broadcasted Stream 后，添加处理 ProcessFunction 时需要使用 BroadcastProcessFunction 来实现，下面是 BroadcastProcessFunction 的 API，代码如下所示：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastProcessFunction&lt;IN1</span>, <span class="title">IN2</span>, <span class="title">OUT&gt;</span> <span class="keyword">extends</span> <span class="title">BaseBroadcastProcessFunction</span> </span>&#123;</span><br><span class="line">		public <span class="keyword">abstract</span> void processElement(<span class="keyword">final</span> <span class="type">IN1</span> value, <span class="keyword">final</span> <span class="type">ReadOnlyContext</span> ctx, <span class="keyword">final</span> <span class="type">Collector</span>&lt;<span class="type">OUT</span>&gt; out) <span class="keyword">throws</span> <span class="type">Exception</span>;</span><br><span class="line">		public <span class="keyword">abstract</span> void processBroadcastElement(<span class="keyword">final</span> <span class="type">IN2</span> value, <span class="keyword">final</span> <span class="type">Context</span> ctx, <span class="keyword">final</span> <span class="type">Collector</span>&lt;<span class="type">OUT</span>&gt; out) <span class="keyword">throws</span> <span class="type">Exception</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></div>

<p>上面泛型中的各个参数的含义，与前面 KeyedBroadcastProcessFunction 的泛型类型中的后 3 个含义相同，只是没有调用 keyBy 操作对原始 Stream 进行分区操作，就不需要 KS 泛型参数。</p>
<p>注意事项：</p>
<ol>
<li>Broadcast State 是 Map 类型，即 K-V 类型。</li>
<li>Broadcast State 只有在广播一侧的方法中 processBroadcastElement 可以修改;在非广播一侧方法中 processElement 只读。</li>
<li>Broadcast State 在运行时保存在内存中。</li>
</ol>
<h4 id="2-场景举例"><a href="#2-场景举例" class="headerlink" title="2) 场景举例"></a>2) 场景举例</h4><ol>
<li>动态更新计算规则: 如事件流需要根据最新的规则进行计算，则可将规则作为广播状态广播到下游 Task 中。</li>
<li>实时增加额外字段: 如事件流需要实时增加用户的基础信息，则可将用户的基础信息作为广播状态广播到下游 Task 中。</li>
</ol>
<h3 id="6-4-调整有状态算子的并行度"><a href="#6-4-调整有状态算子的并行度" class="headerlink" title="6.4  调整有状态算子的并行度"></a>6.4  调整有状态算子的并行度</h3><p>流应用程序的一个常见要求是，<strong>为了增大或较小输入数据的速率，需要灵活地调整算子的并行度。</strong>对于无状态算子而言，并行度的调整没有任何问题，但更改有状态算子的并行度显然就没那么简单了，因为它们的状态需要重新分区并分配给更多或更少的并行任务。Flink 支持四种模式来调整不同类型的状态。</p>
<p>具有键控状态的算子通过<strong>将键重新分区为更少或更多任务来缩放并行度</strong>。不过，并行度调整时任务之间会有一些必要的状态转移。为了提高效率，Flink 并不会对单独的key做重新分配，而是用<strong>所谓的‘‘键组’’（key group）把键管理起来</strong>。键组是key 的分区形式，同时也是Flink 为任务分配key 的方式。图3-13 显示了如何在键组中重新分配键控状态。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228144537775.png"
                      alt="image-20230228144537775"
                ></p>
<p>具有算子列表状态的算子，<strong>会通过重新分配列表中的数据项目来进行并行度缩放</strong>。从概念上讲，所有并行算子任务的列表项目会被收集起来，并将其均匀地重新分配给更少或更多的任务<strong>。如果列表条目少于算子的新并行度，则某些任务将以空状态开始。</strong>图3-14 显示了算子列表状态的重新分配。</p>
<p>具有算子联合列表状态的算子，会通过向每个任务广播状态的完整列表，来进行并行度的缩放。然后，任务可以选择要使用的状态项和要丢弃的状态项。图3-15 显示了如何重新分配算子联合列表状态。</p>
<p><strong>具有算子广播状态的算子，通过将状态复制到新任务，来增大任务的并行度。</strong>这是没问题的，因为广播状态保证了所有任务都具有相同的状态。而对于缩小并行度的情况，我们可以直接取消剩余任务，因为状态是相同的，已经被复制并且不会丢失。图3-16 显示了算子广播状态的重新分配。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228144846417.png"
                      alt="image-20230228144846417"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228145022090.png"
                      alt="image-20230228145022090"
                ></p>
<h2 id="7-Flink的容错"><a href="#7-Flink的容错" class="headerlink" title="7  Flink的容错"></a>7  Flink的容错</h2><h3 id="7-1-Checkpoint-介绍"><a href="#7-1-Checkpoint-介绍" class="headerlink" title="7.1  Checkpoint 介绍"></a>7.1  Checkpoint 介绍</h3><p>checkpoint 机制是 Flink 可靠性的基石，可以保证 Flink 集群在某个算子因为某些原因(如 异常退出)出现故障时，能够将整个应用流图的状态恢复到故障之前的某一状态，保 证应用流图状态的一致性。Flink 的 checkpoint 机制原理来自“Chandy-Lamport algorithm”算法。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228145513761.png"
                      alt="image-20230228145513761"
                ></p>
<p>每个需要 checkpoint 的应用在启动时，Flink 的 <strong>JobManager</strong> 为其创建一个 **CheckpointCoordinator(检查点协调器)**，CheckpointCoordinator 全权负责本应用的快照制作。 </p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210124_1.png"
                      alt="img"
                ></p>
<ol>
<li>CheckpointCoordinator(检查点协调器) 周期性的向该流应用的所有 source 算子发送 barrier(屏障)。</li>
<li>当某个 source 算子收到一个 barrier 时，便暂停数据处理过程，然后将自己的当前状态制作成快照，并保存到指定的持久化存储中，最后向 CheckpointCoordinator 报告自己快照制作情况，<strong>同时向自身所有下游算子广播该 barrier，恢复数据处理</strong></li>
<li>下游算子收到 barrier 之后，会暂停自己的数据处理过程，然后将自身的相关状态制作成快照，并保存到指定的持久化存储中，最后向 CheckpointCoordinator 报告自身快照情况，<strong>同时向自身所有下游算子广播该 barrier</strong>，恢复数据处理。</li>
<li>每个算子按照步骤 3 不断制作快照并向下游广播，直到最后 barrier 传递到 sink 算子，快照制作完成。</li>
<li>当 CheckpointCoordinator 收到所有算子的报告之后，认为该周期的快照制作成功; 否则，如果<strong>在规定的时间内</strong>没有收到所有算子的报告，则认为本周期快照制作失败。</li>
</ol>
<p>如果一个算子有两个输入源，则暂时阻塞先收到 barrier 的输入源，等到<strong>第二个输入源相 同编号的 barrier 到来</strong>时，再制作自身快照并向下游广播该 barrier。具体如下图所示：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210124_2.png"
                      alt="img"
                ></p>
<ol>
<li>假设算子 C 有 A 和 B 两个输入源</li>
<li>在第 i 个快照周期中，由于某些原因(如处理时延、网络时延等)输入源 A 发出的 barrier 先到来，这时算子 C 暂时将输入源 A 的输入通道阻塞，仅收输入源 B 的数据。</li>
<li>当输入源 B 发出的 barrier 到来时，算子 C 制作自身快照并向 CheckpointCoordinator 报告自身的快照制作情况，然后将两个 barrier 合并为一个，向下游所有的算子广播。</li>
<li>当由于某些原因出现故障时，CheckpointCoordinator 通知流图上所有算子统一恢复到某个周期的 checkpoint 状态，然后恢复数据流处理。分布式 checkpoint 机制保证了数据仅被处理一次(Exactly Once)。</li>
</ol>
<h3 id="7-1-从一致性检查点中恢复状态"><a href="#7-1-从一致性检查点中恢复状态" class="headerlink" title="7.1  从一致性检查点中恢复状态"></a>7.1  从一致性检查点中恢复状态</h3><p>在执行流应用程序期间，Flink 会定期检查状态的一致检查点。如果发生故障，Flink将会使用最近的检查点来一致恢复应用程序的状态，并重新启动处理流程。图3-18 显示了恢复过程。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228145806240.png"
                      alt="image-20230228145806240"
                ></p>
<p>应用程序从检查点的恢复分为三步：</p>
<ul>
<li>重新启动整个应用程序。</li>
<li>将所有的有状态任务的状态重置为最近一次的检查点。</li>
<li>恢复所有任务的处理。</li>
</ul>
<p>这种检查点的保存和恢复机制可以为应用程序状态提供<strong>“精确一次”（exactly-once）的一致性</strong>，因为所有算子都会保存检查点并恢复其所有状态，这样一来<strong>所有的输入流就都会被重置到检查点完成时的位置</strong>。至于数据源是否可以重置它的输入流，这取决于其实现方式和消费流数据的外部接口。例如，像Apache Kafka 这样的事件日志系统可以提供流上之前偏移位置的数据，所以<strong>我们可以将源重置到之前的偏移量</strong>，重新消费数据。而从套接字（socket）消费数据的流就不能被重置了，因为套接字的数据一旦被消费就会丢弃掉。因此，对于应用程序而言，只有当所有的输入流消费的都是可重置的数据源时，才能确保在“精确一次”的状态一致性下运行。</p>
<p>从检查点重新启动应用程序后，其内部状态与检查点完成时的状态完全相同。然后它就会开始消费并处理检查点和发生故障之间的所有数据。尽管这意味着Flink 会对一些数据处理两次（在故障之前和之后），我们仍然可以说这个机制实现了精确一次的一致性语义，<strong>因为所有算子的状态都已被重置，而重置后的状态下还不曾看到这些数据。</strong></p>
<p>我们必须指出，Flink 的检查点保存和恢复机制仅仅可以重置流应用程序的内部状态。对于应用中的一些的输出（sink）算子，在恢复期间，某些结果数据可能会多次发送到下游系统，比如事件日志、文件系统或数据库。对于某些存储系统，Flink 提供了具有精确一次输出功能的sink 函数，比如，可以在检查点完成时提交发出的记录。另一种适用于许多存储系统的方法是幂等更新。在“应用程序一致性保证”一节中，我们还会详<br>细讨论如何解决应用程序端到端的精确一次一致性问题。</p>
<h3 id="7-2-Flink的检查点算法"><a href="#7-2-Flink的检查点算法" class="headerlink" title="7.2  Flink的检查点算法"></a>7.2  Flink的检查点算法</h3><p>Flink 的恢复机制，基于它的一致性检查点。前面我们已经了解了从流应用中创建检查点的简单方法——先暂停应用，保存检查点，然后再恢复应用程序，这种方法很好理解，但它的理念是“停止一切”，这对于即使是中等延迟要求的应用程序而言也是不实用的。所以Flink 没有这么简单粗暴，而是基于Chandy-Lamport 算法实现了<strong>分布式快照的检查点保存。</strong>该算法并不会暂停整个应用程序，而是将检查点的保存与数据处理分离，<br>这样就可以实现在其它任务做检查点状态保存状态时，让某些任务继续进行而不受影响。</p>
<p>接下来我们将解释此算法的工作原理。</p>
<p>Flink 的检查点算法用到了一种称为<strong>“检查点分界线”（checkpoint barrier）</strong>的特殊数据形式。<strong>与水位线（watermark）类似，检查点分界线由source 算子注入到常规的数据流中，它的位置是限定好的，不能超过其他数据，也不能被后面的数据超过</strong>。检查点分界线带有检查点ID，用来标识它所属的检查点；这样，这个分界线就将一条流逻辑上分成了两部分。分界线之前到来的数据导致的状态更改，都会被包含在当前分界线所属的检查点中；而基于分界线之后的数据导致的所有更改，就会被包含在之后的检查点中。我们用一个简单的流应用程序作为示例，来一步一步解释这个算法。该应用程序有两个源（source）任务，每个任务都消费一个增长的数字流。源任务的输出被划分为两部分：偶数和奇数的流。每个分区由一个任务处理，该任务计算所有收到的数字的总和，并将更新的总和转发给输出（sink）任务。这个应用程序的结构如图 所示。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228150614198.png"
                      alt="image-20230228150614198"
                ></p>
<p>作业管理器会向每个数据源（source）任务发送一条带有新检查点ID 的消息，通过这种方式来启动检查点，如图所示。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228150703072.png"
                      alt="image-20230228150703072"
                ></p>
<p>当source 任务收到消息时，它会<strong>暂停发出新的数据</strong>，在<strong>状态后端触发本地状态的检查点保存</strong>，并向所有传出的流分区<strong>广播</strong>带着<strong>检查点ID 的分界线（barriers）</strong>。状态后端在状态检查点完成后会通知任务，而<strong>任务会向作业管理器确认检查点完成</strong>。在发出所有分界线后，source 任务就可以继续常规操作，发出新的数据了。通过将分界线注入到输出流中，源函数（source function）定义了检查点在流中所处的位置。图3-21 显示了两个源任务将本地状态保存到检查点，并发出检查点分界线之后的流应用程序。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228151003991.png"
                      alt="image-20230228151003991"
                ></p>
<p>源任务发出的检查点分界线（barrier），将被传递给所连接的任务。与水位线（watermark）类似，barrier 会被广播到所有连接的并行任务，以确保每个任务从它的每个输入流中都能接收到。<strong>当任务收到一个新检查点的barrier 时，它会等待这个检查点的所有输入分区的barrier 到达。</strong>在等待的过程中，任务并不会闲着，而是<strong>会继续处理尚未提供barrier 的流分区中的数据</strong>。对于那些barrier 已经到达的分区，如果继续有新的数据到达，<strong>它们就不会被立即处理，而是先缓存起来</strong>。这个等待所有分界线到达的过程，称为<strong>‘‘分界线对齐’’（barrier alignment）</strong>，如图所示。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228151314875.png"
                      alt="image-20230228151314875"
                ></p>
<p>当任务从所有输入分区都收到barrier 时，它就会在状态后端启动一个检查点的保存，并继续向所有下游连接的任务<strong>广播</strong>检查点分界线，如图所示。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228151407703.png"
                      alt="image-20230228151407703"
                ></p>
<p>所有的检查点barrier 都发出后，任务就开始处理之前缓冲的数据。在处理并发出所有缓冲数据之后，任务就可以继续正常处理输入流了。图显示了此时的应用程序。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228151544280.png"
                      alt="image-20230228151544280"
                ></p>
<p>最终，检查点分界线会到达输出（sink）任务。当sink 任务接收到barrier 时，它也会先执行‘‘分界线对齐’’，然后将自己的状态保存到检查点，并向作业管理器确认已接收到barrier。<strong>一旦从应用程序的所有任务收到一个检查点的确认信息，作业管理器就会将这个检查点记录为已完成</strong>。图 显示了检查点算法的最后一步。这样，当发生故障时，我们就可以用已完成的检查点恢复应用程序了。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228151657825.png"
                      alt="image-20230228151657825"
                ></p>
<p>&#x3D;&#x3D;<strong>检查点算法流程总结：</strong>&#x3D;&#x3D;</p>
<p><strong>jobManager发送带id的检查点:<strong>首先会由jobManager向source 任务发送一个带id的检查点，source收到以后会将自己的状态进行保存到检查点，并向下游算子</strong>广播</strong>这个带id的检查点checkpoint，任务会向作业管理器确认检查点完成，此时数据就被分割为了两部分(在检查点之前 和  在检查点之后)。 </p>
<p><strong>下游算子：</strong>而下游算子在在收到检查点之前正常处理数据，在收到这个分区的检查点，就不再处理这个分区来的数据了，而是将这些数据缓存起来。其他分区数据依然正常处理，直到那个分区的检查点也被该算子收到。这种状态一直持续到所有分区的检查点都送到了(等待的过程叫做分界线对齐barrier alignment)。当分界线对齐之后，就会将自己的状态进行检查点保存。接着向下游继续广播这个带id的检查点。广播检查点之后开始处理缓存的数据。</p>
<p>**sink:**到sink到达后也会先执行自己的分界线对齐，然后将自己的状态保存到检查点。然后向job manager确认已收到barrier。一旦收到了所有任务检查点确认信息，job manager就会将这个检查点标记为已完成。</p>
<h3 id="7-3-检查点的性能影响"><a href="#7-3-检查点的性能影响" class="headerlink" title="7.3  检查点的性能影响"></a>7.3  检查点的性能影响</h3><p>Flink 的检查点算法可以在不停止整个应用程序的情况下，生成一致的分布式检查点。但是，<strong>它可能会增加应用程序的处理延迟</strong>。Flink 对此有一些调整措施，可以在某些场景下显得对性能的影响没那么大。</p>
<p>当任务将其状态保存到检查点时，它其实处于一个阻塞状态，而此时新的输入会被缓存起来。由于状态可能变得非常大，而且检查点需要通过网络将数据写入远程存储系统，<strong>检查点的写入很容易就会花费几秒到几分钟的时间</strong>——这对于要求低延迟的应用程序而言，显然是不可接受的。在Flink 的设计中，<strong>真正负责执行检查点写入的，其实是状态后端</strong>。具体怎样复制任务的状态，取决于状态后端的实现方式。例如，文件系统（FileSystem）状态后端和RocksDB 状态后端都支持了<strong>异步（asynchronous）检查点</strong>。触发检查点操作时，状态后端会先创建状态的本地副本。<strong>本地拷贝完成后，任务就将继续常规的数据处理</strong>，这往往并不会花费太多时间。一个后台线程会将本地快照<strong>异步复制到远程存储</strong>，并在完成检查点后再回来通知任务。异步检查点的机制，<strong>显著减少了任务继续处理数据之前的等待时间。</strong>此外，RocksDB 状态后端还实现了增量的检查点，这样可以大大减少要传输的数据量。</p>
<p>为了减少检查点算法对处理延迟的影响，另一种技术是调整分界线对齐的步骤。对于需要非常低的延迟、并且可以容忍“至少一次”（at-least-once）状态保证的应用程序，Flink 可以将检查点算法配置为，在等待barrier 对齐期间处理所有到达的数据，而不是把barrier 已经到达的那些分区的数据缓存起来。当检查点的所有barrier 到达，算子任务就会将状态写入检查点——当然，现在的状态中，就可能包括了一些“提前”的更改，这些更改由本该属于下一个检查点的数据到来时触发。如果发生故障，从检查点恢复时，就将再次处理这些数据：这意味着检查点现在提供的是“至少一次”（at-least-once）而不是“精确一次”（exactly-once）的一致性保证。</p>
<p>&#x3D;&#x3D;总结:&#x3D;&#x3D;由于检查点到达后处于阻塞状态，且检查点的写入操作十分的耗时，增加处理延迟。因此需要使用几个方式减小影响。</p>
<p>1）负责检查点的写入的是状态后端。状态后端采用了异步检查点的策略。本地仅进行一个本地复制工作，就开始继续常规的数据处理。后台线程会继续将本地快照异步复制到远程存储，然后通知算子任务。</p>
<p>2）需要非常低的延迟、并且可以容忍“至少一次”（at-least-once）状态保证的应用程序。在等待barrier 对齐期间处理所有到达的数据，而不是把barrier 已经到达的那些分区的数据缓存起来。这样就会包括了一些”提前”处理</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228204145733.png"
                      alt="image-20230228204145733"
                ></p>
<h3 id="7-4-保存点"><a href="#7-4-保存点" class="headerlink" title="7.4  保存点"></a>7.4  保存点</h3><p>Flink 的恢复算法是基于状态检查点的。Flink 根据可配置的策略，定期保存并自动丢弃检查点。检查点的目的是确保在发生故障时可以重新启动应用程序，<strong>所以当应用程序被显式地撤销（cancel）时，检查点会被删除掉。</strong>除此之外，应用程序状态的一致性快照还可用于除故障恢复之外的更多功能。</p>
<p>Flink 中一个最有价值，也是最独特的功能是保存点（savepoints）。原则上，<strong>创建保存点使用的算法与检查点完全相同，因此保存点可以认为就是具有一些额外元数据的检查点。Flink 不会自动创建保存点，因此用户（或者外部调度程序）必须明确地触发创建操作。</strong>同样，Flink 也不会自动清理保存点。第10 章将会具体介绍如何触发和处理保存点。</p>
<p><strong>使用保存点</strong><br>有了应用程序和与之兼容的保存点，我们就可以从保存点启动应用程序了。这会将应用程序的状态初始化为保存点的状态，并从保存点创建时的状态开始运行应用程序。虽然看起来这种行为似乎与用检查点从故障中恢复应用程序完全相同，但实际上故障恢复只是一种特殊情况，它只是在相同的集群上以相同的配置启动相同的应用程序。而从保存点启动应用程序会更加灵活，这就可以让我们做更多事情了。</p>
<ul>
<li>可以从保存点启动不同但兼容的应用程序。这样一来，我们就可以及时修复应用程序中的逻辑bug，并让流式应用的源尽可能多地提供之前发生的事件，然后重新处理，以便修复之前的计算结果。修改后的应用程序还可用于运行A &#x2F; B 测试，或者具有不同业务逻辑的假设场景。这里要注意，应用程序和保存点必须兼容才可以这么做——也就是说，应用程序必须能够加载保存点的状态。</li>
<li><strong>可以使用不同的并行度来启动相同的应用程序，可以将应用程序的并行度增大或减小。</strong></li>
<li><strong>可以在不同的集群上启动同样的应用程序。</strong>这非常有意义，意味着我们可以将应用程序迁移到较新的Flink 版本或不同的集群上去。</li>
<li>可以使用保存点暂停应用程序，稍后再恢复。这样做的意义在于，可以为更高优先级的应用程序释放集群资源，或者在输入数据不连续生成时释放集群资源。</li>
<li>还可以将保存点设置为某一版本，并归档（archive）存储应用程序的状态。</li>
</ul>
<p>保存点是非常强大的功能，所以许多用户会定期创建保存点以便能够及时退回之前的状态。我们见到的各种场景中，保存点一个最有趣的应用是不断将流应用程序迁移到更便宜的数据中心上去。</p>
<p><strong>从保存点启动应用程序</strong><br>前面提到的保存点的所有用例，都遵循相同的模式。那就是首先创建正在运行的应用程序的保存点，然后在一个新启动的应用程序中用它来恢复状态。之前我们已经知道，保存点的创建和检查点非常相似，而接下来我们就将介绍对于一个从保存点启动的应用程序，Flink 如何初始化其状态。</p>
<p>应用程序由多个算子组成。每个算子可以定义一个或多个键控状态和算子状态。算子由一个或多个算子任务并行执行。因此，一个典型的应用程序会包含多个状态，这些状态分布在多个算子任务中，这些任务可以运行在不同的TaskManager 进程上。</p>
<p>图3-26 显示了一个具有三个算子的应用程序，每个算子执行两个算子任务。一个算子（OP-1）具有单一的算子状态（OS-1），而另一个算子（OP-2）具有两个键控状态（KS-1和KS-2）。当保存点创建时，会将所有任务的状态复制到持久化的存储位置。</p>
<p>保存点中的状态拷贝会以算子标识符（operator ID）和状态名称（state name）组织起来。算子ID 和状态名称必须能够将保存点的状态数据，映射到一个正在启动的应用程序的算子状态。从保存点启动应用程序时，Flink 会将保存点的数据重新分配给相应的算子任务。</p>
<blockquote>
<p>笔记请注意，保存点不包含有关算子任务的信息。这是因为当应用程序以不同的并行度启动时，任务数量可能会更改。</p>
</blockquote>
<p>如果我们要从保存点启动一个修改过的应用程序，那么保存点中的状态只能映射到符合标准的应用程序——它里面的算子必须具有相应的ID 和状态名称。默认情况下，Flink 会自动分配唯一的算子ID。然而，一个算子的ID，是基于它之前算子的ID 确定性地生成的。因此，算子的ID 会在其前序算子改变时改变，比如，当我们添加了新的或移除掉一个算子时，前序算子ID 改变，当前算子ID 就会变化。所以对于具有默认算子ID的应用程序而言，如果想在不丢失状态的前提下升级，就会受到极大的限制。因此，我们强烈建议在程序中为算子手动分配唯一ID，而不是依靠Flink 的默认分配。我们将在“指定唯一的算子标识符”一节中详细说明如何分配算子标识符。</p>
<h3 id="7-2-持久化存储-状态后端"><a href="#7-2-持久化存储-状态后端" class="headerlink" title="7.2  持久化存储(状态后端)"></a>7.2  持久化存储(状态后端)</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230228200949595.png"
                      alt="image-20230228200949595"
                ></p>
<h4 id="1-MemStateBackend"><a href="#1-MemStateBackend" class="headerlink" title="1) MemStateBackend"></a>1) MemStateBackend</h4><p>该持久化存储主要将快照数据保存到 JobManager 的内存中，仅适合作为测试以及快照的数据量非常小时使用，并不推荐用作大规模商业部署。</p>
<p><strong>MemoryStateBackend 的局限性</strong>：</p>
<p>默认情况下，每个状态的大小限制为 5 MB。可以在 MemoryStateBackend 的构造函数中增加此值。</p>
<p>无论配置的最大状态大小如何，状态都不能大于 akka 帧的大小（请参阅配置）。</p>
<p><strong>聚合状态必须适合 JobManager 内存。</strong></p>
<p><strong>建议 MemoryStateBackend 用于</strong>：</p>
<ul>
<li>本地开发和调试。</li>
<li>状态很少的作业，例如仅包含一次记录功能的作业（Map，FlatMap，Filter，…），kafka 的消费者需要很少的状态。</li>
</ul>
<h4 id="2-FsStateBackend"><a href="#2-FsStateBackend" class="headerlink" title="2) FsStateBackend"></a>2) FsStateBackend</h4><p>该持久化存储主要将快照数据保存到文件系统中，目前支持的文件系统主要是 HDFS 和本地文件。如果使用 HDFS，则初始化 FsStateBackend 时，需要传入以 “hdfs:&#x2F;&#x2F;”开头的路径(即: <code>new FsStateBackend(&quot;hdfs:///hacluster/checkpoint&quot;))</code>， 如果使用本地文件，则需要传入以“file:&#x2F;&#x2F;”开头的路径(即:new FsStateBackend(“file:&#x2F;&#x2F;&#x2F;Data”))。<strong>在分布式情况下，不推荐使用本地文件。</strong>如果某 个算子在节点 A 上失败，在节点 B 上恢复，使用本地文件时，在 B 上无法读取节点 A 上的数据，导致状态恢复失败。</p>
<p>建议 FsStateBackend：</p>
<ul>
<li>具有大状态，长窗口，大键 &#x2F; 值状态的作业。</li>
<li>所有高可用性设置。</li>
</ul>
<h4 id="3-RocksDBStateBackend"><a href="#3-RocksDBStateBackend" class="headerlink" title="3) RocksDBStateBackend"></a>3) RocksDBStateBackend</h4><p>RocksDBStatBackend 介于本地文件和 HDFS 之间，平时使用 RocksDB 的功能，将数据持久化到本地文件中，当制作快照时，将本地数据制作成快照，并持久化到 FsStateBackend 中(FsStateBackend 不必用户特别指明，只需在初始化时传入 HDFS 或本地路径即可，如<code> new RocksDBStateBackend(&quot;hdfs:///hacluster/checkpoint&quot;)</code>或<code> new RocksDBStateBackend(&quot;file:///Data&quot;))</code>。</p>
<p>如果用户使用自定义窗口(window)，不推荐用户使用 RocksDBStateBackend。在自定义窗口中，<strong>状态以 ListState 的形式</strong>保存在 StatBackend 中，<strong>如果一个 key 值中有多个 value 值，则 RocksDB 读取该种 ListState 非常缓慢</strong>，影响性能。用户可以根据应用的具体情况选择 FsStateBackend+HDFS 或 RocksStateBackend+HDFS。</p>
<h4 id="4-语法"><a href="#4-语法" class="headerlink" title="4) 语法"></a>4) 语法</h4><div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment()</span><br><span class="line"><span class="comment">// start a checkpoint every 1000 ms</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>)</span><br><span class="line"><span class="comment">// advanced options:</span></span><br><span class="line"><span class="comment">// 设置checkpoint的执行模式，最多执行一次或者至少执行一次</span></span><br><span class="line">env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line"><span class="comment">// 设置checkpoint的超时时间</span></span><br><span class="line">env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line"><span class="comment">// 如果在只做快照过程中出现错误，是否让整体任务失败：true是  false不是</span></span><br><span class="line">env.getCheckpointConfig.setFailTasksOnCheckpointingErrors(<span class="literal">false</span>)</span><br><span class="line"><span class="comment">//设置同一时间有多少 个checkpoint可以同时执行</span></span><br><span class="line">env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></div>

<h4 id="5-修改-State-Backend-的两种方式"><a href="#5-修改-State-Backend-的两种方式" class="headerlink" title="5) 修改 State Backend 的两种方式"></a>5) 修改 State Backend 的两种方式</h4><p><strong>第一种：单任务调整</strong></p>
<p>修改当前任务代码</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.setStateBackend(<span class="keyword">new</span> <span class="type">FsStateBackend</span>(<span class="string">&quot;hdfs://namenode:9000/flink/checkpoints&quot;</span>));</span><br></pre></td></tr></table></figure></div>

<p>或者<code>new MemoryStateBackend()</code></p>
<p>或者<code>new RocksDBStateBackend(filebackend, true);</code>【需要添加第三方依赖】</p>
<p><strong>第二种：全局调整</strong></p>
<p>修改<code>flink-conf.yaml</code></p>
<div class="highlight-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">state.backend:</span> <span class="string">filesystem</span></span><br><span class="line"><span class="attr">state.checkpoints.dir:</span> <span class="string">hdfs://namenode:9000/flink/checkpoints</span></span><br></pre></td></tr></table></figure></div>

<p>注意：state.backend 的值可以是下面几种：<code>jobmanager(MemoryStateBackend)</code>, <code>filesystem(FsStateBackend)</code>, <code>rocksdb(RocksDBStateBackend)</code></p>
<h4 id="6-Checkpoint-的高级选项"><a href="#6-Checkpoint-的高级选项" class="headerlink" title="6) Checkpoint 的高级选项"></a>6) Checkpoint 的高级选项</h4><p>默认 checkpoint 功能是 disabled 的，想要使用的时候需要先启用 checkpoint 开启之后，默认的 checkPointMode 是 Exactly-once</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//配置一秒钟开启一个checkpoint</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>)</span><br><span class="line"><span class="comment">//指定checkpoint的执行模式</span></span><br><span class="line"><span class="comment">//两种可选：</span></span><br><span class="line"><span class="comment">//CheckpointingMode.EXACTLY_ONCE：默认值</span></span><br><span class="line"><span class="comment">//CheckpointingMode.AT_LEAST_ONCE</span></span><br><span class="line"></span><br><span class="line">env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line"></span><br><span class="line">一般情况下选择<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>，除非场景要求极低的延迟（几毫秒）</span><br><span class="line"></span><br><span class="line">注意：如果需要保证<span class="type">EXACTLY_ONCE</span>，source和sink要求必须同时保证<span class="type">EXACTLY_ONCE</span></span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//如果程序被cancle，保留以前做的checkpoint</span></span><br><span class="line">env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line"></span><br><span class="line">默认情况下，检查点不被保留，仅用于在故障中恢复作业，可以启用外部持久化检查点，同时指定保留策略:</span><br><span class="line"></span><br><span class="line"><span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>:在作业取消时保留检查点，注意，在这种情况下，您必须在取消后手动清理检查点状态</span><br><span class="line"></span><br><span class="line"><span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">DELETE_ON_CANCELLATION</span>：当作业在被cancel时，删除检查点，检查点仅在作业失败时可用</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置checkpoint超时时间</span></span><br><span class="line">env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line"><span class="comment">//Checkpointing的超时时间，超时时间内没有完成则被终止</span></span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Checkpointing最小时间间隔，用于指定上一个checkpoint完成之后</span></span><br><span class="line"><span class="comment">//最小等多久可以触发另一个checkpoint，当指定这个参数时，maxConcurrentCheckpoints的值为1</span></span><br><span class="line">env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置同一个时间是否可以有多个checkpoint执行</span></span><br><span class="line">env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">指定运行中的checkpoint最多可以有多少个</span><br><span class="line"></span><br><span class="line">env.getCheckpointConfig.setFailOnCheckpointingErrors(<span class="literal">true</span>)</span><br><span class="line">用于指定在checkpoint发生异常的时候，是否应该fail该task，默认是<span class="literal">true</span>，如果设置为<span class="literal">false</span>，则task会拒绝checkpoint然后继续运行</span><br></pre></td></tr></table></figure></div>

<h3 id="7-3-0-Flink的各组件故障"><a href="#7-3-0-Flink的各组件故障" class="headerlink" title="7.3.0  Flink的各组件故障"></a>7.3.0  Flink的各组件故障</h3><p>&#x3D;&#x3D;<strong>TaskManager 故障</strong>&#x3D;&#x3D;<br>如前所述，Flink 需要足够数目的slot，来执行一个应用的所有任务。假设一个Flink环境有4 个TaskManager，每个提供2 个插槽，那么流应用程序执行的最高并行度为8。如果其中一个TaskManager 挂掉了，那么可用的slots 会降到6。<strong>在这种情况下，作业管理器会请求ResourceManager 提供更多的slots</strong>。如果此请求无法满足——例如应用跑在一个独立集群——<strong>那么作业管理器在有足够的slots 之前，无法重启应用</strong>。应用的重启策略决定了作业管理器的重启频率，以及两次重启尝试之间的时间间隔。</p>
<p>&#x3D;&#x3D;<strong>作业管理器故障</strong>&#x3D;&#x3D;<br>比TaskManager 故障更严重的问题是作业管理器故障。作业管理器控制整个流应用程序的执行，并维护执行中的元数据——例如指向已完成检查点的指针。<strong>若是对应的作业管理器挂掉，则流程序无法继续运行。</strong>所以这就导致在Flink 应用中，作业管理器是单点故障。为了解决这个问题，Flink 提供了高可用模式。在原先的作业管理器挂掉后，可以将一个作业的状态和元数据迁移到另一个作业管理器，并继续执行。</p>
<p>Flink 的高可用模式基于Apache ZooKeeper，我们知道，ZooKeeper 是用来管理需要协调和共识的分布式服务的系统。Flink 主要利用ZooKeeper 来进行领导者（leader）的选举，并把它作为一个高可用和持久化的数据存储。当在高可用模式下运行时，<strong>作业管理器会将JobGraph 以及所有需要的元数据（例如应用程序的jar 文件），写入到一个远程的持久化存储系统中。</strong>而且，作业管理器会将指向存储位置的指针，写入到ZooKeeper 的数据存储中。在执行一个应用的过程中，作业管理器会接收每个独立任务检查点的状态句柄（也就是存储位置）。当一个检查点完成时（所有任务已经成功地将它们的状态写入到远程存储），<strong>作业管理器把状态句柄写入远程存储，并将指向这个远程存储的指针写入ZooKeeper。</strong>这样，一个作业管理器挂掉之后再恢复，所需要的所有数据信息已经都保存在了远程存储，而ZooKeeper 里存有指向此存储位置的指针。图3-3 描述了这个设计：</p>
<p>当一个作业管理器失败，所有属于这个应用的任务都会自动取消。一个新的作业管理器接管工作，会执行以下操作：</p>
<ul>
<li>从ZooKeeper 请求存储位置（storage location），从远端存储获取JobGraph，Jar 文件，以及应用最近一次检查点（checkpoint）的状态句柄（state handles）</li>
<li>从ResourceManager 请求slots，用来继续运行应用</li>
<li>重启应用，并将所有任务的状态，重设为最近一次已完成的检查点</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230227214817846.png"
                      alt="image-20230227214817846"
                ></p>
<p>如果我们是在容器环境里运行应用（如Kubernetes），故障的作业管理器或TaskManager容器通常会由容器服务自动重启。当运行在YARN 或Mesos 之上时，<strong>作业管理器或TaskManager 进程会由Flink 的保留进程自动触发重启。</strong>而在standalone 模式下，Flink 并未提供重启故障进程的工具。所以，此模式下我们可以增加备用（standby）的作业管理器和TaskManager，用于接管故障的进程。我们将会在“高可用配置”一节中做进一步讨论。</p>
<h3 id="7-3-Flink-的重启策略"><a href="#7-3-Flink-的重启策略" class="headerlink" title="7.3. Flink 的重启策略"></a>7.3. Flink 的重启策略</h3><p>Flink 支持不同的重启策略，这些重启策略控制着 job 失败后如何重启。集群可以通过默认的重启策略来重启，这个默认的重启策略通常在未指定重启策略的情况下使用，而如果 Job 提交的时候指定了重启策略，这个重启策略就会覆盖掉集群的默认重启策略。</p>
<h4 id="1-概览"><a href="#1-概览" class="headerlink" title="1) 概览"></a>1) 概览</h4><p>默认的重启策略是通过 Flink 的 <strong>flink-conf.yaml</strong> 来指定的，这个配置参数 <strong>restart-strategy</strong> 定义了哪种策略会被采用。<strong>如果 checkpoint 未启动</strong>，就会采用 <strong>no restart</strong> 策略，如果启动了 checkpoint 机制，但是未指定重启策略的话，就会采用 <strong>fixed-delay</strong> 策略，重试 <strong>Integer.MAX_VALUE</strong> 次。请参考下面的可用重启策略来了解哪些值是支持的。</p>
<p>每个重启策略都有自己的参数来控制它的行为，这些值也可以在配置文件中设置，每个重启策略的描述都包含着各自的配置值信息。</p>
<table>
<thead>
<tr>
<th>重启策略</th>
<th>重启策略值</th>
</tr>
</thead>
<tbody><tr>
<td>Fixed delay</td>
<td>fixed-delay</td>
</tr>
<tr>
<td>Failure rate</td>
<td>failure-rate</td>
</tr>
<tr>
<td>No restart</td>
<td>None</td>
</tr>
</tbody></table>
<p>除了定义一个默认的重启策略之外，你还可以为每一个 Job 指定它自己的重启策略，这个重启策略可以在 <strong>ExecutionEnvironment</strong> 中调用 <strong>setRestartStrategy()</strong> 方法来程序化地调用，注意这种方式同样适用于 <strong>StreamExecutionEnvironment</strong>。</p>
<p>下面的例子展示了如何为 Job 设置一个固定延迟重启策略，一旦有失败，系统就会尝试每 10 秒重启一次，重启 3 次。</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment()</span><br><span class="line">env.setRestartStrategy(<span class="type">RestartStrategies</span>.fixedDelayRestart(</span><br><span class="line">  <span class="number">3</span>, <span class="comment">// 重启次数</span></span><br><span class="line">  <span class="type">Time</span>.of(<span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>) <span class="comment">// 延迟时间间隔</span></span><br><span class="line">))</span><br></pre></td></tr></table></figure></div>

<h4 id="2-固定延迟重启策略-Fixed-Delay-Restart-Strategy"><a href="#2-固定延迟重启策略-Fixed-Delay-Restart-Strategy" class="headerlink" title="2) 固定延迟重启策略(Fixed Delay Restart Strategy)"></a>2) 固定延迟重启策略(Fixed Delay Restart Strategy)</h4><p>固定延迟重启策略会尝试一个给定的次数来重启 Job，如果超过了最大的重启次数，Job 最终将失败。在连续的两次重启尝试之间，重启策略会等待一个固定的时间。</p>
<p>重启策略可以配置 flink-conf.yaml 的下面配置参数来启用，作为默认的重启策略:</p>
<div class="highlight-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">restart-strategy:</span> <span class="string">fixed-delay</span></span><br></pre></td></tr></table></figure></div>

<table>
<thead>
<tr>
<th>配置参数</th>
<th>描述</th>
<th>默认值</th>
</tr>
</thead>
<tbody><tr>
<td>restart-strategy.fixed-delay.attempts</td>
<td>在 Job 最终宣告失败之前，Flink 尝试执行的次数</td>
<td>1，如果启用 checkpoint 的话是 Integer.MAX_VALUE</td>
</tr>
<tr>
<td>restart-strategy.fixed-delay.delay</td>
<td>延迟重启意味着一个执行失败之后，并不会立即重启，而是要等待一段时间。</td>
<td>akka.ask.timeout,如果启用 checkpoint 的话是 1s</td>
</tr>
</tbody></table>
<p>例子:</p>
<div class="highlight-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">restart-strategy.fixed-delay.attempts:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.delay:</span> <span class="number">10</span> <span class="string">s</span></span><br></pre></td></tr></table></figure></div>

<p>固定延迟重启也可以在程序中设置:</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment()</span><br><span class="line">env.setRestartStrategy(<span class="type">RestartStrategies</span>.fixedDelayRestart(</span><br><span class="line">  <span class="number">3</span>, <span class="comment">// 重启次数</span></span><br><span class="line">  <span class="type">Time</span>.of(<span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>) <span class="comment">// 重启时间间隔</span></span><br><span class="line">))</span><br></pre></td></tr></table></figure></div>

<h4 id="3-失败率重启策略"><a href="#3-失败率重启策略" class="headerlink" title="3) 失败率重启策略"></a>3) 失败率重启策略</h4><p>失败率重启策略在 Job 失败后会重启，但是超过失败率后，Job 会最终被认定失败。在两个连续的重启尝试之间，重启策略会等待一个固定的时间。</p>
<p>失败率重启策略可以在 flink-conf.yaml 中设置下面的配置参数来启用:</p>
<div class="highlight-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">restart-strategy:failure-rate</span></span><br></pre></td></tr></table></figure></div>

<table>
<thead>
<tr>
<th>配置参数</th>
<th>描述</th>
<th>默认值</th>
</tr>
</thead>
<tbody><tr>
<td>restart-strategy.failure-rate.max-failures-per-interval</td>
<td>在一个 Job 认定为失败之前，最大的重启次数</td>
<td>1</td>
</tr>
<tr>
<td>restart-strategy.failure-rate.failure-rate-interval</td>
<td>计算失败率的时间间隔</td>
<td>1 分钟</td>
</tr>
<tr>
<td>restart-strategy.failure-rate.delay</td>
<td>两次连续重启尝试之间的时间间隔</td>
<td>akka.ask.timeout</td>
</tr>
</tbody></table>
<p>例子:</p>
<div class="highlight-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">restart-strategy.failure-rate.max-failures-per-interval:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.failure-rate-interval:</span> <span class="number">5</span> <span class="string">min</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.delay:</span> <span class="number">10</span> <span class="string">s</span></span><br></pre></td></tr></table></figure></div>

<p>失败率重启策略也可以在程序中设置:</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment()</span><br><span class="line">env.setRestartStrategy(<span class="type">RestartStrategies</span>.failureRateRestart(</span><br><span class="line">  <span class="number">3</span>, <span class="comment">// 每个测量时间间隔最大失败次数</span></span><br><span class="line">  <span class="type">Time</span>.of(<span class="number">5</span>, <span class="type">TimeUnit</span>.<span class="type">MINUTES</span>), <span class="comment">//失败率测量的时间间隔</span></span><br><span class="line">  <span class="type">Time</span>.of(<span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>) <span class="comment">// 两次连续重启尝试的时间间隔</span></span><br><span class="line">))</span><br></pre></td></tr></table></figure></div>

<h4 id="4-无重启策略"><a href="#4-无重启策略" class="headerlink" title="4) 无重启策略"></a>4) 无重启策略</h4><p>Job 直接失败，不会尝试进行重启</p>
<div class="highlight-container" data-rel="Text"><figure class="iseeu highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">restart-strategy: none</span><br></pre></td></tr></table></figure></div>

<p>无重启策略也可以在程序中设置</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment()</span><br><span class="line">env.setRestartStrategy(<span class="type">RestartStrategies</span>.noRestart())</span><br></pre></td></tr></table></figure></div>

<h4 id="5-案例"><a href="#5-案例" class="headerlink" title="5) 案例"></a>5) 案例</h4><p>需求：<strong>输入五次 zhangsan，程序挂掉</strong>。</p>
<p>代码：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.restartstrategy.<span class="type">RestartStrategies</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.filesystem.<span class="type">FsStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FixDelayRestartStrategiesDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//如果想要开启重启策略，就必须开启CheckPoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>L)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定状态存储后端,默认就是内存</span></span><br><span class="line">    <span class="comment">//现在指定的是FsStateBackend，支持本地系统、</span></span><br><span class="line">    <span class="comment">//new FsStateBackend要指定存储系统的协议： scheme (hdfs://, file://, etc)</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">FsStateBackend</span>(args(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//如果程序被cancle，保留以前做的checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定以后存储多个checkpoint目录</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定重启策略,默认的重启策略是不停的重启</span></span><br><span class="line">    <span class="comment">//程序出现异常是会重启，重启五次，每次延迟5秒，如果超过了5次，程序退出</span></span><br><span class="line">    env.setRestartStrategy(<span class="type">RestartStrategies</span>.fixedDelayRestart(<span class="number">5</span>, <span class="number">5000</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(args(<span class="number">1</span>), <span class="number">8888</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>).map(word =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span>(word.equals(<span class="string">&quot;zhangsan&quot;</span>)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">&quot;zhangsan，程序重启！&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      (word, <span class="number">1</span>)</span><br><span class="line">    &#125;)).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>)</span><br><span class="line">    result.print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="7-4-checkpoint-案例"><a href="#7-4-checkpoint-案例" class="headerlink" title="7.4  checkpoint 案例"></a>7.4  checkpoint 案例</h3><p><em>1. 需求</em>：</p>
<p>假定用户需要每隔 1 秒钟需要统计 4 秒中窗口中数据的量，然后对统计的结果值进行 checkpoint 处理</p>
<p><em>2. 数据规划</em>：</p>
<ol>
<li>使用自定义算子每秒钟产生大约 10000 条数据。</li>
<li>产生的数据为一个四元组(Long，String，String，Integer)———(id,name,info,count)。</li>
<li>数据经统计后，统计结果打印到终端输出。</li>
<li>打印输出的结果为 Long 类型的数据 。</li>
</ol>
<p><em>3. 开发思路</em>：</p>
<ol>
<li>source 算子每隔 1 秒钟发送 10000 条数据，并注入到 Window 算子中。</li>
<li>window 算子每隔 1 秒钟统计一次最近 4 秒钟内数据数量。</li>
<li>每隔 1 秒钟将统计结果打印到终端。</li>
<li>每隔 6 秒钟触发一次 checkpoint，然后将 checkpoint 的结果保存到 HDFS 中。</li>
</ol>
<p><em>5. 开发步骤</em>：</p>
<ol>
<li>获取流处理执行环境</li>
<li>设置检查点机制</li>
<li>自定义数据源</li>
<li>数据分组</li>
<li>划分时间窗口</li>
<li>数据聚合</li>
<li>数据打印</li>
<li>触发执行</li>
</ol>
<p><strong>示例代码</strong>：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//发送数据形式</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SEvent</span>(<span class="params">id: <span class="type">Long</span>, name: <span class="type">String</span>, info: <span class="type">String</span>, count: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SEventSourceWithChk</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>[<span class="type">SEvent</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> count = <span class="number">0</span>L</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> alphabet = <span class="string">&quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWZYX0987654321&quot;</span></span><br><span class="line">  <span class="comment">// 任务取消时调用</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    isRunning = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//// source算子的逻辑，即:每秒钟向流图中注入10000个元组</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(sourceContext: <span class="type">SourceContext</span>[<span class="type">SEvent</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">while</span>(isRunning) &#123;</span><br><span class="line">      <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until <span class="number">10000</span>) &#123;</span><br><span class="line">        sourceContext.collect(<span class="type">SEvent</span>(<span class="number">1</span>, <span class="string">&quot;hello-&quot;</span>+count, alphabet,<span class="number">1</span>))</span><br><span class="line">        count += <span class="number">1</span>L</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">该段代码是流图定义代码，具体实现业务流程，另外，代码中窗口的触发时间使 用了event time。</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FlinkEventTimeAPIChkMain</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">FsStateBackend</span>(<span class="string">&quot;hdfs://hadoop01:9000/flink-checkpoint/checkpoint/&quot;</span>))</span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    env.getCheckpointConfig.setCheckpointInterval(<span class="number">6000</span>)</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"><span class="comment">//保留策略:默认情况下，检查点不会被保留，仅用于故障中恢复作业，可以启用外部持久化检查点，同时指定保留策略</span></span><br><span class="line"><span class="comment">//ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION:在作业取消时保留检查点，注意在这种情况下，您必须在取消后手动清理检查点状态</span></span><br><span class="line"><span class="comment">//ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION:当作业被cancel时，删除检查点，检查点状态仅在作业失败时可用</span></span><br><span class="line">env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">DELETE_ON_CANCELLATION</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 应用逻辑</span></span><br><span class="line">    <span class="keyword">val</span> source: <span class="type">DataStream</span>[<span class="type">SEvent</span>] = env.addSource(<span class="keyword">new</span> <span class="type">SEventSourceWithChk</span>)</span><br><span class="line">    source.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">AssignerWithPeriodicWatermarks</span>[<span class="type">SEvent</span>] &#123;</span><br><span class="line">      <span class="comment">// 设置watermark</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCurrentWatermark</span></span>: <span class="type">Watermark</span> = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Watermark</span>(<span class="type">System</span>.currentTimeMillis())</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 给每个元组打上时间戳</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(t: <span class="type">SEvent</span>, l: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">        <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">      .keyBy(<span class="number">0</span>)</span><br><span class="line">      .window(<span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">4</span>), <span class="type">Time</span>.seconds(<span class="number">1</span>)))</span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowStatisticWithChk</span>)</span><br><span class="line">      .print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//该数据在算子制作快照时用于保存到目前为止算子记录的数据条数。</span></span><br><span class="line"><span class="comment">// 用户自定义状态</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UDFState</span> <span class="keyword">extends</span> <span class="title">Serializable</span></span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> count = <span class="number">0</span>L</span><br><span class="line">  <span class="comment">// 设置用户自定义状态</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setState</span></span>(s: <span class="type">Long</span>) = count = s</span><br><span class="line">  <span class="comment">// 获取用户自定状态</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getState</span> </span>= count</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//该段代码是window算子的代码，每当触发计算时统计窗口中元组数量。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WindowStatisticWithChk</span> <span class="keyword">extends</span> <span class="title">WindowFunction</span>[<span class="type">SEvent</span>, <span class="type">Long</span>, <span class="type">Tuple</span>, <span class="type">TimeWindow</span>] <span class="keyword">with</span> <span class="title">ListCheckpointed</span>[<span class="type">UDFState</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> total = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">  <span class="comment">// window算子的实现逻辑，即:统计window中元组的数量</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[<span class="type">SEvent</span>], out: <span class="type">Collector</span>[<span class="type">Long</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> count = <span class="number">0</span>L</span><br><span class="line">    <span class="keyword">for</span> (event &lt;- input) &#123;</span><br><span class="line">      count += <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">    total += count</span><br><span class="line">    out.collect(count)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 从自定义快照中恢复状态</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">restoreState</span></span>(state: util.<span class="type">List</span>[<span class="type">UDFState</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> udfState = state.get(<span class="number">0</span>)</span><br><span class="line">    total = udfState.getState</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 制作自定义状态快照</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">snapshotState</span></span>(checkpointId: <span class="type">Long</span>, timestamp: <span class="type">Long</span>): util.<span class="type">List</span>[<span class="type">UDFState</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> udfList: util.<span class="type">ArrayList</span>[<span class="type">UDFState</span>] = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">UDFState</span>]</span><br><span class="line">    <span class="keyword">val</span> udfState = <span class="keyword">new</span> <span class="type">UDFState</span></span><br><span class="line">    udfState.setState(total)</span><br><span class="line">    udfList.add(udfState)</span><br><span class="line">    udfList</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="7-5-端对端仅处理一次语义"><a href="#7-5-端对端仅处理一次语义" class="headerlink" title="7.5   端对端仅处理一次语义"></a>7.5   端对端仅处理一次语义</h3><p>当谈及仅一次处理时，我们真正想表达的是每条输入消息只会影响最终结果一次！（<strong>影响应用状态一次，而非被处理一次</strong>）即使出现机器故障或软件崩溃，Flink 也要保证不会有数据被重复处理或压根就没有被处理从而影响状态。</p>
<p>在 Flink 1.4 版本之前，精准一次处理只限于 Flink 应用内，也就是所有的 Operator 完全由 Flink 状态保存并管理的才能实现精确一次处理。但 Flink 处理完数据后大多需要将结果发送到外部系统，比如 Sink 到 Kafka 中，这个过程中 Flink 并不保证精准一次处理。</p>
<p>在 Flink 1.4 版本正式引入了一个里程碑式的功能：两阶段提交 Sink，即 TwoPhaseCommitSinkFunction 函数。该 SinkFunction 提取并封装<strong>了两阶段提交协议</strong>中的公共逻辑，自此 Flink 搭配特定 Source 和 Sink（如 Kafka 0.11 版）<strong>实现精确一次处理语义</strong>(英文简称：EOS，即 Exactly-Once Semantics)。</p>
<p>在 Flink 中需要端到端精准一次处理的位置有三个：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210130_8.png"
                      alt="Flink 端到端精准一次处理"
                ></p>
<ul>
<li><p><strong>Source 端</strong>：数据从上一阶段进入到 Flink 时，需要保证消息精准一次消费。</p>
</li>
<li><p><strong>Flink 内部端</strong>：这个我们已经了解，利用 Checkpoint 机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性。不了解的小伙伴可以看下我之前的文章：</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&mid=2247483947&idx=1&sn=adae434f4e32b31be51627888e7d9f76&chksm=ce77f4faf9007decd2f78a788a89e6777bb7bec79f4e59093474532ca5cf774284e2fe35e1bd&token=1679639512&lang=zh_CN#rd" >Flink 可靠性的基石-checkpoint 机制详细解析(opens new window) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
</li>
<li><p><strong>Sink 端</strong>：将处理完的数据发送到下一阶段时，需要保证数据能够准确无误发送到下一阶段。</p>
</li>
</ul>
<h4 id="1-Flink-端到端精准一次处理语义（EOS）"><a href="#1-Flink-端到端精准一次处理语义（EOS）" class="headerlink" title="1) Flink 端到端精准一次处理语义（EOS）"></a>1) Flink 端到端精准一次处理语义（EOS）</h4><p><strong>以下内容适用于 Flink 1.4 及之后版本</strong></p>
<p><strong>对于 Source 端</strong>：Source 端的精准一次处理比较简单，毕竟数据是落到 Flink 中，所以 Flink 只需要保存消费数据的偏移量即可， 如消费 Kafka 中的数据，Flink 将 Kafka Consumer 作为 Source，可以将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性。</p>
<p><strong>对于 Sink 端</strong>：<strong>Sink 端是最复杂的</strong>，因为数据是落地到其他系统上的，数据一旦离开 Flink 之后，Flink 就监控不到这些数据了，所以精准一次处理语义必须也要应用于 Flink 写入数据的外部系统，故这些外部系统必须提供一种手段允许提交或回滚这些写入操作，同时还要保证与 Flink Checkpoint 能够协调使用（Kafka 0.11 版本已经实现精确一次处理语义）。</p>
<p>我们以 Flink 与 Kafka 组合为例，Flink 从 Kafka 中读数据，处理完的数据在写入 Kafka 中。</p>
<p>为什么以 Kafka 为例，第一个原因是目前大多数的 Flink 系统读写数据都是与 Kafka 系统进行的。第二个原因，也是<strong>最重要的原因 Kafka 0.11 版本正式发布了对于事务的支持，这是与 Kafka 交互的 Flink 应用要实现端到端精准一次语义的必要条件</strong>。</p>
<p>当然，Flink 支持这种精准一次处理语义并不只是限于与 Kafka 的结合，可以使用任何 Source&#x2F;Sink，只要它们提供了必要的协调机制。</p>
<h4 id="2-Flink-与-Kafka-组合"><a href="#2-Flink-与-Kafka-组合" class="headerlink" title="2) Flink 与 Kafka 组合"></a>2) Flink 与 Kafka 组合</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210130_1.png"
                      alt="Flink 应用示例"
                ></p>
<p>如上图所示，Flink 中包含以下组件：</p>
<ol>
<li>一个 Source，从 Kafka 中读取数据（即 KafkaConsumer）</li>
<li>一个时间窗口化的聚会操作（Window）</li>
<li>一个 Sink，将结果写入到 Kafka（即 KafkaProducer）</li>
</ol>
<p><strong>若要 Sink 支持精准一次处理语义(EOS)，它必须以事务的方式写数据到 Kafka</strong>，这样当提交事务时两次 Checkpoint 间的所有写入操作当作为一个事务被提交。这确保了出现故障或崩溃时这些写入操作能够被回滚。</p>
<p>当然了，<strong>在一个分布式且含有多个并发执行 Sink 的应用中，仅仅执行单次提交或回滚是不够的，因为所有组件都必须对这些提交或回滚达成共识，这样才能保证得到一个一致性的结果。Flink 使用两阶段提交协议以及预提交(Pre-commit)阶段来解决这个问题</strong>。</p>
<h4 id="3-两阶段提交协议（2PC）"><a href="#3-两阶段提交协议（2PC）" class="headerlink" title="3) 两阶段提交协议（2PC）"></a>3) 两阶段提交协议（2PC）</h4><p><strong>两阶段提交协议（Two-Phase Commit，2PC）是很常用的解决分布式事务问题的方式，它可以保证在分布式事务中，要么所有参与进程都提交事务，要么都取消，即实现 ACID 中的 A （原子性）</strong>。</p>
<p>在数据一致性的环境下，其代表的含义是：要么所有备份数据同时更改某个数值，要么都不改，以此来达到数据的<strong>强一致性</strong>。</p>
<p><strong>两阶段提交协议中有两个重要角色，协调者（Coordinator）和参与者（Participant），其中协调者只有一个，起到分布式事务的协调管理作用，参与者有多个</strong>。</p>
<p>顾名思义，两阶段提交将提交过程划分为连续的两个阶段：<strong>表决阶段（Voting）和提交阶段（Commit）</strong>。</p>
<p>两阶段提交协议过程如下图所示：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210130_2.png"
                      alt="两阶段提交协议"
                ></p>
<p><strong>第一阶段：表决阶段</strong></p>
<ol>
<li>协调者向所有参与者发送一个 VOTE_REQUEST 消息。</li>
<li>当参与者接收到 VOTE_REQUEST 消息，向协调者发送 VOTE_COMMIT 消息作为回应，告诉协调者自己已经做好准备提交准备，如果参与者没有准备好或遇到其他故障，就返回一个 VOTE_ABORT 消息，告诉协调者目前无法提交事务。</li>
</ol>
<p><strong>第二阶段：提交阶段</strong></p>
<ol>
<li>协调者收集来自各个参与者的表决消息。如果<strong>所有参与者一致认为可以提交事务，那么协调者决定事务的最终提交</strong>，在此情形下协调者向所有参与者发送一个 GLOBAL_COMMIT 消息，通知参与者进行本地提交；如果所有参与者中有<strong>任意一个返回消息是 VOTE_ABORT，协调者就会取消事务</strong>，向所有参与者广播一条 GLOBAL_ABORT 消息通知所有的参与者取消事务。</li>
<li>每个提交了表决信息的参与者等候协调者返回消息，如果参与者接收到一个 GLOBAL_COMMIT 消息，那么参与者提交本地事务，否则如果接收到 GLOBAL_ABORT 消息，则参与者取消本地事务。</li>
</ol>
<h4 id="4-两阶段提交协议在-Flink-中的应用"><a href="#4-两阶段提交协议在-Flink-中的应用" class="headerlink" title="4) 两阶段提交协议在 Flink 中的应用"></a>4) 两阶段提交协议在 Flink 中的应用</h4><p><strong>Flink 的两阶段提交思路</strong>：</p>
<p>我们从 Flink 程序启动到消费 Kafka 数据，最后到 Flink 将数据 Sink 到 Kafka 为止，来分析 Flink 的精准一次处理。</p>
<ol>
<li>当 Checkpoint 启动时，JobManager 会将检查点分界线（checkpoint battier）注入数据流，checkpoint barrier 会在算子间传递下去，如下如所示：</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210130_3.png"
                      alt="Flink 精准一次处理：Checkpoint 启动"
                ></p>
<ol>
<li><p><strong>Source 端</strong>：<strong>Flink Kafka Source 负责保存 Kafka 消费 offset</strong>，当 Chckpoint 成功时 Flink 负责提交这些写入，否则就终止取消掉它们，当 Chckpoint 完成位移保存，它会将 checkpoint barrier（检查点分界线） 传给下一个 Operator，然后每个算子会对当前的状态做个快照，<strong>保存到状态后端</strong>（State Backend）。</p>
<p><strong>对于 Source 任务而言，就会把当前的 offset 作为状态保存起来。下次从 Checkpoint 恢复时，Source 任务可以重新提交偏移量，从上次保存的位置开始重新消费数据</strong>，如下图所示：</p>
</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210130_4_1.png"
                      alt="Flink 精准一次处理：checkpoint barrier 及 offset 保存"
                ></p>
<ol>
<li><strong>Slink 端</strong>：从 Source 端开始，每个内部的 transform 任务遇到 checkpoint barrier（检查点分界线）时，都会把状态存到 Checkpoint 里。数据处理完毕到 Sink 端时，Sink 任务首先把数据写入外部 Kafka，这些数据都属于预提交的事务（还不能被消费），<strong>此时的 Pre-commit 预提交阶段下 Data Sink 在保存状态到状态后端的同时还必须预提交它的外部事务</strong>，如下图所示：</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210130_5_1.png"
                      alt="Flink 精准一次处理：预提交到外部系统"
                ></p>
<ol>
<li><p><strong>当所有算子任务的快照完成</strong>（所有创建的快照都被视为是 Checkpoint 的一部分），<strong>也就是这次的 Checkpoint 完成时，JobManager 会向所有任务发通知，确认这次 Checkpoint 完成，此时 Pre-commit 预提交阶段才算完成</strong>。才正式到<strong>两阶段提交协议的第二个阶段：commit 阶段</strong>。该阶段中 JobManager 会为应用中每个 Operator 发起 Checkpoint 已完成的回调逻辑。</p>
<p>本例中的 Data Source 和窗口操作无外部状态，因此在该阶段，这两个 Opeartor 无需执行任何逻辑，但是 <strong>Data Sink 是有外部状态的，此时我们必须提交外部事务</strong>，当 Sink 任务收到确认通知，就会正式提交之前的事务，Kafka 中未确认的数据就改为“已确认”，数据就真正可以被消费了，如下图所示：</p>
</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210130_6_1.png"
                      alt="Flink 精准一次处理：数据精准被消费"
                ></p>
<blockquote>
<p>注：Flink 由 JobManager 协调各个 TaskManager 进行 Checkpoint 存储，Checkpoint 保存在 StateBackend（状态后端） 中，默认 StateBackend 是内存级的，也可以改为文件级的进行持久化保存。</p>
</blockquote>
<p>最后，一张图总结下 Flink 的 EOS：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210130_7.png"
                      alt="Flink 端到端精准一次处理"
                ></p>
<p><strong>此图建议保存，总结全面且简明扼要，再也不怂面试官！</strong></p>
<h4 id="5-Exactly-Once-案例"><a href="#5-Exactly-Once-案例" class="headerlink" title="5) Exactly-Once 案例"></a>5) Exactly-Once 案例</h4><p><strong>Kafka 来实现 End-to-End Exactly-Once 语义</strong>：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaProducer011</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.util.serialization.<span class="type">KeyedSerializationSchemaWrapper</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Kafka Producer的容错-Kafka 0.9 and 0.10</span></span><br><span class="line"><span class="comment"> * 如果Flink开启了checkpoint，针对FlinkKafkaProducer09 和FlinkKafkaProducer010 可以提供 at-least-once的语义，还需要配置下面两个参数</span></span><br><span class="line"><span class="comment"> * •setLogFailuresOnly(false)</span></span><br><span class="line"><span class="comment"> * •setFlushOnCheckpoint(true)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意：建议修改kafka 生产者的重试次数 </span></span><br><span class="line"><span class="comment"> * retries【这个参数的值默认是0】</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Kafka Producer的容错-Kafka 0.11</span></span><br><span class="line"><span class="comment"> * 如果Flink开启了checkpoint，针对FlinkKafkaProducer011 就可以提供 exactly-once的语义</span></span><br><span class="line"><span class="comment"> * 但是需要选择具体的语义</span></span><br><span class="line"><span class="comment"> * •Semantic.NONE</span></span><br><span class="line"><span class="comment"> * •Semantic.AT_LEAST_ONCE【默认】</span></span><br><span class="line"><span class="comment"> * •Semantic.EXACTLY_ONCE</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingKafkaSinkScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//checkpoint配置</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">&quot;node01&quot;</span>, <span class="number">9001</span>, &#x27;\n&#x27;)</span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">&quot;test&quot;</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node01:9092&quot;</span>)</span><br><span class="line">    <span class="comment">//设置事务超时时间，也可在kafka配置中设置</span></span><br><span class="line">    prop.setProperty(<span class="string">&quot;transaction.timeout.ms&quot;</span>,<span class="number">60000</span>*<span class="number">15</span>+<span class="string">&quot;&quot;</span>);</span><br><span class="line">    <span class="comment">//使用至少一次语义的形式</span></span><br><span class="line">    <span class="comment">//val myProducer = new FlinkKafkaProducer011&lt;&gt;(brokerList, topic, new SimpleStringSchema());</span></span><br><span class="line">    <span class="comment">//使用支持仅一次语义的形式</span></span><br><span class="line">    <span class="keyword">val</span> myProducer =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FlinkKafkaProducer011</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">KeyedSerializationSchemaWrapper</span>[<span class="type">String</span>](<span class="keyword">new</span> <span class="type">SimpleStringSchema</span>), prop, <span class="type">FlinkKafkaProducer011</span>.<span class="type">Semantic</span>.<span class="type">EXACTLY_ONCE</span>);</span><br><span class="line">    text.addSink(myProducer)</span><br><span class="line">    env.execute(<span class="string">&quot;StreamingKafkaSinkScala&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Java"><figure class="iseeu highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>, <span class="string">&quot;\n&quot;</span>);</span><br><span class="line"><span class="type">String</span> <span class="variable">brokerList</span> <span class="operator">=</span> <span class="string">&quot;localhost:9092&quot;</span>;</span><br><span class="line"><span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> <span class="string">&quot;topic&quot;</span>;</span><br><span class="line"><span class="type">Properties</span> <span class="variable">prop</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">prop.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, brokerList);</span><br><span class="line"><span class="comment">//第一种解决方案，设置FlinkKafkaProducer 里面的事务超时时间</span></span><br><span class="line"><span class="comment">//设置事务超时时间</span></span><br><span class="line"><span class="comment">//prop.setProperty(&quot;transaction.timeout.ms&quot;,60000*15+&quot;&quot;);</span></span><br><span class="line"><span class="comment">//第二种解决方案，设置kafka 的最大事务超时时间</span></span><br><span class="line"><span class="comment">//FlinkKafkaProducer&lt;String&gt; myProducer = new FlinkKafkaProducer&lt;&gt;(brokerList, topic, new SimpleStringSchema());</span></span><br><span class="line"><span class="comment">//使用仅一次语义的kafkaProducer</span></span><br><span class="line">FlinkKafkaProducer&lt;String&gt; myProducer = <span class="keyword">new</span> <span class="title class_">FlinkKafkaProducer</span>&lt;&gt;(</span><br><span class="line">topic,</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">KeyedSerializationSchemaWrapper</span>&lt;String&gt;(<span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>()),</span><br><span class="line">prop,</span><br><span class="line"><span class="comment">//添加精准一次语义</span></span><br><span class="line">FlinkKafkaProducer.Semantic.EXACTLY_ONCE</span><br><span class="line">);</span><br><span class="line">text.addSink(myProducer);</span><br></pre></td></tr></table></figure></div>

<p><strong>Redis 实现 End-to-End Exactly-Once 语义</strong>:</p>
<p>依赖:</p>
<div class="highlight-container" data-rel="Xml"><figure class="iseeu highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

<p>代码开发步骤：</p>
<ol>
<li>获取流处理执行环境</li>
<li>设置检查点机制</li>
<li>定义 kafkaConsumer</li>
<li>数据转换：分组，求和</li>
<li>数据写入 redis</li>
<li>触发执行</li>
</ol>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExactlyRedisSink</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">FsStateBackend</span>(<span class="string">&quot;hdfs://node01:8020/check/11&quot;</span>))</span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    env.getCheckpointConfig.setFailOnCheckpointingErrors(<span class="literal">false</span>)</span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">DELETE_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置kafka，加载kafka数据源</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node01:9092,node02:9092,node03:9092&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>](<span class="string">&quot;test2&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties)</span><br><span class="line">    kafkaConsumer.setStartFromLatest()</span><br><span class="line">    <span class="comment">//检查点制作成功，才开始提交偏移量</span></span><br><span class="line">    kafkaConsumer.setCommitOffsetsOnCheckpoints(<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaSource: <span class="type">DataStream</span>[<span class="type">String</span>] = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据转换</span></span><br><span class="line">    <span class="keyword">val</span> sumData: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = kafkaSource.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">      .map(_ -&gt; <span class="number">1</span>)</span><br><span class="line">      .keyBy(<span class="number">0</span>)</span><br><span class="line">      .sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> set = <span class="keyword">new</span> util.<span class="type">HashSet</span>[<span class="type">InetSocketAddress</span>]()</span><br><span class="line">    set.add(<span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="type">InetAddress</span>.getByName(<span class="string">&quot;node01&quot;</span>),<span class="number">7001</span>))</span><br><span class="line">    set.add(<span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="type">InetAddress</span>.getByName(<span class="string">&quot;node01&quot;</span>),<span class="number">7002</span>))</span><br><span class="line">    set.add(<span class="keyword">new</span> <span class="type">InetSocketAddress</span>(<span class="type">InetAddress</span>.getByName(<span class="string">&quot;node01&quot;</span>),<span class="number">7003</span>))</span><br><span class="line">    <span class="keyword">val</span> config: <span class="type">FlinkJedisClusterConfig</span> = <span class="keyword">new</span> <span class="type">FlinkJedisClusterConfig</span>.<span class="type">Builder</span>()</span><br><span class="line">      .setNodes(set)</span><br><span class="line">      .setMaxIdle(<span class="number">5</span>)</span><br><span class="line">      .setMaxTotal(<span class="number">10</span>)</span><br><span class="line">      .setMinIdle(<span class="number">5</span>)</span><br><span class="line">      .setTimeout(<span class="number">10</span>)</span><br><span class="line">      .build()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据写入</span></span><br><span class="line">    sumData.addSink(<span class="keyword">new</span> <span class="type">RedisSink</span>(config,<span class="keyword">new</span> <span class="type">MyRedisSink</span>))</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRedisSink</span> <span class="keyword">extends</span> <span class="title">RedisMapper</span>[(<span class="type">String</span>,<span class="type">Int</span>)] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCommandDescription</span></span>: <span class="type">RedisCommandDescription</span> = &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">RedisCommandDescription</span>(<span class="type">RedisCommand</span>.<span class="type">HSET</span>,<span class="string">&quot;resink&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKeyFromData</span></span>(data: (<span class="type">String</span>, <span class="type">Int</span>)): <span class="type">String</span> = &#123;</span><br><span class="line">    data._1</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValueFromData</span></span>(data: (<span class="type">String</span>, <span class="type">Int</span>)): <span class="type">String</span> = &#123;</span><br><span class="line">    data._2.toString</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="8-Flink-SQL"><a href="#8-Flink-SQL" class="headerlink" title="8  Flink  SQL"></a>8  Flink  SQL</h2><p>Flink SQL 是 Flink 实时计算为简化计算模型，降低用户使用实时计算门槛而设计的一套符合标准 SQL 语义的开发语言。 自 2015 年开始，阿里巴巴开始调研开源流计算引擎，最终决定基于 Flink 打造新一代计算引擎，针对 Flink 存在的不足进行优化和改进，并且在 2019 年初将最终代码开源，也就是我们熟知的 Blink。Blink 在原来的 Flink 基础上最显著的一个贡献就是 Flink SQL 的实现。</p>
<p>Flink SQL 是面向用户的 API 层，在我们传统的流式计算领域，比如 Storm、Spark Streaming 都会提供一些 Function 或者 Datastream API，用户通过 Java 或 Scala 写业务逻辑，这种方式虽然灵活，但有一些不足，比如具备一定门槛且调优较难，随着版本的不断更新，API 也出现了很多不兼容的地方。</p>
<p>在这个背景下，毫无疑问，SQL 就成了我们最佳选择，之所以选择将 SQL 作为核心 API，是因为其具有几个非常重要的特点：</p>
<ul>
<li>SQL 属于设定式语言，用户只要表达清楚需求即可，不需要了解具体做法；</li>
<li>SQL 可优化，内置多种查询优化器，这些查询优化器可为 SQL 翻译出最优执行计划；</li>
<li>SQL 易于理解，不同行业和领域的人都懂，学习成本较低；</li>
<li>SQL 非常稳定，在数据库 30 多年的历史中，SQL 本身变化较少；</li>
<li>流与批的统一，Flink 底层 Runtime 本身就是一个流与批统一的引擎，而 SQL 可以做到 API 层的流与批统一。</li>
</ul>
<h3 id="8-1-Flink-SQL-常用算子"><a href="#8-1-Flink-SQL-常用算子" class="headerlink" title="8.1   Flink SQL 常用算子"></a>8.1   Flink SQL 常用算子</h3><h4 id="SELECT："><a href="#SELECT：" class="headerlink" title="SELECT："></a><strong>SELECT</strong>：</h4><p>SELECT 用于从 DataSet&#x2F;DataStream 中选择数据，用于筛选出某些列。</p>
<p>示例：</p>
<p><code>SELECT * FROM Table;</code> &#x2F;&#x2F; 取出表中的所有列</p>
<p><code>SELECT name，age FROM Table;</code> &#x2F;&#x2F; 取出表中 name 和 age 两列</p>
<p>与此同时 SELECT 语句中可以使用函数和别名，例如我们上面提到的 WordCount 中：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> word, <span class="built_in">COUNT</span>(word) <span class="keyword">FROM</span> <span class="keyword">table</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> word;</span><br></pre></td></tr></table></figure></div>

<h4 id="WHERE："><a href="#WHERE：" class="headerlink" title="WHERE："></a><strong>WHERE</strong>：</h4><p>WHERE 用于从数据集&#x2F;流中过滤数据，与 SELECT 一起使用，用于根据某些条件对关系做水平分割，即选择符合条件的记录。</p>
<p>示例：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name，age <span class="keyword">FROM</span> <span class="keyword">Table</span> <span class="keyword">where</span> name <span class="keyword">LIKE</span> ‘<span class="operator">%</span> 小明 <span class="operator">%</span>’;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> <span class="keyword">Table</span> <span class="keyword">WHERE</span> age <span class="operator">=</span> <span class="number">20</span>;</span><br></pre></td></tr></table></figure></div>

<p>WHERE 是从原数据中进行过滤，那么在 WHERE 条件中，Flink SQL 同样支持 <code>=、&lt;、&gt;、&lt;&gt;、&gt;=、&lt;=</code>，以及 <code>AND、OR</code> 等表达式的组合，最终满足过滤条件的数据会被选择出来。并且 WHERE 可以结合 IN、NOT IN 联合使用。举个例子：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name, age</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">Table</span></span><br><span class="line"><span class="keyword">WHERE</span> name <span class="keyword">IN</span> (<span class="keyword">SELECT</span> name <span class="keyword">FROM</span> Table2)</span><br></pre></td></tr></table></figure></div>

<h4 id="DISTINCT："><a href="#DISTINCT：" class="headerlink" title="DISTINCT："></a><strong>DISTINCT</strong>：</h4><p>DISTINCT 用于从数据集&#x2F;流中去重根据 SELECT 的结果进行去重。</p>
<p>示例：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> name <span class="keyword">FROM</span> <span class="keyword">Table</span>;</span><br></pre></td></tr></table></figure></div>

<p>对于流式查询，计算查询结果所需的 State 可能会无限增长，用户需要自己控制查询的状态范围，以防止状态过大。</p>
<h4 id="GROUP-BY："><a href="#GROUP-BY：" class="headerlink" title="GROUP BY："></a><strong>GROUP BY</strong>：</h4><p>GROUP BY 是对数据进行分组操作。例如我们需要计算成绩明细表中，每个学生的总分。</p>
<p>示例：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name, <span class="built_in">SUM</span>(score) <span class="keyword">as</span> TotalScore <span class="keyword">FROM</span> <span class="keyword">Table</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> name;</span><br></pre></td></tr></table></figure></div>

<h4 id="UNION-和-UNION-ALL："><a href="#UNION-和-UNION-ALL：" class="headerlink" title="UNION 和 UNION ALL："></a><strong>UNION 和 UNION ALL</strong>：</h4><p>UNION 用于将两个结果集合并起来，要求两个结果集字段完全一致，包括字段类型、字段顺序。不同于 UNION ALL 的是，UNION 会对结果数据去重。</p>
<p>示例：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> T1 <span class="keyword">UNION</span> (<span class="keyword">ALL</span>) <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> T2;</span><br></pre></td></tr></table></figure></div>

<h4 id="JOIN："><a href="#JOIN：" class="headerlink" title="JOIN："></a><strong>JOIN</strong>：</h4><p>JOIN 用于把来自两个表的数据联合起来形成结果表，Flink 支持的 JOIN 类型包括：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">JOIN</span> <span class="operator">-</span> <span class="keyword">INNER</span> <span class="keyword">JOIN</span></span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> <span class="operator">-</span> <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span></span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> <span class="operator">-</span> <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span></span><br><span class="line"><span class="keyword">FULL</span> <span class="keyword">JOIN</span> <span class="operator">-</span> <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span></span><br></pre></td></tr></table></figure></div>

<p>这里的 JOIN 的语义和我们在关系型数据库中使用的 JOIN 语义一致。</p>
<p>示例：</p>
<p>JOIN（将订单表数据和商品表进行关联）</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> Orders <span class="keyword">INNER</span> <span class="keyword">JOIN</span> Product <span class="keyword">ON</span> Orders.productId <span class="operator">=</span> Product.id</span><br></pre></td></tr></table></figure></div>

<p>LEFT JOIN 与 JOIN 的区别是当右表没有与左边相 JOIN 的数据时候，右边对应的字段补 NULL 输出，RIGHT JOIN 相当于 LEFT JOIN 左右两个表交互一下位置。FULL JOIN 相当于 RIGHT JOIN 和 LEFT JOIN 之后进行 UNION ALL 操作。</p>
<p>示例：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> Orders <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> Product <span class="keyword">ON</span> Orders.productId <span class="operator">=</span> Product.id</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> Orders <span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> Product <span class="keyword">ON</span> Orders.productId <span class="operator">=</span> Product.id</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> Orders <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> Product <span class="keyword">ON</span> Orders.productId <span class="operator">=</span> Product.id</span><br></pre></td></tr></table></figure></div>

<h4 id="Group-Window："><a href="#Group-Window：" class="headerlink" title="Group Window："></a><strong>Group Window</strong>：</h4><p>根据窗口数据划分的不同，目前 Apache Flink 有如下 3 种 Bounded Window：</p>
<p><em>Tumble，滚动窗口</em>，窗口数据有固定的大小，窗口数据无叠加；</p>
<p><em>Hop，滑动窗口</em>，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</p>
<p><em>Session，会话窗口</em>，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。</p>
<h4 id="Tumble-Window："><a href="#Tumble-Window：" class="headerlink" title="Tumble Window："></a><strong>Tumble Window</strong>：</h4><p>Tumble 滚动窗口有固定大小，窗口数据不重叠，具体语义如下：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_10.png"
                      alt="img"
                ></p>
<p>Tumble 滚动窗口对应的语法如下：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    [gk],</span><br><span class="line">    [TUMBLE_START(timeCol, size)],</span><br><span class="line">    [TUMBLE_END(timeCol, size)],</span><br><span class="line">    agg1(col1),</span><br><span class="line">    ...</span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> [gk], TUMBLE(timeCol, size)</span><br></pre></td></tr></table></figure></div>

<p>其中：</p>
<p>[gk] 决定了是否需要按照字段进行聚合；</p>
<p>TUMBLE_START 代表窗口开始时间；</p>
<p>TUMBLE_END 代表窗口结束时间；</p>
<p>timeCol 是流表中表示时间字段；</p>
<p>size 表示窗口的大小，如 秒、分钟、小时、天。</p>
<p>举个例子，假如我们要计算每个人每天的订单量，按照 user 进行聚合分组：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">user</span>,</span><br><span class="line">      TUMBLE_START(rowtime, <span class="type">INTERVAL</span> ‘<span class="number">1</span>’ <span class="keyword">DAY</span>) <span class="keyword">as</span> wStart,</span><br><span class="line">      <span class="built_in">SUM</span>(amount)</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(rowtime, <span class="type">INTERVAL</span> ‘<span class="number">1</span>’ <span class="keyword">DAY</span>), <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure></div>

<h4 id="Hop-Window："><a href="#Hop-Window：" class="headerlink" title="Hop Window："></a><strong>Hop Window</strong>：</h4><p>Hop 滑动窗口和滚动窗口类似，窗口有固定的 size，与滚动窗口不同的是滑动窗口可以通过 slide 参数控制滑动窗口的新建频率。因此当 slide 值小于窗口 size 的值的时候多个滑动窗口会重叠，具体语义如下：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_11.png"
                      alt="img"
                ></p>
<p>Hop 滑动窗口对应语法如下：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    [gk],</span><br><span class="line">    [HOP_START(timeCol, slide, size)] ,</span><br><span class="line">    [HOP_END(timeCol, slide, size)],</span><br><span class="line">    agg1(col1),</span><br><span class="line">    ...</span><br><span class="line">    aggN(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> [gk], HOP(timeCol, slide, size)</span><br></pre></td></tr></table></figure></div>

<p>每次字段的意思和 Tumble 窗口类似：</p>
<p>[gk] 决定了是否需要按照字段进行聚合；</p>
<p>HOP_START 表示窗口开始时间；</p>
<p>HOP_END 表示窗口结束时间；</p>
<p>timeCol 表示流表中表示时间字段；</p>
<p>slide 表示每次窗口滑动的大小；</p>
<p>size 表示整个窗口的大小，如 秒、分钟、小时、天。</p>
<p>举例说明，我们要每过一小时计算一次过去 24 小时内每个商品的销量：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> product,</span><br><span class="line">      <span class="built_in">SUM</span>(amount)</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> HOP(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">HOUR</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>), product</span><br></pre></td></tr></table></figure></div>

<h4 id="Session-Window："><a href="#Session-Window：" class="headerlink" title="Session Window："></a><strong>Session Window</strong>：</h4><p>会话时间窗口没有固定的持续时间，但它们的界限由 interval 不活动时间定义，即如果在定义的间隙期间没有出现事件，则会话窗口关闭。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_12.png"
                      alt="img"
                ></p>
<p>Seeeion 会话窗口对应语法如下：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    [gk],</span><br><span class="line">    SESSION_START(timeCol, gap) <span class="keyword">AS</span> winStart,</span><br><span class="line">    SESSION_END(timeCol, gap) <span class="keyword">AS</span> winEnd,</span><br><span class="line">    agg1(col1),</span><br><span class="line">     ...</span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> [gk], SESSION(timeCol, gap)</span><br></pre></td></tr></table></figure></div>

<p>[gk] 决定了是否需要按照字段进行聚合；</p>
<p>SESSION_START 表示窗口开始时间；</p>
<p>SESSION_END 表示窗口结束时间；</p>
<p>timeCol 表示流表中表示时间字段；</p>
<p>gap 表示窗口数据非活跃周期的时长。</p>
<p>例如，我们需要计算每个用户访问时间 12 小时内的订单量：</p>
<div class="highlight-container" data-rel="Sql"><figure class="iseeu highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">user</span>,</span><br><span class="line">      SESSION_START(rowtime, <span class="type">INTERVAL</span> ‘<span class="number">12</span>’ <span class="keyword">HOUR</span>) <span class="keyword">AS</span> sStart,</span><br><span class="line">      SESSION_ROWTIME(rowtime, <span class="type">INTERVAL</span> ‘<span class="number">12</span>’ <span class="keyword">HOUR</span>) <span class="keyword">AS</span> sEnd,</span><br><span class="line">      <span class="built_in">SUM</span>(amount)</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> SESSION(rowtime, <span class="type">INTERVAL</span> ‘<span class="number">12</span>’ <span class="keyword">HOUR</span>), <span class="keyword">user</span></span><br></pre></td></tr></table></figure></div>

<p>Table API 和 SQL 捆绑在 flink-table Maven 工件中。必须将以下依赖项添加到你的项目才能使用 Table API 和 SQL：</p>
<div class="highlight-container" data-rel="Xml"><figure class="iseeu highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

<p>另外，你需要为 Flink 的 Scala 批处理或流式 API 添加依赖项。对于批量查询，您需要添加：</p>
<div class="highlight-container" data-rel="Xml"><figure class="iseeu highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

<h3 id="7-2-Flink-SQL-实战案例"><a href="#7-2-Flink-SQL-实战案例" class="headerlink" title="7.2  Flink SQL 实战案例"></a>7.2  Flink SQL 实战案例</h3><h4 id="1-批数据-SQL"><a href="#1-批数据-SQL" class="headerlink" title="1) 批数据 SQL"></a>1) 批数据 SQL</h4><p>用法：</p>
<ol>
<li>构建 Table 运行环境</li>
<li>将 DataSet 注册为一张表</li>
<li>使用 Table 运行环境的 sqlQuery 方法来执行 SQL 语句</li>
</ol>
<p>示例：<strong>使用 Flink SQL 统计用户消费订单的总金额、最大金额、最小金额、订单总数</strong>。</p>
<table>
<thead>
<tr>
<th align="center">订单 id</th>
<th align="center">用户名</th>
<th align="center">订单日期</th>
<th align="center">消费金额</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">Zhangsan</td>
<td align="center">2018-10-20 15:30</td>
<td align="center">358.5</td>
</tr>
</tbody></table>
<p>测试数据（订单 ID、用户名、订单日期、订单金额）:</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Order</span>(<span class="number">1</span>, <span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;2018-10-20 15:30&quot;</span>, <span class="number">358.5</span>),</span><br><span class="line"><span class="type">Order</span>(<span class="number">2</span>, <span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;2018-10-20 16:30&quot;</span>, <span class="number">131.5</span>),</span><br><span class="line"><span class="type">Order</span>(<span class="number">3</span>, <span class="string">&quot;lisi&quot;</span>, <span class="string">&quot;2018-10-20 16:30&quot;</span>, <span class="number">127.5</span>),</span><br><span class="line"><span class="type">Order</span>(<span class="number">4</span>, <span class="string">&quot;lisi&quot;</span>, <span class="string">&quot;2018-10-20 16:30&quot;</span>, <span class="number">328.5</span>),</span><br><span class="line"><span class="type">Order</span>(<span class="number">5</span>, <span class="string">&quot;lisi&quot;</span>, <span class="string">&quot;2018-10-20 16:30&quot;</span>, <span class="number">432.5</span>),</span><br><span class="line"><span class="type">Order</span>(<span class="number">6</span>, <span class="string">&quot;zhaoliu&quot;</span>, <span class="string">&quot;2018-10-20 22:30&quot;</span>, <span class="number">451.0</span>),</span><br><span class="line"><span class="type">Order</span>(<span class="number">7</span>, <span class="string">&quot;zhaoliu&quot;</span>, <span class="string">&quot;2018-10-20 22:30&quot;</span>, <span class="number">362.0</span>),</span><br><span class="line"><span class="type">Order</span>(<span class="number">8</span>, <span class="string">&quot;zhaoliu&quot;</span>, <span class="string">&quot;2018-10-20 22:30&quot;</span>, <span class="number">364.0</span>),</span><br><span class="line"><span class="type">Order</span>(<span class="number">9</span>, <span class="string">&quot;zhaoliu&quot;</span>, <span class="string">&quot;2018-10-20 22:30&quot;</span>, <span class="number">341.0</span>)</span><br></pre></td></tr></table></figure></div>

<p>步骤:</p>
<ol>
<li>获取一个批处理运行环境</li>
<li>获取一个 Table 运行环境</li>
<li>创建一个样例类 Order 用来映射数据（订单名、用户名、订单日期、订单金额）</li>
<li>基于本地 Order 集合创建一个 DataSet source</li>
<li>使用 Table 运行环境将 DataSet 注册为一张表</li>
<li>使用 SQL 语句来操作数据（统计用户消费订单的总金额、最大金额、最小金额、订单总数）</li>
<li>使用 TableEnv.toDataSet 将 Table 转换为 DataSet</li>
<li>打印测试</li>
</ol>
<p><strong>示例代码</strong>：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">Table</span>, <span class="type">TableEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala.<span class="type">BatchTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用Flink SQL统计用户消费订单的总金额、最大金额、最小金额、订单总数。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchFlinkSqlDemo</span> </span>&#123;</span><br><span class="line">  <span class="comment">//3. 创建一个样例类 Order 用来映射数据（订单名、用户名、订单日期、订单金额）</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Order</span>(<span class="params">id:<span class="type">Int</span>, userName:<span class="type">String</span>, createTime:<span class="type">String</span>, money:<span class="type">Double</span></span>)</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 实现思路：</span></span><br><span class="line"><span class="comment">     * 1. 获取一个批处理运行环境</span></span><br><span class="line"><span class="comment">     * 2. 获取一个Table运行环境</span></span><br><span class="line"><span class="comment">     * 3. 创建一个样例类 Order 用来映射数据（订单名、用户名、订单日期、订单金额）</span></span><br><span class="line"><span class="comment">     * 4. 基于本地 Order 集合创建一个DataSet source</span></span><br><span class="line"><span class="comment">     * 5. 使用Table运行环境将DataSet注册为一张表</span></span><br><span class="line"><span class="comment">     * 6. 使用SQL语句来操作数据（统计用户消费订单的总金额、最大金额、最小金额、订单总数）</span></span><br><span class="line"><span class="comment">     * 7. 使用TableEnv.toDataSet将Table转换为DataSet</span></span><br><span class="line"><span class="comment">     * 8. 打印测试</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//1. 获取一个批处理运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">ExecutionEnvironment</span> = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//2. 获取一个Table运行环境</span></span><br><span class="line">    <span class="keyword">val</span> tabEnv: <span class="type">BatchTableEnvironment</span> = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line">    <span class="comment">//4. 基于本地 Order 集合创建一个DataSet source</span></span><br><span class="line">    <span class="keyword">val</span> orderDataSet: <span class="type">DataSet</span>[<span class="type">Order</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Order</span>(<span class="number">1</span>, <span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;2018-10-20 15:30&quot;</span>, <span class="number">358.5</span>),</span><br><span class="line">      <span class="type">Order</span>(<span class="number">2</span>, <span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;2018-10-20 16:30&quot;</span>, <span class="number">131.5</span>),</span><br><span class="line">      <span class="type">Order</span>(<span class="number">3</span>, <span class="string">&quot;lisi&quot;</span>, <span class="string">&quot;2018-10-20 16:30&quot;</span>, <span class="number">127.5</span>),</span><br><span class="line">      <span class="type">Order</span>(<span class="number">4</span>, <span class="string">&quot;lisi&quot;</span>, <span class="string">&quot;2018-10-20 16:30&quot;</span>, <span class="number">328.5</span>),</span><br><span class="line">      <span class="type">Order</span>(<span class="number">5</span>, <span class="string">&quot;lisi&quot;</span>, <span class="string">&quot;2018-10-20 16:30&quot;</span>, <span class="number">432.5</span>),</span><br><span class="line">      <span class="type">Order</span>(<span class="number">6</span>, <span class="string">&quot;zhaoliu&quot;</span>, <span class="string">&quot;2018-10-20 22:30&quot;</span>, <span class="number">451.0</span>),</span><br><span class="line">      <span class="type">Order</span>(<span class="number">7</span>, <span class="string">&quot;zhaoliu&quot;</span>, <span class="string">&quot;2018-10-20 22:30&quot;</span>, <span class="number">362.0</span>),</span><br><span class="line">      <span class="type">Order</span>(<span class="number">8</span>, <span class="string">&quot;zhaoliu&quot;</span>, <span class="string">&quot;2018-10-20 22:30&quot;</span>, <span class="number">364.0</span>),</span><br><span class="line">      <span class="type">Order</span>(<span class="number">9</span>, <span class="string">&quot;zhaoliu&quot;</span>, <span class="string">&quot;2018-10-20 22:30&quot;</span>, <span class="number">341.0</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5. 使用Table运行环境将DataSet注册为一张表</span></span><br><span class="line">    tabEnv.registerDataSet(<span class="string">&quot;t_order&quot;</span>, orderDataSet)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6. 使用SQL语句来操作数据（统计用户消费订单的总金额、最大金额、最小金额、订单总数）</span></span><br><span class="line">    <span class="comment">//用户消费订单的总金额、最大金额、最小金额、订单总数。</span></span><br><span class="line">    <span class="keyword">val</span> sql =</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        | select</span></span><br><span class="line"><span class="string">        |   userName,</span></span><br><span class="line"><span class="string">        |   sum(money) totalMoney,</span></span><br><span class="line"><span class="string">        |   max(money) maxMoney,</span></span><br><span class="line"><span class="string">        |   min(money) minMoney,</span></span><br><span class="line"><span class="string">        |   count(1) totalCount</span></span><br><span class="line"><span class="string">        |  from t_order</span></span><br><span class="line"><span class="string">        |  group by userName</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin  <span class="comment">//在scala中stripMargin默认是“|”作为多行连接符</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//7. 使用TableEnv.toDataSet将Table转换为DataSet</span></span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">Table</span> = tabEnv.sqlQuery(sql)</span><br><span class="line">    table.printSchema()</span><br><span class="line">    tabEnv.toDataSet[<span class="type">Row</span>](table).print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="2-流数据-SQL"><a href="#2-流数据-SQL" class="headerlink" title="2) 流数据 SQL"></a>2) 流数据 SQL</h4><p>流处理中也可以支持 SQL。但是需要注意以下几点：</p>
<ol>
<li>要使用流处理的 SQL，必须要添加水印时间</li>
<li>使用 registerDataStream 注册表的时候，使用 ‘ 来指定字段</li>
<li>注册表的时候，必须要指定一个 rowtime，否则无法在 SQL 中使用窗口</li>
<li>必须要导入 import org.apache.flink.table.api.scala._ 隐式参数</li>
<li>SQL 中使用 trumble(时间列名, interval ‘时间’ sencond) 来进行定义窗口</li>
</ol>
<p>示例：<strong>使用 Flink SQL 来统计 5 秒内 用户的 订单总数、订单的最大金额、订单的最小金额</strong>。</p>
<p>步骤</p>
<ol>
<li>获取流处理运行环境</li>
<li>获取 Table 运行环境</li>
<li>设置处理时间为 EventTime</li>
<li>创建一个订单样例类 Order ，包含四个字段（订单 ID、用户 ID、订单金额、时间戳）</li>
<li>创建一个自定义数据源<ul>
<li>使用 for 循环生成 1000 个订单</li>
<li>随机生成订单 ID（UUID）</li>
<li>随机生成用户 ID（0-2）</li>
<li>随机生成订单金额（0-100）</li>
<li>时间戳为当前系统时间</li>
<li>每隔 1 秒生成一个订单</li>
</ul>
</li>
<li>添加水印，允许延迟 2 秒</li>
<li>导入 import org.apache.flink.table.api.scala._ 隐式参数</li>
<li>使用 registerDataStream 注册表，并分别指定字段，还要指定 rowtime 字段</li>
<li>编写 SQL 语句统计用户订单总数、最大金额、最小金额 分组时要使用 tumble(时间列, interval ‘窗口时间’ second) 来创建窗口</li>
<li>使用 tableEnv.sqlQuery 执行 sql 语句</li>
<li>将 SQL 的执行结果转换成 DataStream 再打印出来</li>
<li>启动流处理程序</li>
</ol>
<p><strong>示例代码</strong>：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">UUID</span></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.<span class="type">TimeUnit</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">RichSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">Table</span>, <span class="type">TableEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.<span class="type">AssignerWithPeriodicWatermarks</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.<span class="type">BoundedOutOfOrdernessTimestampExtractor</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.<span class="type">Watermark</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：</span></span><br><span class="line"><span class="comment"> *  使用Flink SQL来统计5秒内 用户的 订单总数、订单的最大金额、订单的最小金额</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  timestamp是关键字不能作为字段的名字（关键字不能作为字段名字）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamFlinkSqlDemo</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *  1. 获取流处理运行环境</span></span><br><span class="line"><span class="comment">     * 2. 获取Table运行环境</span></span><br><span class="line"><span class="comment">     * 3. 设置处理时间为 EventTime</span></span><br><span class="line"><span class="comment">     * 4. 创建一个订单样例类 Order ，包含四个字段（订单ID、用户ID、订单金额、时间戳）</span></span><br><span class="line"><span class="comment">     * 5. 创建一个自定义数据源</span></span><br><span class="line"><span class="comment">     *    使用for循环生成1000个订单</span></span><br><span class="line"><span class="comment">     *    随机生成订单ID（UUID）</span></span><br><span class="line"><span class="comment">     *    随机生成用户ID（0-2）</span></span><br><span class="line"><span class="comment">     *    随机生成订单金额（0-100）</span></span><br><span class="line"><span class="comment">     *    时间戳为当前系统时间</span></span><br><span class="line"><span class="comment">     *    每隔1秒生成一个订单</span></span><br><span class="line"><span class="comment">     * 6. 添加水印，允许延迟2秒</span></span><br><span class="line"><span class="comment">     * 7. 导入 import org.apache.flink.table.api.scala._ 隐式参数</span></span><br><span class="line"><span class="comment">     * 8. 使用 registerDataStream 注册表，并分别指定字段，还要指定rowtime字段</span></span><br><span class="line"><span class="comment">     * 9. 编写SQL语句统计用户订单总数、最大金额、最小金额</span></span><br><span class="line"><span class="comment">     * 分组时要使用 tumble(时间列, interval &#x27;窗口时间&#x27; second) 来创建窗口</span></span><br><span class="line"><span class="comment">     * 10. 使用 tableEnv.sqlQuery 执行sql语句</span></span><br><span class="line"><span class="comment">     * 11. 将SQL的执行结果转换成DataStream再打印出来</span></span><br><span class="line"><span class="comment">     * 12. 启动流处理程序</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">// 3. 创建一个订单样例类`Order`，包含四个字段（订单ID、用户ID、订单金额、时间戳）</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Order</span>(<span class="params">orderId:<span class="type">String</span>, userId:<span class="type">Int</span>, money:<span class="type">Long</span>, createTime:<span class="type">Long</span></span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 1. 创建流处理运行环境</span></span><br><span class="line">      <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">      <span class="comment">// 2. 设置处理时间为`EventTime`</span></span><br><span class="line">      env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">      <span class="comment">//获取table的运行环境</span></span><br><span class="line">      <span class="keyword">val</span> tableEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line">      <span class="comment">// 4. 创建一个自定义数据源</span></span><br><span class="line">      <span class="keyword">val</span> orderDataStream = env.addSource(<span class="keyword">new</span> <span class="type">RichSourceFunction</span>[<span class="type">Order</span>] &#123;</span><br><span class="line">        <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Order</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="comment">// - 随机生成订单ID（UUID）</span></span><br><span class="line">          <span class="comment">// - 随机生成用户ID（0-2）</span></span><br><span class="line">          <span class="comment">// - 随机生成订单金额（0-100）</span></span><br><span class="line">          <span class="comment">// - 时间戳为当前系统时间</span></span><br><span class="line">          <span class="comment">// - 每隔1秒生成一个订单</span></span><br><span class="line">          <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until <span class="number">1000</span> <span class="keyword">if</span> isRunning) &#123;</span><br><span class="line">            <span class="keyword">val</span> order = <span class="type">Order</span>(<span class="type">UUID</span>.randomUUID().toString, <span class="type">Random</span>.nextInt(<span class="number">3</span>), <span class="type">Random</span>.nextInt(<span class="number">101</span>),</span><br><span class="line">              <span class="type">System</span>.currentTimeMillis())</span><br><span class="line">            <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>.sleep(<span class="number">1</span>)</span><br><span class="line">            ctx.collect(order)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123; isRunning = <span class="literal">false</span> &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">// 5. 添加水印，允许延迟2秒</span></span><br><span class="line">      <span class="keyword">val</span> watermarkDataStream = orderDataStream.assignTimestampsAndWatermarks(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">Order</span>](<span class="type">Time</span>.seconds(<span class="number">2</span>)) &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Order</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> eventTime = element.createTime</span><br><span class="line">            eventTime</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line">      <span class="comment">// 6. 导入`import org.apache.flink.table.api.scala._`隐式参数</span></span><br><span class="line">      <span class="comment">// 7. 使用`registerDataStream`注册表，并分别指定字段，还要指定rowtime字段</span></span><br><span class="line">      <span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line">      tableEnv.registerDataStream(<span class="string">&quot;t_order&quot;</span>, watermarkDataStream, &#x27;orderId, &#x27;userId, &#x27;money,&#x27;createTime.rowtime)</span><br><span class="line">      <span class="comment">// 8. 编写SQL语句统计用户订单总数、最大金额、最小金额</span></span><br><span class="line">      <span class="comment">// - 分组时要使用`tumble(时间列, interval &#x27;窗口时间&#x27; second)`来创建窗口</span></span><br><span class="line">      <span class="keyword">val</span> sql =</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |select</span></span><br><span class="line"><span class="string">        | userId,</span></span><br><span class="line"><span class="string">        | count(1) as totalCount,</span></span><br><span class="line"><span class="string">        | max(money) as maxMoney,</span></span><br><span class="line"><span class="string">        | min(money) as minMoney</span></span><br><span class="line"><span class="string">        | from</span></span><br><span class="line"><span class="string">        | t_order</span></span><br><span class="line"><span class="string">        | group by</span></span><br><span class="line"><span class="string">        | tumble(createTime, interval &#x27;5&#x27; second),</span></span><br><span class="line"><span class="string">        | userId</span></span><br><span class="line"><span class="string">      &quot;&quot;&quot;</span>.stripMargin</span><br><span class="line">      <span class="comment">// 9. 使用`tableEnv.sqlQuery`执行sql语句</span></span><br><span class="line">      <span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.sqlQuery(sql)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 10. 将SQL的执行结果转换成DataStream再打印出来</span></span><br><span class="line">      table.toRetractStream[<span class="type">Row</span>].print()</span><br><span class="line">      env.execute(<span class="string">&quot;StreamSQLApp&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="9-Flink-CEP"><a href="#9-Flink-CEP" class="headerlink" title="9  Flink  CEP"></a>9  Flink  CEP</h2><p>我们在看直播的时候，不管对于主播还是用户来说，非常重要的一项就是弹幕文化。为了增加直播趣味性和互动性, 各大网络直播平台纷纷采用弹窗弹幕作为用户<strong>实时交流</strong>的方式，内容丰富且形式多样的弹幕数据中隐含着复杂的用户属性与用户行为, 研究并理解在线直播平台用户具有弹幕内容审核与监控、舆论热点预测、个性化摘要标注等多方面的应用价值。</p>
<p>本文不分析弹幕数据的应用价值，只通过弹幕内容审核与监控案例来了解下Flink CEP的概念及功能。</p>
<p>在用户发弹幕时，直播平台主要实时监控识别两类弹幕内容：<em>一类是发布不友善弹幕的用户 ，一类是刷屏的用户</em>。</p>
<p>我们先记住上述需要<strong>实时监控识别</strong>的两类用户，接下来介绍Flink CEP的API，然后使用CEP解决上述问题。</p>
<h3 id="9-1-Flink-CEP-是什么"><a href="#9-1-Flink-CEP-是什么" class="headerlink" title="9.1  Flink CEP 是什么"></a>9.1  Flink CEP 是什么</h3><p>Flink CEP是一个基于Flink的复杂事件处理库，可以从多个数据流中发现复杂事件，识别有意义的事件（例如机会或者威胁），并尽快的做出响应，而不是需要等待几天或则几个月相当长的时间，才发现问题。</p>
<h3 id="9-2-Flink-CEP-API"><a href="#9-2-Flink-CEP-API" class="headerlink" title="9.2 Flink CEP API"></a>9.2 Flink CEP API</h3><p>CEP API的核心是Pattern(模式) API，它允许你快速定义复杂的事件模式。每个模式包含多个阶段（stage）或者我们也可称为状态（state）。从一个状态切换到另一个状态，用户可以指定条件，这些条件可以作用在邻近的事件或独立事件上。</p>
<p>介绍API之前先来理解几个概念：</p>
<h4 id="1-模式与模式序列"><a href="#1-模式与模式序列" class="headerlink" title="1) 模式与模式序列"></a>1) 模式与模式序列</h4><ul>
<li>简单模式称为模式，将最终在数据流中进行搜索匹配的复杂模式序列称为模式序列，每个复杂模式序列是由多个简单模式组成。</li>
<li>每个模式必须具有唯一的名称，我们可以使用模式名称来标识该模式匹配到的事件。</li>
</ul>
<h4 id="2-单个模式"><a href="#2-单个模式" class="headerlink" title="2) 单个模式"></a>2) 单个模式</h4><p>一个模式既可以是单例的，也可以是循环的。<strong>单例模式接受单个事件，循环模式可以接受多个事件</strong>。</p>
<h4 id="3-模式示例："><a href="#3-模式示例：" class="headerlink" title="3) 模式示例："></a>3) 模式示例：</h4><p>有如下模式：<code>a b+ c？d</code></p>
<p>其中<code>a,b,c,d</code>这些字母代表的是模式，<code>+</code>代表循环，<code>b+</code>就是循环模式；<code>?</code>代表可选，<code>c?</code>就是可选模式；</p>
<p>所以上述模式的意思就是：<code>a</code>后面可以跟一个或多个<code>b</code>，后面再可选的跟<code>c</code>，最后跟<code>d</code>。</p>
<p>其中<code>a、c? 、d</code>是单例模式，<code>b+</code>是循环模式。</p>
<p>一般情况下，模式都是单例模式，可以使用量词（Quantifiers）将其转换为循环模式。</p>
<p>每个模式可以带有一个或多个条件，这些条件是基于事件接收进行定义的。或者说，每个模式通过一个或多个条件来匹配和接收事件。</p>
<p>了解完上述概念后，接下来介绍下案例中需要用到的几个CEP API：</p>
<h4 id="4-案例中用到的CEP-API："><a href="#4-案例中用到的CEP-API：" class="headerlink" title="4) 案例中用到的CEP API："></a>4) <em>案例中用到的CEP API：</em></h4><ul>
<li><p>Begin：定义一个起始模式状态</p>
<p>用法：<code>start = Pattern.&lt;Event&gt;begin(&quot;start&quot;);</code></p>
</li>
<li><p>Next：附加一个新的模式状态。匹配事件必须直接接续上一个匹配事件</p>
<p>用法：<code>next = start.next(&quot;next&quot;);</code></p>
</li>
<li><p>Where：定义当前模式状态的过滤条件。仅当事件通过过滤器时，它才能与状态匹配</p>
<p>用法：<code>patternState.where(_.message == &quot;yyds&quot;);</code></p>
</li>
<li><p>Within: 定义事件序列与模式匹配的最大时间间隔。如果未完成的事件序列超过此时间，则将其丢弃</p>
<p>用法：<code>patternState.within(Time.seconds(10));</code></p>
</li>
<li><p>Times：一个给定类型的事件出现了指定次数</p>
<p>用法：<code>patternState.times(5);</code></p>
</li>
</ul>
<p>API 先介绍以上这几个，接下来我们解决下文章开头提到的案例：</p>
<h3 id="9-3-监测用户弹幕行为案例"><a href="#9-3-监测用户弹幕行为案例" class="headerlink" title="9.3   监测用户弹幕行为案例"></a>9.3   监测用户弹幕行为案例</h3><h4 id="案例一：监测恶意用户"><a href="#案例一：监测恶意用户" class="headerlink" title="案例一：监测恶意用户"></a>案例一：监测恶意用户</h4><p>规则：<strong>用户如果在10s内，同时输入 TMD 超过5次，就认为用户为恶意攻击，识别出该用户</strong>。</p>
<p>使用 Flink CEP 检测恶意用户：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.<span class="type">PatternSelectFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.scala.&#123;<span class="type">CEP</span>, <span class="type">PatternStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.scala.pattern.<span class="type">Pattern</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">OutputTag</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BarrageBehavior01</span> </span>&#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span>  <span class="title">LoginEvent</span>(<span class="params">userId:<span class="type">String</span>, message:<span class="type">String</span>, timestamp:<span class="type">Long</span></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = userId</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用IngestionTime作为EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用于观察测试数据处理顺序</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 模拟数据源</span></span><br><span class="line">    <span class="keyword">val</span> loginEventStream: <span class="type">DataStream</span>[<span class="type">LoginEvent</span>] = env.fromCollection(</span><br><span class="line">      <span class="type">List</span>(</span><br><span class="line">        <span class="type">LoginEvent</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;TMD&quot;</span>, <span class="number">1618498576</span>),</span><br><span class="line">        <span class="type">LoginEvent</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;TMD&quot;</span>, <span class="number">1618498577</span>),</span><br><span class="line">        <span class="type">LoginEvent</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;TMD&quot;</span>, <span class="number">1618498579</span>),</span><br><span class="line">        <span class="type">LoginEvent</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;TMD&quot;</span>, <span class="number">1618498582</span>),</span><br><span class="line">        <span class="type">LoginEvent</span>(<span class="string">&quot;2&quot;</span>, <span class="string">&quot;TMD&quot;</span>, <span class="number">1618498583</span>), </span><br><span class="line">        <span class="type">LoginEvent</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;TMD&quot;</span>, <span class="number">1618498585</span>)</span><br><span class="line">      )</span><br><span class="line">    ).assignAscendingTimestamps(_.timestamp * <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义模式</span></span><br><span class="line">    <span class="keyword">val</span> loginEventPattern: <span class="type">Pattern</span>[<span class="type">LoginEvent</span>, <span class="type">LoginEvent</span>] = <span class="type">Pattern</span>.begin[<span class="type">LoginEvent</span>](<span class="string">&quot;begin&quot;</span>)</span><br><span class="line">      .where(_.message == <span class="string">&quot;TMD&quot;</span>)</span><br><span class="line">      .times(<span class="number">5</span>)</span><br><span class="line">      .within(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//匹配模式</span></span><br><span class="line">    <span class="keyword">val</span> patternStream: <span class="type">PatternStream</span>[<span class="type">LoginEvent</span>] = <span class="type">CEP</span>.pattern(loginEventStream.keyBy(_.userId), loginEventPattern)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> scala.collection.<span class="type">Map</span></span><br><span class="line">    <span class="keyword">val</span> result = patternStream.select((pattern:<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">LoginEvent</span>]])=&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> first = pattern.getOrElse(<span class="string">&quot;begin&quot;</span>, <span class="literal">null</span>).iterator.next()</span><br><span class="line">      (first.userId, first.timestamp)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//恶意用户，实际处理可将按用户进行禁言等处理，为简化此处仅打印出该用户</span></span><br><span class="line">    result.print(<span class="string">&quot;恶意用户&gt;&gt;&gt;&quot;</span>)</span><br><span class="line">    env.execute(<span class="string">&quot;BarrageBehavior01&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h4 id="案例二：监测刷屏用户"><a href="#案例二：监测刷屏用户" class="headerlink" title="案例二：监测刷屏用户"></a>案例二：监测刷屏用户</h4><p>规则：<strong>用户如果在10s内，同时连续输入同样一句话超过5次，就认为是恶意刷屏</strong>。</p>
<p>使用 Flink CEP检测刷屏用户</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BarrageBehavior02</span> </span>&#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Message</span>(<span class="params">userId: <span class="type">String</span>, ip: <span class="type">String</span>, msg: <span class="type">String</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//初始化运行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置并行度</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 模拟数据源</span></span><br><span class="line">    <span class="keyword">val</span> loginEventStream: <span class="type">DataStream</span>[<span class="type">Message</span>] = env.fromCollection(</span><br><span class="line">      <span class="type">List</span>(</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;192.168.0.1&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;192.168.0.2&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;192.168.0.3&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;1&quot;</span>, <span class="string">&quot;192.168.0.4&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;2&quot;</span>, <span class="string">&quot;192.168.10.10&quot;</span>, <span class="string">&quot;shanghai&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;3&quot;</span>, <span class="string">&quot;192.168.10.10&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;3&quot;</span>, <span class="string">&quot;192.168.10.11&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;4&quot;</span>, <span class="string">&quot;192.168.10.10&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;5&quot;</span>, <span class="string">&quot;192.168.10.11&quot;</span>, <span class="string">&quot;shanghai&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;4&quot;</span>, <span class="string">&quot;192.168.10.12&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;5&quot;</span>, <span class="string">&quot;192.168.10.13&quot;</span>, <span class="string">&quot;shanghai&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;5&quot;</span>, <span class="string">&quot;192.168.10.14&quot;</span>, <span class="string">&quot;shanghai&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;5&quot;</span>, <span class="string">&quot;192.168.10.15&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;6&quot;</span>, <span class="string">&quot;192.168.10.16&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;6&quot;</span>, <span class="string">&quot;192.168.10.17&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;6&quot;</span>, <span class="string">&quot;192.168.10.18&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;5&quot;</span>, <span class="string">&quot;192.168.10.18&quot;</span>, <span class="string">&quot;shanghai&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;6&quot;</span>, <span class="string">&quot;192.168.10.19&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;6&quot;</span>, <span class="string">&quot;192.168.10.19&quot;</span>, <span class="string">&quot;beijing&quot;</span>),</span><br><span class="line">        <span class="type">Message</span>(<span class="string">&quot;5&quot;</span>, <span class="string">&quot;192.168.10.18&quot;</span>, <span class="string">&quot;shanghai&quot;</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义模式</span></span><br><span class="line">    <span class="keyword">val</span> loginbeijingPattern = <span class="type">Pattern</span>.begin[<span class="type">Message</span>](<span class="string">&quot;start&quot;</span>)</span><br><span class="line">      .where(_.msg != <span class="literal">null</span>) <span class="comment">//一条登录失败</span></span><br><span class="line">      .times(<span class="number">5</span>).optional  <span class="comment">//将满足五次的数据配对打印</span></span><br><span class="line">      .within(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//进行分组匹配</span></span><br><span class="line">    <span class="keyword">val</span> loginbeijingDataPattern = <span class="type">CEP</span>.pattern(loginEventStream.keyBy(_.userId), loginbeijingPattern)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//查找符合规则的数据</span></span><br><span class="line">    <span class="keyword">val</span> loginbeijingResult: <span class="type">DataStream</span>[<span class="type">Option</span>[<span class="type">Iterable</span>[<span class="type">Message</span>]]] = loginbeijingDataPattern.select(patternSelectFun = (pattern: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Message</span>]]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> loginEventList: <span class="type">Option</span>[<span class="type">Iterable</span>[<span class="type">Message</span>]] = <span class="literal">null</span></span><br><span class="line">      loginEventList = pattern.get(<span class="string">&quot;start&quot;</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(value) =&gt; &#123;</span><br><span class="line">          <span class="keyword">if</span> (value.toList.map(x =&gt; (x.userId, x.msg)).distinct.size == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="type">Some</span>(value)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">None</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      loginEventList</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打印测试</span></span><br><span class="line">    loginbeijingResult.filter(x=&gt;x!=<span class="type">None</span>).map(x=&gt;&#123;</span><br><span class="line">      x <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(value)=&gt; value</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">&quot;BarrageBehavior02)</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure></div>

<h3 id="9-4-Flink-CEP-API"><a href="#9-4-Flink-CEP-API" class="headerlink" title="9.4  Flink CEP API"></a>9.4  Flink CEP API</h3><p>除了案例中介绍的几个API外，我们在介绍下其他的常用API：</p>
<h4 id="1-条件-API"><a href="#1-条件-API" class="headerlink" title="1) 条件 API"></a>1) 条件 API</h4><p>为了让传入事件被模式所接受，给模式指定传入事件必须满足的条件，这些条件由事件本身的属性或者前面匹配过的事件的属性统计量等来设定。比如，事件的某个值大于5，或者大于先前接受事件的某个值的平均值。</p>
<p>可以使用<code>pattern.where()、pattern.or()、pattern.until()</code>方法来指定条件。条件既可以是迭代条件<code>IterativeConditions</code>，也可以是简单条件<code>SimpleConditions</code>。</p>
<p>FlinkCEP支持事件之间的三种临近条件：</p>
<ul>
<li><p>**next()**：严格的满足条件</p>
<p>示例：模式为<code>begin(&quot;first&quot;).where(_.name=&#39;a&#39;).next(&quot;second&quot;).where(.name=&#39;b&#39;)</code>当且仅当数据为a,b时，模式才会被命中。如果数据为a,c,b，由于a的后面跟了c，所以a会被直接丢弃，模式不会命中。</p>
</li>
<li><p>**followedBy()**：松散的满足条件</p>
<p>示例：模式为<code>begin(&quot;first&quot;).where(_.name=&#39;a&#39;).followedBy(&quot;second&quot;).where(.name=&#39;b&#39;)</code>当且仅当数据为a,b或者为a,c,b，模式均被命中，中间的c会被忽略掉。</p>
</li>
<li><p>**followedByAny()**：非确定的松散满足条件</p>
<p>示例： 模式为<code>begin(&quot;first&quot;).where(_.name=&#39;a&#39;).followedByAny(&quot;second&quot;).where(.name=&#39;b&#39;)</code>当且仅当数据为<code>a,c,b,b</code>时，对于followedBy模式而言命中的为{a,b}，对于followedByAny而言会有两次命中{a,b},{a,b}。</p>
</li>
</ul>
<h4 id="2-量词-API"><a href="#2-量词-API" class="headerlink" title="2) 量词 API"></a>2) 量词 API</h4><p>还记得我们在上面讲解模式的概念时说过的一句话嘛：<em>一般情况下，模式都是单例模式，可以使用量词（Quantifiers）将其转换为循环模式</em>。这里的量词就是指的量词API。</p>
<p>以下这几个量词API，可以将模式指定为循环模式：</p>
<ul>
<li><code>pattern.oneOrMore()</code>： 一个给定的事件有一次或多次出现，例如上面提到的b+。</li>
<li><code>pattern.times(#ofTimes)</code>： 一个给定类型的事件出现了指定次数，例如4次。</li>
<li><code>pattern.times(#fromTimes, #toTimes)</code>：一个给定类型的事件出现的次数在指定次数范围内，例如2~4次。</li>
<li>可以使用<code>pattern.greedy()</code>方法将模式变成<strong>循环模式</strong>，但是不能让一组模式都变成循环模式。greedy：就是尽可能的重复。</li>
<li>使用<code>pattern.optional()</code>方法将循环模式变成<strong>可选的</strong>，即可以是循环模式也可以是单个模式。</li>
</ul>
<h4 id="3-匹配后的跳过策略"><a href="#3-匹配后的跳过策略" class="headerlink" title="3) 匹配后的跳过策略"></a>3) 匹配后的跳过策略</h4><p>所谓的匹配跳过策略，是对多个成功匹配的模式进行筛选。也就是说如果多个匹配成功，可能我不需要这么多，按照匹配策略，过滤下就可以。</p>
<p>Flink中有<em>五种</em>跳过策略：</p>
<ul>
<li><strong>NO_SKIP</strong>: 不过滤，所有可能的匹配都会被发出。</li>
<li><strong>SKIP_TO_NEXT</strong>: 丢弃与开始匹配到的事件相同的事件，发出开始匹配到的事件，即直接跳到下一个模式匹配到的事件，以此类推。</li>
<li><strong>SKIP_PAST_LAST_EVENT</strong>: 丢弃匹配开始后但结束之前匹配到的事件。</li>
<li><strong>SKIP_TO_FIRST[PatternName]</strong>: 丢弃匹配开始后但在PatternName模式匹配到的第一个事件之前匹配到的事件。</li>
<li><strong>SKIP_TO_LAST[PatternName]</strong>: 丢弃匹配开始后但在PatternName模式匹配到的最后一个事件之前匹配到的事件。</li>
</ul>
<p>怎么理解上述策略，我们以<strong>NO_SKIP</strong>和<strong>SKIP_PAST_LAST_EVENT</strong>为例讲解下：</p>
<p>在模式为：<code>begin(&quot;start&quot;).where(_.name=&#39;a&#39;).oneOrMore().followedBy(&quot;second&quot;).where(_.name=&#39;b&#39;)</code>中，我们输入数据：<code>a,a,a,a,b</code> ，如果是NO_SKIP策略，即不过滤策略，模式匹配到的是:{a,b},{a,a,b},{a,a,a,b},{a,a,a,a,b}；如果是SKIP_PAST_LAST_EVENT策略，即丢弃匹配开始后但结束之前匹配到的事件，模式匹配到的是:{a,a,a,a,b}。</p>
<h3 id="9-5-Flink-CEP-的使用场景"><a href="#9-5-Flink-CEP-的使用场景" class="headerlink" title="9.5  Flink CEP 的使用场景"></a>9.5  Flink CEP 的使用场景</h3><p>除上述案例场景外，Flink CEP 还广泛用于网络欺诈，故障检测，风险规避，智能营销等领域。</p>
<h4 id="1-实时反作弊和风控"><a href="#1-实时反作弊和风控" class="headerlink" title="1) 实时反作弊和风控"></a>1) 实时反作弊和风控</h4><p>对于电商来说，羊毛党是必不可少的，国内拼多多曾爆出 100 元的无门槛券随便领，当晚被人褥几百亿，对于这种情况肯定是没有做好及时的风控。另外还有就是商家上架商品时通过频繁修改商品的名称和滥用标题来提高搜索关键字的排名、批量注册一批机器账号快速刷单来提高商品的销售量等作弊行为，各种各样的作弊手法也是需要不断的去制定规则去匹配这种行为。</p>
<h4 id="2-实时营销"><a href="#2-实时营销" class="headerlink" title="2) 实时营销"></a>2) 实时营销</h4><p>分析用户在手机 APP 的实时行为，统计用户的活动周期，通过为用户画像来给用户进行推荐。比如用户在登录 APP 后 1 分钟内只浏览了商品没有下单；用户在浏览一个商品后，3 分钟内又去查看其他同类的商品，进行比价行为；用户商品下单后 1 分钟内是否支付了该订单。如果这些数据都可以很好的利用起来，那么就可以给用户推荐浏览过的类似商品，这样可以大大提高购买率。</p>
<h4 id="3-实时网络攻击检测"><a href="#3-实时网络攻击检测" class="headerlink" title="3) 实时网络攻击检测"></a>3) 实时网络攻击检测</h4><p>当下互联网安全形势仍然严峻，网络攻击屡见不鲜且花样众多，这里我们以 DDOS（分布式拒绝服务攻击）产生的流入流量来作为遭受攻击的判断依据。对网络遭受的潜在攻击进行实时检测并给出预警，云服务厂商的多个数据中心会定时向监控中心上报其瞬时流量，如果流量在预设的正常范围内则认为是正常现象，不做任何操作；如果某数据中心在 10 秒内连续 5 次上报的流量超过正常范围的阈值，则触发一条警告的事件；如果某数据中心 30 秒内连续出现 30 次上报的流量超过正常范围的阈值，则触发严重的告警。</p>
<h3 id="6-Flink-CEP-的原理简单介绍"><a href="#6-Flink-CEP-的原理简单介绍" class="headerlink" title="6. Flink CEP 的原理简单介绍"></a>6. Flink CEP 的原理简单介绍</h3><p>Apache Flink在实现CEP时借鉴了<code>Efficient Pattern Matching over Event Streams</code>论文中NFA的模型，在这篇论文中，还提到了一些优化，我们在这里先跳过，只说下NFA的概念。</p>
<p>在这篇论文中，提到了NFA，也就是Non-determined Finite Automaton，叫做<strong>不确定的有限状态机</strong>，指的是状态有限，但是每个状态可能被转换成多个状态（不确定）。</p>
<p>非确定有限自动状态机</p>
<p>先介绍两个概念：</p>
<ul>
<li><strong>状态</strong>：状态分为三类，起始状态、中间状态和最终状态。</li>
<li><strong>转换</strong>：take&#x2F;ignore&#x2F;proceed都是转换的名称。</li>
</ul>
<p>在NFA匹配规则里，本质上是一个状态转换的过程。三种转换的含义如下所示：</p>
<ul>
<li><strong>Take:</strong> 主要是条件的判断，当过来一条数据进行判断，一旦满足条件，获取当前元素，放入到结果集中，然后将当前状态转移到下一个的状态。</li>
<li><strong>Proceed</strong>：当前的状态可以不依赖任何的事件转移到下一个状态，比如说透传的意思。</li>
<li><strong>Ignore</strong>：当一条数据到来的时候，可以忽略这个消息事件，当前的状态保持不变，相当于自己到自己的一个状态。</li>
</ul>
<p><strong>NFA的特点</strong>：在NFA中，给定当前状态，可能有多个下一个状态。可以随机选择下一个状态，也可以并行（同时）选择下一个状态。输入符号可以为空。</p>
<h3 id="7-规则引擎"><a href="#7-规则引擎" class="headerlink" title="7. 规则引擎"></a>7. 规则引擎</h3><blockquote>
<p>规则引擎：将业务决策从应用程序代码中分离出来，并使用预定义的语义模块编写业务决策。接受数据输入，解释业务规则，并根据业务规则做出业务决策。<br>使用规则引擎可以通过降低实现复杂业务逻辑的组件的复杂性，降低应用程序的维护和可扩展性成本。</p>
</blockquote>
<h4 id="1-Drools"><a href="#1-Drools" class="headerlink" title="1) Drools"></a>1) Drools</h4><p>Drools 是一款使用 Java 编写的开源规则引擎，通常用来解决业务代码与业务规则的分离，它内置的 Drools Fusion 模块也提供 CEP 的功能。</p>
<p>优势：</p>
<ul>
<li>功能较为完善，具有如系统监控、操作平台等功能。</li>
<li>规则支持动态更新。</li>
</ul>
<p>劣势：</p>
<ul>
<li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li>
<li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li>
</ul>
<h4 id="2-Aviator"><a href="#2-Aviator" class="headerlink" title="2) Aviator"></a>2) Aviator</h4><p>Aviator 是一个高性能、轻量级的 Java 语言实现的表达式求值引擎，主要用于各种表达式的动态求值。</p>
<p>优势：</p>
<ul>
<li>支持大部分运算操作符。</li>
<li>支持函数调用和自定义函数。</li>
<li>支持正则表达式匹配。</li>
<li>支持传入变量并且性能优秀。</li>
</ul>
<p>劣势：</p>
<ul>
<li>没有 if else、do while 等语句，没有赋值语句，没有位运算符。</li>
</ul>
<h4 id="3-EasyRules"><a href="#3-EasyRules" class="headerlink" title="3) EasyRules"></a>3) EasyRules</h4><p>EasyRules 集成了 MVEL 和 SpEL 表达式的一款轻量级规则引擎。</p>
<p>优势：</p>
<ul>
<li>轻量级框架，学习成本低。</li>
<li>基于 POJO。</li>
<li>为定义业务引擎提供有用的抽象和简便的应用。</li>
<li>支持从简单的规则组建成复杂规则。</li>
</ul>
<h4 id="4-Esper"><a href="#4-Esper" class="headerlink" title="4) Esper"></a>4) Esper</h4><p>Esper 设计目标为 CEP 的轻量级解决方案，可以方便的嵌入服务中，提供 CEP 功能。</p>
<p>优势：</p>
<ul>
<li>轻量级可嵌入开发，常用的 CEP 功能简单好用。</li>
<li>EPL 语法与 SQL 类似，学习成本较低。</li>
</ul>
<p>劣势：</p>
<ul>
<li>单机全内存方案，需要整合其他分布式和存储。</li>
<li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li>
<li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li>
</ul>
<h4 id="5-Flink-CEP"><a href="#5-Flink-CEP" class="headerlink" title="5) Flink CEP"></a>5) Flink CEP</h4><p>Flink 是一个流式系统，具有高吞吐低延迟的特点，Flink CEP 是一套极具通用性、易于使用的实时流式事件处理方案。</p>
<p>优势：</p>
<ul>
<li>继承了 Flink 高吞吐的特点。</li>
<li>事件支持存储到外部，可以支持较长跨度的时间窗。</li>
<li>可以支持定时触达（用 followedBy ＋ PartternTimeoutFunction 实现）。</li>
</ul>
<h2 id="10-Flink-CDC"><a href="#10-Flink-CDC" class="headerlink" title="10  Flink  CDC"></a>10  Flink  CDC</h2><h3 id="10-1-CDC是什么"><a href="#10-1-CDC是什么" class="headerlink" title="10.1  CDC是什么"></a>10.1  CDC是什么</h3><p>CDC 是 <strong>Change Data Capture（变更数据获取）的简称</strong>。核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入、更新以及删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。</p>
<p>在广义的概念上，只要能捕获数据变更的技术，我们都可以称为 CDC 。通常我们说的 CDC 技术主要面向数据库的变更，是一种用于捕获数据库中数据变更的技术。</p>
<p>CDC 技术应用场景非常广泛：</p>
<ul>
<li><strong>数据同步</strong>，用于备份，容灾；</li>
<li><strong>数据分发</strong>，一个数据源分发给多个下游；</li>
<li><strong>数据采集</strong>(E)，面向数据仓库&#x2F;数据湖的 ETL 数据集成。</li>
</ul>
<h3 id="10-2-CDC-的种类"><a href="#10-2-CDC-的种类" class="headerlink" title="10.2  CDC 的种类"></a>10.2  CDC 的种类</h3><p>CDC 主要分为<strong>基于查询</strong>和<strong>基于 Binlog</strong> 两种方式，我们主要了解一下这两种之间的区别：</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">基于查询的 CDC</th>
<th align="center">基于 Binlog 的 CDC</th>
</tr>
</thead>
<tbody><tr>
<td align="center">开源产品</td>
<td align="center">Sqoop、Kafka JDBC Source</td>
<td align="center">Canal、Maxwell、Debezium</td>
</tr>
<tr>
<td align="center">执行模式</td>
<td align="center">Batch</td>
<td align="center">Streaming</td>
</tr>
<tr>
<td align="center">是否可以捕获所有数据变化</td>
<td align="center">否</td>
<td align="center">是</td>
</tr>
<tr>
<td align="center">延迟性</td>
<td align="center">高延迟</td>
<td align="center">低延迟</td>
</tr>
<tr>
<td align="center">是否增加数据库压力</td>
<td align="center">是</td>
<td align="center">否</td>
</tr>
</tbody></table>
<h3 id="10-3-传统CDC与Flink-CDC对比"><a href="#10-3-传统CDC与Flink-CDC对比" class="headerlink" title="10.3  传统CDC与Flink CDC对比"></a>10.3  传统CDC与Flink CDC对比</h3><h4 id="1-传统-CDC-ETL-分析"><a href="#1-传统-CDC-ETL-分析" class="headerlink" title="1) 传统 CDC ETL 分析"></a>1) 传统 CDC ETL 分析</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_13.png"
                      alt="img"
                ></p>
<h4 id="2-基于-Flink-CDC-的-ETL-分析"><a href="#2-基于-Flink-CDC-的-ETL-分析" class="headerlink" title="2) 基于 Flink CDC 的 ETL 分析"></a>2) 基于 Flink CDC 的 ETL 分析</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_14.png"
                      alt="img"
                ></p>
<h4 id="3-基于-Flink-CDC-的聚合分析"><a href="#3-基于-Flink-CDC-的聚合分析" class="headerlink" title="3) 基于 Flink CDC 的聚合分析"></a>3) 基于 Flink CDC 的聚合分析</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_15.png"
                      alt="img"
                ></p>
<h4 id="4-基于-Flink-CDC-的数据打宽"><a href="#4-基于-Flink-CDC-的数据打宽" class="headerlink" title="4) 基于 Flink CDC 的数据打宽"></a>4) 基于 Flink CDC 的数据打宽</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210904_16.png"
                      alt="img"
                ></p>
<h3 id="10-4-Flink-CDC-案例"><a href="#10-4-Flink-CDC-案例" class="headerlink" title="10.4  Flink-CDC 案例"></a>10.4  Flink-CDC 案例</h3><p>Flink 社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、PostgreSQL 等数据库直接读取全量数据和增量变更数据的 source 组件。</p>
<p>开源地址：<a class="link"   target="_blank" rel="noopener" href="https://github.com/ververica/flink-cdc-connectors%E3%80%82" >https://github.com/ververica/flink-cdc-connectors。 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p><strong>示例代码</strong>：</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.ververica.cdc.connectors.mysql.<span class="type">MySQLSource</span>;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.ververica.cdc.debezium.<span class="type">DebeziumSourceFunction</span>;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.ververica.cdc.debezium.<span class="type">StringDebeziumDeserializationSchema</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.restartstrategy.<span class="type">RestartStrategies</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.filesystem.<span class="type">FsStateBackend</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.<span class="type">DataStreamSource</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">StreamExecutionEnvironment</span>;</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">FlinkCDC</span> </span>&#123;</span><br><span class="line"> public static void main(<span class="type">String</span>[] args) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line"> <span class="comment">//1.创建执行环境</span></span><br><span class="line"> <span class="type">StreamExecutionEnvironment</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment();</span><br><span class="line"> env.setParallelism(<span class="number">1</span>);</span><br><span class="line"> <span class="comment">//2.Flink-CDC 将读取 binlog 的位置信息以状态的方式保存在 CK,如果想要做到断点续传,需要从 Checkpoint 或者 Savepoint 启动程序</span></span><br><span class="line"> <span class="comment">//2.1 开启 Checkpoint,每隔 5 秒钟做一次 CK</span></span><br><span class="line"> env.enableCheckpointing(<span class="number">5000</span>L);</span><br><span class="line"> <span class="comment">//2.2 指定 CK 的一致性语义</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>);</span><br><span class="line"> <span class="comment">//2.3 设置任务关闭的时候保留最后一次 CK 数据</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>);</span><br><span class="line"> <span class="comment">//2.4 指定从 CK 自动重启策略</span></span><br><span class="line"> env.setRestartStrategy(<span class="type">RestartStrategies</span>.fixedDelayRestart(<span class="number">3</span>, <span class="number">2000</span>L));</span><br><span class="line"> <span class="comment">//2.5 设置状态后端</span></span><br><span class="line"> env.setStateBackend(<span class="keyword">new</span> <span class="type">FsStateBackend</span>(<span class="string">&quot;hdfs://hadoop102:8020/flinkCDC&quot;</span>));</span><br><span class="line"> <span class="comment">//2.6 设置访问 HDFS 的用户名</span></span><br><span class="line"> <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;atguigu&quot;</span>);</span><br><span class="line"> <span class="comment">//3.创建 Flink-MySQL-CDC 的 Source</span></span><br><span class="line"> <span class="comment">//initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest binlog.</span></span><br><span class="line"> <span class="comment">//latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the binlog which means only have the changes since the connector was started.</span></span><br><span class="line"> <span class="comment">//timestamp: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified timestamp. The consumer will traverse the binlog from the beginning and ignore change events whose timestamp is smaller than the specified timestamp.</span></span><br><span class="line"> <span class="comment">//specific-offset: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified offset.</span></span><br><span class="line"> <span class="type">DebeziumSourceFunction</span>&lt;<span class="type">String</span>&gt; mysqlSource = <span class="type">MySQLSource</span>.&lt;<span class="type">String</span>&gt;builder()</span><br><span class="line"> .hostname(<span class="string">&quot;hadoop01&quot;</span>)</span><br><span class="line"> .port(<span class="number">3306</span>)</span><br><span class="line"> .username(<span class="string">&quot;root&quot;</span>)</span><br><span class="line"> .password(<span class="string">&quot;000000&quot;</span>)</span><br><span class="line"> .databaseList(<span class="string">&quot;gmall-flink&quot;</span>)</span><br><span class="line"> .tableList(<span class="string">&quot;gmall-flink.z_user_info&quot;</span>) <span class="comment">//可选配置项,如果不指定该参数,则会</span></span><br><span class="line">读取上一个配置下的所有表的数据，注意：指定的时候需要使用<span class="string">&quot;db.table&quot;</span>的方式</span><br><span class="line"> .startupOptions(<span class="type">StartupOptions</span>.initial())</span><br><span class="line"> .deserializer(<span class="keyword">new</span> <span class="type">StringDebeziumDeserializationSchema</span>())</span><br><span class="line"> .build();</span><br><span class="line"> <span class="comment">//4.使用 CDC Source 从 MySQL 读取数据</span></span><br><span class="line"> <span class="type">DataStreamSource</span>&lt;<span class="type">String</span>&gt; mysqlDS = env.addSource(mysqlSource);</span><br><span class="line"> <span class="comment">//5.打印数据</span></span><br><span class="line"> mysqlDS.print();</span><br><span class="line"> <span class="comment">//6.执行任务</span></span><br><span class="line"> env.execute();</span><br><span class="line"> &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="10-5-Flink-SQL-方式的案例"><a href="#10-5-Flink-SQL-方式的案例" class="headerlink" title="10.5  Flink SQL 方式的案例"></a>10.5  Flink SQL 方式的案例</h3><div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.restartstrategy.<span class="type">RestartStrategies</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.filesystem.<span class="type">FsStateBackend</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">StreamExecutionEnvironment</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.<span class="type">StreamTableEnvironment</span>;</span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">FlinkSQL_CDC</span> </span>&#123;</span><br><span class="line"> public static void main(<span class="type">String</span>[] args) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line"> <span class="comment">//1.创建执行环境</span></span><br><span class="line"> <span class="type">StreamExecutionEnvironment</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment();</span><br><span class="line"> env.setParallelism(<span class="number">1</span>);</span><br><span class="line"> <span class="type">StreamTableEnvironment</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(env);</span><br><span class="line"> <span class="comment">//2.创建 Flink-MySQL-CDC 的 Source</span></span><br><span class="line"> tableEnv.executeSql(<span class="string">&quot;CREATE TABLE user_info (&quot;</span> +</span><br><span class="line"> <span class="string">&quot; id INT,&quot;</span> +</span><br><span class="line"> <span class="string">&quot; name STRING,&quot;</span> +</span><br><span class="line"> <span class="string">&quot; phone_num STRING&quot;</span> +</span><br><span class="line"> <span class="string">&quot;) WITH (&quot;</span> +</span><br><span class="line"> <span class="string">&quot; &#x27;connector&#x27; = &#x27;mysql-cdc&#x27;,&quot;</span> +</span><br><span class="line"> <span class="string">&quot; &#x27;hostname&#x27; = &#x27;hadoop01&#x27;,&quot;</span> +</span><br><span class="line"> <span class="string">&quot; &#x27;port&#x27; = &#x27;3306&#x27;,&quot;</span> +</span><br><span class="line"> <span class="string">&quot; &#x27;username&#x27; = &#x27;root&#x27;,&quot;</span> +</span><br><span class="line"> <span class="string">&quot; &#x27;password&#x27; = &#x27;000000&#x27;,&quot;</span> +</span><br><span class="line"> <span class="string">&quot; &#x27;database-name&#x27; = &#x27;gmall-flink&#x27;,&quot;</span> +</span><br><span class="line"> <span class="string">&quot; &#x27;table-name&#x27; = &#x27;z_user_info&#x27;&quot;</span> +</span><br><span class="line"> <span class="string">&quot;)&quot;</span>);</span><br><span class="line"> tableEnv.executeSql(<span class="string">&quot;select * from user_info&quot;</span>).print();</span><br><span class="line"> env.execute();</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="11-Flink面试题"><a href="#11-Flink面试题" class="headerlink" title="11  Flink面试题"></a>11  Flink面试题</h2><h3 id="11-1-Flink-的容错机制（checkpoint）"><a href="#11-1-Flink-的容错机制（checkpoint）" class="headerlink" title="11.1  Flink 的容错机制（checkpoint）"></a>11.1  Flink 的容错机制（checkpoint）</h3><p>Checkpoint机制是Flink可靠性的基石，可以保证Flink集群在某个算子因为某些原因(如 异常退出)出现故障时，能够将整个应用流图的状态恢复到故障之前的某一状态，保证应用流图状态的一致性。Flink的Checkpoint机制原理来自“Chandy-Lamport algorithm”算法。</p>
<p>每个需要Checkpoint的应用在启动时，Flink的JobManager为其创建一个 CheckpointCoordinator(检查点协调器)，CheckpointCoordinator全权负责本应用的快照制作。</p>
<p>**CheckpointCoordinator(检查点协调器)**，CheckpointCoordinator全权负责本应用的快照制作。</p>
<p> <img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210124_1.png"
                      alt="img"
                ></p>
<ol>
<li>CheckpointCoordinator(检查点协调器) 周期性的向该流应用的所有source算子发送 barrier(屏障)。</li>
<li>当某个source算子收到一个barrier时，便暂停数据处理过程，然后将自己的当前状态制作成快照，并保存到指定的持久化存储中，最后向CheckpointCoordinator报告自己快照制作情况，同时向自身所有下游算子广播该barrier，恢复数据处理</li>
<li>下游算子收到barrier之后，会暂停自己的数据处理过程，然后将自身的相关状态制作成快照，并保存到指定的持久化存储中，最后向CheckpointCoordinator报告自身快照情况，同时向自身所有下游算子广播该barrier，恢复数据处理。</li>
<li>每个算子按照步骤3不断制作快照并向下游广播，直到最后barrier传递到sink算子，快照制作完成。</li>
<li>当CheckpointCoordinator收到所有算子的报告之后，认为该周期的快照制作成功; 否则，如果在规定的时间内没有收到所有算子的报告，则认为本周期快照制作失败。</li>
</ol>
<p><strong>文章推荐</strong>：</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/QMTmPeYH-AhWWEnGNSjajQ" >Flink可靠性的基石-checkpoint机制详细解析(opens new window) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="11-2-Flink-Checkpoint与-Spark-的相比，Flink-有什么区别或优势吗"><a href="#11-2-Flink-Checkpoint与-Spark-的相比，Flink-有什么区别或优势吗" class="headerlink" title="11.2  Flink Checkpoint与 Spark 的相比，Flink 有什么区别或优势吗"></a>11.2  Flink Checkpoint与 Spark 的相比，Flink 有什么区别或优势吗</h3><p>Spark Streaming 的 Checkpoint 仅仅是针对 Driver 的故障恢复做了数据和元数据的 Checkpoint。而 Flink 的 Checkpoint 机制要复杂了很多，它采用的是轻量级的分布式快照，实现了每个算子的快照，及流动中的数据的快照。</p>
<h3 id="11-2-1-Flink与spark-streaming的一些主要区别"><a href="#11-2-1-Flink与spark-streaming的一些主要区别" class="headerlink" title="11.2.1  Flink与spark streaming的一些主要区别"></a>11.2.1  Flink与spark streaming的一些主要区别</h3><blockquote>
<p>以下是Flink和Spark Streaming的一些主要区别：</p>
<ol>
<li><strong>数据处理模型</strong></li>
</ol>
<p>Flink采用基于事件驱动的模型来处理数据流，而Spark Streaming则采用微批处理模型。在Flink中，事件被处理为单个记录，可以进行准确的事件时间处理，支持更灵活的窗口操作。而Spark Streaming将流数据划分为一批批微小的数据块，再进行处理。</p>
<ol>
<li><strong>数据处理延迟</strong></li>
</ol>
<p>Flink通常具有更低的处理延迟，这是因为它使用了基于事件时间的处理模型。而Spark Streaming的延迟通常比Flink要高，因为它使用了批处理模型。</p>
<ol>
<li><strong>状态管理</strong></li>
</ol>
<p>Flink支持灵活的状态管理机制，可以在不同的算子之间共享状态。而Spark Streaming则使用DStream来处理数据流，每个DStream都是独立的，因此状态管理相对较为简单。</p>
<ol>
<li><strong>可靠性</strong></li>
</ol>
<p>Flink对于数据丢失或失败有更好的容错处理能力，它可以在数据丢失时进行重播或回溯。而Spark Streaming的容错机制相对较为简单。</p>
<ol>
<li><strong>生态系统</strong></li>
</ol>
<p>Spark Streaming作为Apache Spark的一部分，具有更广泛的生态系统和更丰富的API。Flink生态系统相对较小，但也有越来越多的应用场景和开源项目。然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能 会重复处理，不能做到恰好一次处理语义。</p>
<ol>
<li><strong>时间机制：</strong><ul>
<li>Spark Streaming 支持的时间机制有限，只支持处理时间。</li>
<li>Flink 支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持 watermark 机 制来处理滞后数据。</li>
</ul>
</li>
</ol>
<p>综上所述，Flink和Spark Streaming都有自己的优缺点，选择哪种框架取决于具体的需求和场景。如果需要更高的处理延迟、更灵活的状态管理和更好的容错处理能力，可以考虑选择Flink；如果需要更广泛的生态系统和更丰富的API，可以选择Spark Streaming。</p>
</blockquote>
<h3 id="11-3-Flink-中的-Time-有哪几种"><a href="#11-3-Flink-中的-Time-有哪几种" class="headerlink" title="11.3  Flink 中的 Time 有哪几种"></a>11.3  Flink 中的 Time 有哪几种</h3><p>Flink中的时间有三种类型，如下图所示：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/210123_1.png"
                      alt="img"
                ></p>
<ul>
<li><strong>Event Time</strong>：是事件创建的时间。它通常由事件中的时间戳描述，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink通过时间戳分配器访问事件时间戳。</li>
<li><strong>Ingestion Time</strong>：是数据进入Flink的时间。</li>
<li><strong>Processing Time</strong>：是每一个执行基于时间操作的算子的本地系统时间，与机器相关，默认的时间属性就是Processing Time。</li>
</ul>
<p>例如，一条日志进入Flink的时间为<code>2021-01-22 10:00:00.123</code>，到达Window的系统时间为<code>2021-01-22 10:00:01.234</code>，日志的内容如下：<code>2021-01-06 18:37:15.624 INFO Fail over to rm2</code></p>
<p>对于业务来说，要统计1min内的故障日志个数，哪个时间是最有意义的？—— eventTime，因为我们要根据日志的生成时间进行统计。</p>
<h3 id="11-4-对于迟到数据是怎么处理的"><a href="#11-4-对于迟到数据是怎么处理的" class="headerlink" title="11.4  对于迟到数据是怎么处理的"></a>11.4  对于迟到数据是怎么处理的</h3><p>Flink中 WaterMark 和 Window 机制解决了流式数据的乱序问题，对于因为延迟而顺序有误的数据，可以根据eventTime进行业务处理，对于延迟的数据Flink也有自己的解决办法，主要的办法是给定一个允许延迟的时间，在该时间范围内仍可以接受处理延迟数据：</p>
<ul>
<li>设置允许延迟的时间是通过allowedLateness(lateness: Time)设置</li>
<li>保存延迟数据则是通过sideOutputLateData(outputTag: OutputTag[T])保存</li>
<li>获取延迟数据是通过DataStream.getSideOutput(tag: OutputTag[X])获取</li>
</ul>
<p><strong>文章推荐</strong>：</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/S-RmP5OWiGqwn-C_TZNO5A" >Flink 中极其重要的 Time 与 Window 详细解析(opens new window) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="11-5-Flink-的运行必须依赖-Hadoop-组件吗"><a href="#11-5-Flink-的运行必须依赖-Hadoop-组件吗" class="headerlink" title="11.5 Flink 的运行必须依赖 Hadoop 组件吗"></a>11.5 Flink 的运行必须依赖 Hadoop 组件吗</h3><p>Flink可以完全独立于Hadoop，在不依赖Hadoop组件下运行。但是做为大数据的基础设施，Hadoop体系是任何大数据框架都绕不过去的。Flink可以集成众多Hadooop 组件，例如Yarn、Hbase、HDFS等等。例如，Flink可以和Yarn集成做资源调度，也可以读写HDFS，或者利用HDFS做检查点。</p>
<h3 id="11-6-Flink集群有哪些角色？各自有什么作用"><a href="#11-6-Flink集群有哪些角色？各自有什么作用" class="headerlink" title="11.6  Flink集群有哪些角色？各自有什么作用"></a>11.6  Flink集群有哪些角色？各自有什么作用</h3><p>有以下三个角色：</p>
<p><strong>JobManager处理器：</strong></p>
<p>也称之为Master，用于协调分布式执行，它们用来调度task，协调检查点，协调失败时恢复等。Flink运行时至少存在一个master处理器，如果配置高可用模式则会存在多个master处理器，它们其中有一个是leader，而其他的都是standby。</p>
<p><strong>TaskManager处理器：</strong></p>
<p>也称之为Worker，用于执行一个dataflow的task(或者特殊的subtask)、数据缓冲和data stream的交换，Flink运行时至少会存在一个worker处理器。</p>
<p><strong>Clint客户端：</strong></p>
<p>Client是Flink程序提交的客户端，当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager</p>
<h3 id="11-7-Flink-资源管理中-Task-Slot-的概念"><a href="#11-7-Flink-资源管理中-Task-Slot-的概念" class="headerlink" title="11.7  Flink 资源管理中 Task Slot 的概念"></a>11.7  Flink 资源管理中 Task Slot 的概念</h3><p>在Flink中每个TaskManager是一个JVM的进程, 可以在不同的线程中执行一个或多个子任务。 为了控制一个worker能接收多少个task。worker通过task slot（任务槽）来进行控制（一个worker至少有一个task slot）。</p>
<h3 id="11-8-Flink的重启策略了解吗"><a href="#11-8-Flink的重启策略了解吗" class="headerlink" title="11.8  Flink的重启策略了解吗"></a>11.8  Flink的重启策略了解吗</h3><p>Flink支持不同的重启策略，这些重启策略控制着job失败后如何重启：</p>
<ol>
<li><strong>固定延迟重启策略</strong></li>
</ol>
<p>固定延迟重启策略会尝试一个给定的次数来重启Job，如果超过了最大的重启次数，Job最终将失败。在连续的两次重启尝试之间，重启策略会等待一个固定的时间。</p>
<ol>
<li><strong>失败率重启策略</strong></li>
</ol>
<p>失败率重启策略在Job失败后会重启，但是超过失败率后，Job会最终被认定失败。在两个连续的重启尝试之间，重启策略会等待一个固定的时间。</p>
<ol>
<li><strong>无重启策略</strong></li>
</ol>
<p>Job直接失败，不会尝试进行重启。</p>
<h3 id="11-9-Flink-是如何保证-Exactly-once-语义的"><a href="#11-9-Flink-是如何保证-Exactly-once-语义的" class="headerlink" title="11.9  Flink 是如何保证 Exactly-once 语义的"></a>11.9  Flink 是如何保证 Exactly-once 语义的</h3><p>Flink通过实现<strong>两阶段提交</strong>和状态保存来实现端到端的一致性语义。分为以下几个步骤：</p>
<p>开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面</p>
<p>预提交（preCommit）将内存中缓存的数据写入文件并关闭</p>
<p>正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟</p>
<p>丢弃（abort）丢弃临时文件</p>
<p>若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。</p>
<p><strong>文章推荐</strong>：</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/WH5KZrh8YMbfFn6GyYUXVA" >八张图搞懂 Flink 端到端精准一次处理语义 Exactly-once(opens new window) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="11-10-如果下级存储不支持事务，Flink-怎么保证-exactly-once"><a href="#11-10-如果下级存储不支持事务，Flink-怎么保证-exactly-once" class="headerlink" title="11.10  如果下级存储不支持事务，Flink 怎么保证 exactly-once"></a>11.10  如果下级存储不支持事务，Flink 怎么保证 exactly-once</h3><p>端到端的 exactly-once 对 sink 要求比较高，具体实现主要有<strong>幂等写入</strong>和<strong>事务性写入</strong>两种方式。</p>
<p>幂等写入的场景依赖于业务逻辑，更常见的是用事务性写入。而事务性写入又有预写日志（WAL）和两阶段提交（2PC）两种方式。</p>
<p>如果外部系统不支持事务，那么可以用预写日志的方式，把结果数据先当成状态保存，然后在收到 checkpoint 完成的通知时，一次性写入 sink 系统。</p>
<h3 id="11-11-Flink是如何处理反压的"><a href="#11-11-Flink是如何处理反压的" class="headerlink" title="11.11  Flink是如何处理反压的"></a>11.11  Flink是如何处理反压的</h3><p>Flink 内部是基于 producer-consumer 模型来进行消息传递的，Flink的反压设计也是基于这个模型。Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。下游消费者消费变慢，上游就会受到阻塞。</p>
<h3 id="11-12-Flink中的状态存储"><a href="#11-12-Flink中的状态存储" class="headerlink" title="11.12  Flink中的状态存储"></a>11.12  Flink中的状态存储</h3><p>Flink在做计算的过程中经常需要存储中间状态，来避免数据丢失和状态恢复。选择的状态存储策略不同，会影响状态持久化如何和 checkpoint 交互。Flink提供了三种状态存储方式：<strong>MemoryStateBackend、FsStateBackend、RocksDBStateBackend</strong>。</p>
<h3 id="11-13-Flink是如何支持流批一体的"><a href="#11-13-Flink是如何支持流批一体的" class="headerlink" title="11.13  Flink是如何支持流批一体的"></a>11.13  Flink是如何支持流批一体的</h3><p>这道题问的比较开阔，如果知道Flink底层原理，可以详细说说，如果不是很了解，就直接简单一句话：<strong>Flink的开发者认为批处理是流处理的一种特殊情况。批处理是有限的流处理。Flink 使用一个引擎支持了 DataSet API 和 DataStream API</strong>。</p>
<h3 id="11-14-Flink的内存管理是如何做的"><a href="#11-14-Flink的内存管理是如何做的" class="headerlink" title="11.14  Flink的内存管理是如何做的"></a>11.14  Flink的内存管理是如何做的</h3><p>Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上。此外，Flink大量的使用了堆外内存。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。Flink 为了直接操作二进制数据实现了自己的序列化框架。</p>
<h3 id="11-15-Flink-CEP-编程中当状态没有到达的时候会将数据保存在哪里"><a href="#11-15-Flink-CEP-编程中当状态没有到达的时候会将数据保存在哪里" class="headerlink" title="11.15  Flink CEP 编程中当状态没有到达的时候会将数据保存在哪里"></a>11.15  Flink CEP 编程中当状态没有到达的时候会将数据保存在哪里</h3><p>在流式处理中，CEP 当然是要支持 EventTime 的，那么相对应的也要支持数据的迟到现象，也就是watermark的处理逻辑。CEP对未匹配成功的事件序列的处理，和迟到数据是类似的。在 Flink CEP的处理逻辑中，状态没有满足的和迟到的数据，都会存储在一个Map数据结构中，也就是说，如果我们限定判断事件序列的时长为5分钟，那么内存中就会存储5分钟的数据，这在我看来，也是对内存的极大损伤之一。</p>
<h2 id="12-Flink面试题附加"><a href="#12-Flink面试题附加" class="headerlink" title="12  Flink面试题附加"></a>12  Flink面试题附加</h2><h3 id="12-1-讲⼀下Flink的运⾏架构"><a href="#12-1-讲⼀下Flink的运⾏架构" class="headerlink" title="12.1  讲⼀下Flink的运⾏架构"></a>12.1  讲⼀下Flink的运⾏架构</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230305171750304.png"
                      alt="image-20230305171750304"
                ></p>
<p>当 Flink 集群启动后，⾸先会启动⼀个 JobManger 和⼀个或多个的 TaskManager。由 Client 提交任务给JobManager，JobManager 再调度任务到各个 TaskManager 去执⾏，然后 TaskManager 将⼼跳和统计信息汇报给 JobManager。<strong>TaskManager 之间以流的形式进⾏数据的传输</strong>。上述三者均为独⽴的 JVM 进程。</p>
<ul>
<li>Client 为提交 Job 的客户端，可以是运⾏在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。</li>
<li>JobManager 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会⽣成优化后的执⾏计划，并以 Task 的单元调度到各个 TaskManager 去执⾏。</li>
<li>TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动⼀个 Task，Task 为线程。从JobManager 处接收需要部署的 Task，部署启动后，与⾃⼰的上游建⽴ Netty 连接，接收数据并处理。</li>
</ul>
<h3 id="12-2-Flink的作业执行流程"><a href="#12-2-Flink的作业执行流程" class="headerlink" title="12.2  Flink的作业执行流程"></a>12.2  Flink的作业执行流程</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230305172021794.png"
                      alt="image-20230305172021794"
                ></p>
<p>以yarn模式Per-job⽅式为例概述作业提交执⾏流程</p>
<ol>
<li>当执⾏executor() 之后,会⾸先在本地client 中将代码转化为可以提交的 JobGraph如果提交为Per-Job模式,则⾸先需要启动AM, client会⾸先向资源系统申请资源, 在yarn下即为申请container开启AM, 如果是Session模式的话则不需要这个步骤</li>
<li>Yarn分配资源, 开启AM</li>
<li>Client将Job提交给Dispatcher</li>
<li>Dispatcher 会开启⼀个新的 JobManager线程</li>
<li>JM 向Flink ⾃⼰的 Resourcemanager申请slot资源来执⾏任务</li>
<li>RM 向 Yarn申请资源来启动 TaskManger (Session模式跳过此步)</li>
<li>Yarn 分配 Container 来启动 taskManger (Session模式跳过此步)</li>
<li>Flink 的 RM 向 TM 申请 slot资源来启动 task</li>
<li>TM 将待分配的 slot 提供给 JM</li>
<li>JM 提交 task, TM 会启动新的线程来执⾏任务,开始启动后就可以通过 shuffle模块进⾏ task之间的数据交换</li>
</ol>
<h3 id="12-3-Flink的部署模式都有哪些？"><a href="#12-3-Flink的部署模式都有哪些？" class="headerlink" title="12.3  Flink的部署模式都有哪些？"></a>12.3  Flink的部署模式都有哪些？</h3><p>flink可以以多种⽅式部署,包括standlone模式&#x2F;yarn&#x2F;Mesos&#x2F;Kubernetes&#x2F;Docker&#x2F;AWS&#x2F;Google Compute  Engine&#x2F;MAPR等<br>⼀般公司中主要采⽤ on yarn模式</p>
<h3 id="12-4-讲一下Flink-on-yarn的部署"><a href="#12-4-讲一下Flink-on-yarn的部署" class="headerlink" title="12.4  讲一下Flink  on yarn的部署"></a>12.4  讲一下Flink  on yarn的部署</h3><p>Flink作业提交有两种类型:</p>
<ol>
<li><strong>yarn session</strong></li>
</ol>
<p>需要先启动集群，然后在提交作业，接着会向yarn申请⼀块空间后，资源永远保持不变。如果资源满了，下⼀个作业就⽆法提交，只能等到yarn中的其中⼀个作业执⾏完成后，释放了资源，那下⼀个作业才会正常提交:</p>
<ul>
<li>客户端模式<br>对于客户端模式⽽⾔，你可以启动多个yarn session，⼀个yarn session模式对应⼀个JobManager,并按照需求提交作业，同⼀个Session中可以提交多个Flink作业。如果想要停⽌Flink Yarn Application，需要通过yarn application -kill命令来停⽌.</li>
<li>分离式模式<br>对于分离式模式，并不像客户端那样可以启动多个yarn session，如果启动多个，会出现下⾯的session⼀直处在等待状态。JobManager的个数只能是⼀个，同⼀个Session中可以提交多个Flink作业。如果想要停⽌Flink Yarn Application，需要通过yarn application -kill命令来停⽌</li>
</ul>
<ol>
<li><strong>Flink run(Per-Job)</strong></li>
</ol>
<p>直接在YARN上提交运⾏Flink作业(Run a Flink job on YARN)，这种⽅式的好处是⼀个任务会对应⼀个job,即没提交⼀个作业会根据⾃身的情况，向yarn申请资源，直到作业执⾏完成，并不会影响下⼀个作业的正常运⾏，除⾮是yarn上⾯没有任何资源的情况下</p>
<table>
<thead>
<tr>
<th>Session</th>
<th>Per-Job</th>
</tr>
</thead>
<tbody><tr>
<td>共享Dispatcher和Resource Manager</td>
<td>Dispatcher和Resource Manager</td>
</tr>
<tr>
<td>共享资源(即 TaskExecutor)</td>
<td>按需要申请资源 (即 TaskExecutor)</td>
</tr>
<tr>
<td>适合规模⼩,执⾏时间短的作业</td>
<td></td>
</tr>
</tbody></table>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230305173305774.png"
                      alt="image-20230305173305774"
                ></p>
<h3 id="12-5-Flink的state是存储在哪里的"><a href="#12-5-Flink的state是存储在哪里的" class="headerlink" title="12.5  Flink的state是存储在哪里的"></a>12.5  Flink的state是存储在哪里的</h3><p>Apache Flink内部有四种state的存储实现，具体如下：</p>
<ul>
<li>基于内存的HeapStateBackend - 在debug模式使⽤，不 建议在⽣产模式下应⽤；</li>
<li>基于HDFS的FsStateBackend - 分布式⽂件持久化，每次读写都产⽣⽹络IO，整体性能不佳；</li>
<li>基于RocksDB的RocksDBStateBackend - 本地⽂件+异步HDFS持久化；</li>
<li>基于Niagara(Alibaba内部实现)NiagaraStateBackend - 分布式持久化- 在Alibaba⽣产环境应⽤；</li>
</ul>
<h3 id="12-6-Flink的window分类"><a href="#12-6-Flink的window分类" class="headerlink" title="12.6  Flink的window分类"></a>12.6  Flink的window分类</h3><p>flink中的窗⼝主要分为3⼤类共5种窗⼝:</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230305173703355.png"
                      alt="image-20230305173703355"
                ></p>
<ul>
<li><p><strong>Time Window 时间窗⼝</strong></p>
<ul>
<li><p><strong>Tumbing Time Window 滚动时间窗⼝：</strong>实现统计每⼀分钟(或其他⻓度)窗⼝内 计算的效果</p>
</li>
<li><p><strong>Sliding Time Window 滑动时间窗⼝：</strong>实现每过xxx时间 统计 xxx时间窗⼝的效果. ⽐如，我们可以每30秒计算⼀次最近⼀分钟⽤户购买的商品总数。</p>
</li>
</ul>
</li>
<li><p><strong>Count Window 计数窗⼝</strong></p>
<ul>
<li><p><strong>Tumbing Count Window 滚动计数窗⼝：</strong>当我们想要每100个⽤户购买⾏为事件统计购买总数，那么每当窗⼝中填满100个元素了，就会对窗⼝进⾏计算，这种窗⼝我们称之为翻滚计数窗⼝（Tumbling Count Window）</p>
</li>
<li><p><strong>Sliding Count Window 滑动计数窗⼝：</strong>和Sliding Time Window含义是类似的，例如计算每10个元素计算⼀次最近100个元素的总和</p>
</li>
</ul>
</li>
<li><p><strong>Session Window 会话窗⼝</strong></p>
</li>
</ul>
<p>在这种⽤户交互事件流中，我们⾸先想到的是将事件聚合到会话窗⼝中（⼀段⽤户持续活跃的周期），由⾮活跃的间隙分隔开。如上图所示，就是需要计算每个⽤户在活跃期间总共购买的商品数量，如果⽤户30秒没有活动则视为会话断开（假设raw data stream是单个⽤户的购买⾏为流）</p>
<h3 id="12-7-Flink的window实现机制"><a href="#12-7-Flink的window实现机制" class="headerlink" title="12.7  Flink的window实现机制"></a>12.7  Flink的window实现机制</h3><p>Flink 中定义⼀个窗⼝主要需要以下三个组件。</p>
<ul>
<li><strong>Window Assigner：</strong>⽤来决定某个元素被分配到哪个&#x2F;哪些窗⼝中去。</li>
<li><strong>Trigger：</strong>触发器。决定了⼀个窗⼝何时能够被计算或清除，每个窗⼝都会拥有⼀个⾃⼰的Trigger。</li>
<li><strong>Evictor：</strong>可以译为“驱逐者”。在Trigger触发之后，在窗⼝被处理之前，Evictor（如果有Evictor的话）会⽤来剔除窗⼝中不需要的元素，相当于⼀个filter。</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230305174135832.png"
                      alt="image-20230305174135832"
                ></p>
<p>⾸先上图中的组件都位于⼀个算⼦（window operator）中，数据流源源不断地进⼊算⼦，每⼀个到达的元素都会被交给 Window Assigner。Window Assigner 会决定元素被放到哪个或哪些窗⼝（window），可能会创建新窗⼝。因为⼀个元素可以被放⼊多个窗⼝中，所以同时存在多个窗⼝是可能的。注意， Window 本身只是⼀个ID标识符，其内部可能存储了⼀些元数据，如TimeWindow 中有开始和结束时间，但是并不会存储窗⼝中的元素。窗⼝中的元素实际存储在 Key&#x2F;Value State 中，key为Window ，value为元素集合（或聚合值）。为了保证窗⼝的容错性，该实现依赖了 Flink 的 State 机制（<a class="link"   target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state/" >参⻅ state ⽂档 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）。</p>
<p>每⼀个窗⼝都拥有⼀个属于⾃⼰的 Trigger，Trigger上会有定时器，⽤来决定⼀个窗⼝何时能够被计算或清除。每当有元素加⼊到该窗⼝，或者之前注册的定时器超时了，那么Trigger都会被调⽤。Trigger的返回结果可以是continue（不做任何操作），fire（处理窗⼝数据），purge（移除窗⼝和窗⼝中的数据），或者 fire + purge。⼀个Trigger的调⽤结果只是fire的话，那么会计算窗⼝并保留窗⼝原样，也就是说窗⼝中的数据仍然保留不变，等待下次Trigger fire的时候再次执⾏计算。⼀个窗⼝可以被重复计算多次知道它被 purge 了。在purge之前，窗⼝会⼀直占⽤着内存。</p>
<p>当Trigger fire了，窗⼝中的元素集合就会交给Evictor （如果指定了的话）。Evictor 主要⽤来遍历窗⼝中的元素列表，并决定最先进⼊窗⼝的多少个元素需要被移除。剩余的元素会交给⽤户指定的函数进⾏窗⼝的计算。如果没有 Evictor 的话，窗⼝中的所有元素会⼀起交给函数进⾏计算。</p>
<p>计算函数收到了窗⼝的元素（可能经过了 Evictor 的过滤），并计算出窗⼝的结果值，并发送给下游。窗⼝的结果值可以是⼀个也可以是多个。DataStream API 上可以接收不同类型的计算函数，包括预定义的sum() , min() , max() ，还有 ReduceFunction ， FoldFunction ，还有WindowFunction 。WindowFunction是最通⽤的计算函数，其他的预定义的函数基本都是基于该函数实现的。</p>
<p>Flink 对于⼀些聚合类的窗⼝计算（如sum,min）做了优化，因为聚合类的计算不需要将窗⼝中的所有数据都保存下来，只需要保存⼀个result值就可以了。每个进⼊窗⼝的元素都会执⾏⼀次聚合函数并修改result值。这样可以⼤⼤降低内存的消耗并提升性能。但是如果⽤户定义了 Evictor，则不会启⽤对聚合窗⼝的优化，因为 Evictor 需要遍历窗⼝中的所有元素，必须要将窗⼝中所有元素都存下来。</p>
<h3 id="12-8-Flink具体是如何时效内excatly-once语义的"><a href="#12-8-Flink具体是如何时效内excatly-once语义的" class="headerlink" title="12.8  Flink具体是如何时效内excatly  once语义的"></a>12.8  Flink具体是如何时效内excatly  once语义的</h3><p>在谈到 flink 所实现的 exactly-once语义时,主要是2个层⾯上的,⾸先 flink在0.9版本以后已经实现了基于state的内部⼀致性语义, 在1.4版本以后也可以实现端到端 Exactly-Once语义</p>
<ul>
<li><strong>状态 Exactly-Once</strong></li>
</ul>
<p>Flink 提供 exactly-once 的状态（state）投递语义，这为有状态的（stateful）计算提供了准确性保证。也就是状态是不会重复使⽤的,有且仅有⼀次消费</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230305175120706.png"
                      alt="image-20230305175120706"
                ></p>
<p>这⾥需要注意的⼀点是如何理解state语义的exactly-once,并不是说在flink中的所有事件均只会处理⼀次,⽽是所有的事件所影响⽣成的state只有作⽤⼀次.</p>
<p>在上图中, 假设每两条消息后出发⼀次checkPoint操作,持久化⼀次state. TaskManager 在 处理完 event c 之后被shutdown, 这时候当 JobManager重启task之后, TaskManager 会从 checkpoint 1 处恢复状态,重新执⾏流处理,也就是说 此时 event c 事件 的的确确是会被再⼀次处理的. 那么 这⾥所说的⼀致性语义是何意思呢? 本身,flink每处理完⼀条数据都会记录当前进度到 state中, 也就是说在 故障前, 处理完 event c 这件事情已经记录到了state中,但是,由于在checkPoint 2 之前, 就已经发⽣了宕机,那么 event c 对于state的影响并没有被记录下来,对于整个flink内部系统来说就好像没有发⽣过⼀样, 在 故障恢复后, 当触发 checkpoint 2 时, event c 的 state才最终被保存下来. 所以说,可以这样理解, 进⼊flink 系统中的 事件 永远只会被 ⼀次state记录并checkpoint下来,⽽state是永远不会发⽣重复被消费的, 这也就是 flink内部的⼀致性语义,就叫做 状态 Exactly once.</p>
<ul>
<li><strong>端到端（end-to-end）Exactly-Once</strong></li>
</ul>
<p>2017年12⽉份发布的Apache Flink 1.4版本，引进了⼀个重要的特性：TwoPhaseCommitSinkFunction.，它抽取了两阶段提交协议的公共部分，使得构建端到端Excatly-Once的Flink程序变为了可能。这些外部系统包括Kafka0.11及以上的版本，以及⼀些其他的数据输⼊（data sources）和数据接收(data sink)。它提供了⼀个抽象层，需要⽤户⾃⼰⼿动去实现Exactly-Once语义.</p>
<p>为了提供端到端Exactly-Once语义，除了Flink应⽤程序本身的状态，Flink写⼊的外部存储也需要满⾜这个语义。也就是说，这些外部系统必须提供提交或者回滚的⽅法，然后通过Flink的checkpoint来协调</p>
<h3 id="12-9-Flink是如何实现反压的"><a href="#12-9-Flink是如何实现反压的" class="headerlink" title="12.9  Flink是如何实现反压的"></a>12.9  Flink是如何实现反压的</h3><p>flink的反压经历了两个发展阶段,分别是基于TCP的反压(&lt;1.5)和基于credit的反压(&gt;1.5)</p>
<ul>
<li><strong>基于 TCP 的反压</strong></li>
</ul>
<p>flink中的消息发送通过RS(ResultPartition),消息接收通过IC(InputGate),两者的数据都是以 LocalBufferPool的形式来存储和提取,进⼀步的依托于Netty的NetworkBufferPool,之后更底层的便是依托于TCP的滑动窗⼝机制,当IC端的buffer池满了之后,两个task之间的滑动窗⼝⼤⼩便为0,此时RS端便⽆法再发送数据</p>
<blockquote>
<p>基于TCP的反压最⼤的问题是会造成整个TaskManager端的反压,所有的task都会受到影响</p>
</blockquote>
<ul>
<li><strong>基于 Credit 的反压(基于信任度机制?(见2.6.1))</strong></li>
</ul>
<p>RS与IC之间通过backlog和credit来确定双⽅可以发送和接受的数据量的⼤⼩以提前感知,⽽不是通过TCP滑动窗⼝的形式来确定buffer的⼤⼩之后再进⾏反压</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://typora-markilue.oss-cn-chengdu.aliyuncs.com/img/image-20230305175519916.png"
                      alt="image-20230305175519916"
                ></p>
<h3 id="12-10-flink中的时间概念-eventTime-和-processTime的区别"><a href="#12-10-flink中的时间概念-eventTime-和-processTime的区别" class="headerlink" title="12.10  flink中的时间概念 , eventTime 和 processTime的区别"></a>12.10  flink中的时间概念 , eventTime 和 processTime的区别</h3><p>Flink中有三种时间概念,分别是 Processing Time、Event Time 和 Ingestion Time</p>
<ul>
<li><strong>Processing Time</strong></li>
</ul>
<p>Processing Time 是指事件被处理时机器的系统时间。<br>当流程序在 Processing Time 上运⾏时，所有基于时间的操作(如时间窗⼝)将使⽤当时机器的系统时间。每⼩时 Processing Time 窗⼝将包括在系统时钟指示整个⼩时之间到达特定操作的所有事件</p>
<ul>
<li><strong>Event Time</strong></li>
</ul>
<p>Event Time 是事件发⽣的时间，⼀般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，⽽跟其他没什么关系。Event Time 程序必须指定如何⽣成 Event Time ⽔印，这是表示 Event Time 进度的机制</p>
<ul>
<li><strong>Ingestion Time</strong></li>
</ul>
<p>Ingestion Time 是事件进⼊ Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗⼝）会利⽤这个时间戳</p>
<p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相⽐，它稍微贵⼀些，但结果更可预测。因为 Ingestion Time 使⽤稳定的时间戳（在源处分配⼀次），所以对事件的不同窗⼝操作将引⽤相同的时间戳，⽽在 Processing Time 中，每个窗⼝操作符可以将事件分配给不同的窗⼝（基于机器系统时间和到达延迟）<br>与 Event Time 相⽐，Ingestion Time 程序⽆法处理任何⽆序事件或延迟数据，但程序不必指定如何⽣成⽔印</p>
<h3 id="12-11-flink的session-window怎么使用"><a href="#12-11-flink的session-window怎么使用" class="headerlink" title="12.11  flink的session  window怎么使用"></a>12.11  flink的session  window怎么使用</h3><p>会话窗⼝主要是将某段时间内活跃度较⾼的数据聚合成⼀个窗⼝进⾏计算,窗⼝的触发条件是 Session Gap, 是指在规定的时间内如果没有数据活跃接⼊,则认为窗⼝结束,然后触发窗⼝结果</p>
<p>Session Windows窗⼝类型⽐较适合⾮连续性数据处理或周期性产⽣数据的场景,根据⽤户在线上某段时间内的活跃度对⽤户⾏为进⾏数据统计</p>
<div class="highlight-container" data-rel="Scala"><figure class="iseeu highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sessionWindowStream = inputStream</span><br><span class="line">.keyBy(_.id)</span><br><span class="line"><span class="comment">//使⽤EventTimeSessionWindow 定义 Event Time 滚动窗⼝</span></span><br><span class="line">.window(<span class="type">EventTimeSessionWindow</span>.withGap(<span class="type">Time</span>.milliseconds(<span class="number">10</span>)))</span><br><span class="line">.process(......)</span><br></pre></td></tr></table></figure></div>

<p>Session Window 本质上没有固定的起⽌时间点,因此底层计算逻辑和Tumbling窗⼝及Sliding 窗⼝有⼀定的区别。</p>
<p>Session Window 为每个进⼊的数据都创建了⼀个窗⼝,最后再将距离窗⼝Session Gap 最近的窗⼝进⾏合并,然后计算窗⼝结果。</p>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> Flink</li>
        <li><strong>Author:</strong> Markilue</li>
        <li><strong>Created at:</strong> 2023-05-27 00:00:00</li>
        
            <li>
                <strong>Updated at:</strong> 2023-05-27 11:45:12
            </li>
        
        <li>
            <strong>Link:</strong> https://redefine.ohevan.com/2023/05/27/Flink/
        </li>
        <li>
            <strong>License:</strong> This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a>.
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">#大数据</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/%E6%B5%81%E5%A4%84%E7%90%86/">#流处理</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/">#实时数仓</a>&nbsp;
                        </li>
                    
                </ul>
            

            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2023/05/27/hello-world/"
                            >
                                <span class="left arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">Hello World</span>
                                    <span class="post-nav-item">Prev posts</span>
                                </span>
                            </a>
                        </div>
                    
                    
                </div>
            


            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;Comments
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-pjax>
        import { init } from 'https://evan.beee.top/js/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">Flink</div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink"><span class="nav-text">Flink</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#0-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93"><span class="nav-text">0  知识点总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Flink%E7%AE%80%E4%BB%8B"><span class="nav-text">1  Flink简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Flink%E7%89%B9%E7%82%B9"><span class="nav-text">1.1  Flink特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Flink%E5%9F%BA%E7%9F%B3"><span class="nav-text">1.2  Flink基石</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E6%89%B9%E5%A4%84%E7%90%86%E5%92%8C%E6%B5%81%E5%A4%84%E7%90%86"><span class="nav-text">1.3  批处理和流处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Flink%E8%BF%90%E8%A1%8C%E6%A1%86%E6%9E%B6"><span class="nav-text">2  Flink运行框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Flink%E7%A8%8B%E5%BA%8F%E6%9E%B6%E6%9E%84"><span class="nav-text">2.1  Flink程序架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Flink-%E5%B9%B6%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%B5%81"><span class="nav-text">2.2  Flink 并行数据流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Task-%E5%92%8C-Operator-chain"><span class="nav-text">2.3   Task 和 Operator chain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E4%B8%8E%E6%89%A7%E8%A1%8C"><span class="nav-text">2.4   任务调度与执行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-%E4%BB%BB%E5%8A%A1%E6%A7%BD%E5%92%8C%E6%A7%BD%E5%85%B1%E4%BA%AB"><span class="nav-text">2.5   任务槽和槽共享</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93"><span class="nav-text">2.6  Flink的数据传输</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-DataSet%E7%AE%97%E5%AD%90%E5%A4%A7%E5%85%A8"><span class="nav-text">3  DataSet算子大全</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-source%E7%AE%97%E5%AD%90"><span class="nav-text">3.1  source算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Transform%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="nav-text">3.2  Transform转换算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Sink%E7%AE%97%E5%AD%90"><span class="nav-text">3.3  Sink算子</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-DataStream%E7%AE%97%E5%AD%90%E5%A4%A7%E5%85%A8"><span class="nav-text">4  DataStream算子大全</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Source%E7%AE%97%E5%AD%90"><span class="nav-text">4.1  Source算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Transform%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="nav-text">4.2  Transform转换算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Sink%E7%AE%97%E5%AD%90"><span class="nav-text">4.3  Sink算子</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E6%B5%81%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84Time%E4%B8%8EWindow"><span class="nav-text">5  流处理中的Time与Window</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Time"><span class="nav-text">5.1. Time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Window"><span class="nav-text">5.2. Window</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Window-API"><span class="nav-text">5.3. Window API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-EventTime-%E4%B8%8E-Window"><span class="nav-text">5.4. EventTime 与 Window</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86"><span class="nav-text">6  状态管理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-State-Keyed-State"><span class="nav-text">6.1  State-Keyed State</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-State-Operator-State"><span class="nav-text">6.2   State-Operator State</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-Broadcast-State"><span class="nav-text">6.3  Broadcast State</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-%E8%B0%83%E6%95%B4%E6%9C%89%E7%8A%B6%E6%80%81%E7%AE%97%E5%AD%90%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%BA%A6"><span class="nav-text">6.4  调整有状态算子的并行度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Flink%E7%9A%84%E5%AE%B9%E9%94%99"><span class="nav-text">7  Flink的容错</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-Checkpoint-%E4%BB%8B%E7%BB%8D"><span class="nav-text">7.1  Checkpoint 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-%E4%BB%8E%E4%B8%80%E8%87%B4%E6%80%A7%E6%A3%80%E6%9F%A5%E7%82%B9%E4%B8%AD%E6%81%A2%E5%A4%8D%E7%8A%B6%E6%80%81"><span class="nav-text">7.1  从一致性检查点中恢复状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-Flink%E7%9A%84%E6%A3%80%E6%9F%A5%E7%82%B9%E7%AE%97%E6%B3%95"><span class="nav-text">7.2  Flink的检查点算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-%E6%A3%80%E6%9F%A5%E7%82%B9%E7%9A%84%E6%80%A7%E8%83%BD%E5%BD%B1%E5%93%8D"><span class="nav-text">7.3  检查点的性能影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-%E4%BF%9D%E5%AD%98%E7%82%B9"><span class="nav-text">7.4  保存点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8-%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF"><span class="nav-text">7.2  持久化存储(状态后端)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-0-Flink%E7%9A%84%E5%90%84%E7%BB%84%E4%BB%B6%E6%95%85%E9%9A%9C"><span class="nav-text">7.3.0  Flink的各组件故障</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-Flink-%E7%9A%84%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5"><span class="nav-text">7.3. Flink 的重启策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-checkpoint-%E6%A1%88%E4%BE%8B"><span class="nav-text">7.4  checkpoint 案例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-%E7%AB%AF%E5%AF%B9%E7%AB%AF%E4%BB%85%E5%A4%84%E7%90%86%E4%B8%80%E6%AC%A1%E8%AF%AD%E4%B9%89"><span class="nav-text">7.5   端对端仅处理一次语义</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-Flink-SQL"><span class="nav-text">8  Flink  SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-Flink-SQL-%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90"><span class="nav-text">8.1   Flink SQL 常用算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-Flink-SQL-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B"><span class="nav-text">7.2  Flink SQL 实战案例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-Flink-CEP"><span class="nav-text">9  Flink  CEP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-Flink-CEP-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">9.1  Flink CEP 是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-Flink-CEP-API"><span class="nav-text">9.2 Flink CEP API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-%E7%9B%91%E6%B5%8B%E7%94%A8%E6%88%B7%E5%BC%B9%E5%B9%95%E8%A1%8C%E4%B8%BA%E6%A1%88%E4%BE%8B"><span class="nav-text">9.3   监测用户弹幕行为案例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-4-Flink-CEP-API"><span class="nav-text">9.4  Flink CEP API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-5-Flink-CEP-%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-text">9.5  Flink CEP 的使用场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Flink-CEP-%E7%9A%84%E5%8E%9F%E7%90%86%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D"><span class="nav-text">6. Flink CEP 的原理简单介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E"><span class="nav-text">7. 规则引擎</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-Flink-CDC"><span class="nav-text">10  Flink  CDC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-CDC%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">10.1  CDC是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-CDC-%E7%9A%84%E7%A7%8D%E7%B1%BB"><span class="nav-text">10.2  CDC 的种类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-3-%E4%BC%A0%E7%BB%9FCDC%E4%B8%8EFlink-CDC%E5%AF%B9%E6%AF%94"><span class="nav-text">10.3  传统CDC与Flink CDC对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-4-Flink-CDC-%E6%A1%88%E4%BE%8B"><span class="nav-text">10.4  Flink-CDC 案例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-5-Flink-SQL-%E6%96%B9%E5%BC%8F%E7%9A%84%E6%A1%88%E4%BE%8B"><span class="nav-text">10.5  Flink SQL 方式的案例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-Flink%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="nav-text">11  Flink面试题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-Flink-%E7%9A%84%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%EF%BC%88checkpoint%EF%BC%89"><span class="nav-text">11.1  Flink 的容错机制（checkpoint）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-2-Flink-Checkpoint%E4%B8%8E-Spark-%E7%9A%84%E7%9B%B8%E6%AF%94%EF%BC%8CFlink-%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%E6%88%96%E4%BC%98%E5%8A%BF%E5%90%97"><span class="nav-text">11.2  Flink Checkpoint与 Spark 的相比，Flink 有什么区别或优势吗</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-2-1-Flink%E4%B8%8Espark-streaming%E7%9A%84%E4%B8%80%E4%BA%9B%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB"><span class="nav-text">11.2.1  Flink与spark streaming的一些主要区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-3-Flink-%E4%B8%AD%E7%9A%84-Time-%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8D"><span class="nav-text">11.3  Flink 中的 Time 有哪几种</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-4-%E5%AF%B9%E4%BA%8E%E8%BF%9F%E5%88%B0%E6%95%B0%E6%8D%AE%E6%98%AF%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86%E7%9A%84"><span class="nav-text">11.4  对于迟到数据是怎么处理的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-5-Flink-%E7%9A%84%E8%BF%90%E8%A1%8C%E5%BF%85%E9%A1%BB%E4%BE%9D%E8%B5%96-Hadoop-%E7%BB%84%E4%BB%B6%E5%90%97"><span class="nav-text">11.5 Flink 的运行必须依赖 Hadoop 组件吗</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-6-Flink%E9%9B%86%E7%BE%A4%E6%9C%89%E5%93%AA%E4%BA%9B%E8%A7%92%E8%89%B2%EF%BC%9F%E5%90%84%E8%87%AA%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8"><span class="nav-text">11.6  Flink集群有哪些角色？各自有什么作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-7-Flink-%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E4%B8%AD-Task-Slot-%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-text">11.7  Flink 资源管理中 Task Slot 的概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-8-Flink%E7%9A%84%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5%E4%BA%86%E8%A7%A3%E5%90%97"><span class="nav-text">11.8  Flink的重启策略了解吗</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-9-Flink-%E6%98%AF%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81-Exactly-once-%E8%AF%AD%E4%B9%89%E7%9A%84"><span class="nav-text">11.9  Flink 是如何保证 Exactly-once 语义的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-10-%E5%A6%82%E6%9E%9C%E4%B8%8B%E7%BA%A7%E5%AD%98%E5%82%A8%E4%B8%8D%E6%94%AF%E6%8C%81%E4%BA%8B%E5%8A%A1%EF%BC%8CFlink-%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81-exactly-once"><span class="nav-text">11.10  如果下级存储不支持事务，Flink 怎么保证 exactly-once</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-11-Flink%E6%98%AF%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%8F%8D%E5%8E%8B%E7%9A%84"><span class="nav-text">11.11  Flink是如何处理反压的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-12-Flink%E4%B8%AD%E7%9A%84%E7%8A%B6%E6%80%81%E5%AD%98%E5%82%A8"><span class="nav-text">11.12  Flink中的状态存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-13-Flink%E6%98%AF%E5%A6%82%E4%BD%95%E6%94%AF%E6%8C%81%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93%E7%9A%84"><span class="nav-text">11.13  Flink是如何支持流批一体的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-14-Flink%E7%9A%84%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%98%AF%E5%A6%82%E4%BD%95%E5%81%9A%E7%9A%84"><span class="nav-text">11.14  Flink的内存管理是如何做的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-15-Flink-CEP-%E7%BC%96%E7%A8%8B%E4%B8%AD%E5%BD%93%E7%8A%B6%E6%80%81%E6%B2%A1%E6%9C%89%E5%88%B0%E8%BE%BE%E7%9A%84%E6%97%B6%E5%80%99%E4%BC%9A%E5%B0%86%E6%95%B0%E6%8D%AE%E4%BF%9D%E5%AD%98%E5%9C%A8%E5%93%AA%E9%87%8C"><span class="nav-text">11.15  Flink CEP 编程中当状态没有到达的时候会将数据保存在哪里</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-Flink%E9%9D%A2%E8%AF%95%E9%A2%98%E9%99%84%E5%8A%A0"><span class="nav-text">12  Flink面试题附加</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-%E8%AE%B2%E2%BC%80%E4%B8%8BFlink%E7%9A%84%E8%BF%90%E2%BE%8F%E6%9E%B6%E6%9E%84"><span class="nav-text">12.1  讲⼀下Flink的运⾏架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-2-Flink%E7%9A%84%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="nav-text">12.2  Flink的作业执行流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-3-Flink%E7%9A%84%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="nav-text">12.3  Flink的部署模式都有哪些？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-4-%E8%AE%B2%E4%B8%80%E4%B8%8BFlink-on-yarn%E7%9A%84%E9%83%A8%E7%BD%B2"><span class="nav-text">12.4  讲一下Flink  on yarn的部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-5-Flink%E7%9A%84state%E6%98%AF%E5%AD%98%E5%82%A8%E5%9C%A8%E5%93%AA%E9%87%8C%E7%9A%84"><span class="nav-text">12.5  Flink的state是存储在哪里的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-6-Flink%E7%9A%84window%E5%88%86%E7%B1%BB"><span class="nav-text">12.6  Flink的window分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-7-Flink%E7%9A%84window%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6"><span class="nav-text">12.7  Flink的window实现机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-8-Flink%E5%85%B7%E4%BD%93%E6%98%AF%E5%A6%82%E4%BD%95%E6%97%B6%E6%95%88%E5%86%85excatly-once%E8%AF%AD%E4%B9%89%E7%9A%84"><span class="nav-text">12.8  Flink具体是如何时效内excatly  once语义的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-9-Flink%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%8F%8D%E5%8E%8B%E7%9A%84"><span class="nav-text">12.9  Flink是如何实现反压的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-10-flink%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4%E6%A6%82%E5%BF%B5-eventTime-%E5%92%8C-processTime%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-text">12.10  flink中的时间概念 , eventTime 和 processTime的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-11-flink%E7%9A%84session-window%E6%80%8E%E4%B9%88%E4%BD%BF%E7%94%A8"><span class="nav-text">12.11  flink的session  window怎么使用</span></a></li></ol></li></ol></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2023</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Markilue</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        VISITOR COUNT&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        TOTAL PAGE VIEWS&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a></span>
                <br>
            <span class="theme-version-container">THEME&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.1.4</a>
        </div>
        
        
        
            <div id="start_div" style="display:none">
                2023/5/27 11:45:14
            </div>
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fa-solid fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa-solid fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    


</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/navbarShrink.js"></script>

<script src="/js/tools/scrollTopBottom.js"></script>

<script src="/js/tools/lightDarkSwitch.js"></script>



    
<script src="/js/tools/localSearch.js"></script>




    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/layouts/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>







<div class="post-scripts pjax">
    
        
<script src="/js/tools/tocToggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/layouts/toc.js"></script>

<script src="/js/plugins/tabs.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax',
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




</body>
</html>
